# Letta é¡¹ç›®æ·±åº¦åˆ†æ

> **æ–‡æ¡£è¯´æ˜**: æœ¬æ–‡æ¡£è®°å½•äº†å¯¹ Letta é¡¹ç›®çš„æ·±åº¦åˆ†æå’Œç†è§£ï¼ŒåŒ…æ‹¬ Provider é€‰æ‹©æœºåˆ¶ã€Embedding é…ç½®ã€API Key ç®¡ç†ã€ä»¥åŠå›¾å½¢ç•Œé¢ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚
>
> **åˆ›å»ºæ—¶é—´**: 2026-01-04
> **æœ€åæ›´æ–°**: 2026-01-09

## ç›®å½•

- [0. é‡è¦æ›´æ–°ï¼šAnthropic Embedding çš„çœŸç›¸](#0-é‡è¦æ›´æ–°anthropic-embedding-çš„çœŸç›¸2026-01-04)
- [1. Provider é€‰æ‹©æœºåˆ¶](#1-provider-é€‰æ‹©æœºåˆ¶)
- [2. å®¹å™¨ç¯å¢ƒä¸‹çš„é…ç½®é€»è¾‘](#2-å®¹å™¨ç¯å¢ƒä¸‹çš„é…ç½®é€»è¾‘)
- [3. é—®é¢˜åœºæ™¯åˆ†æ](#3-é—®é¢˜åœºæ™¯åˆ†æ)
- [4. å…³é”®ä»£ç æ–‡ä»¶æ±‡æ€»](#4-å…³é”®ä»£ç æ–‡ä»¶æ±‡æ€»)
- [5. æ€»ç»“](#5-æ€»ç»“)
- [6. å®é™…éªŒè¯æµ‹è¯•](#6-å®é™…éªŒè¯æµ‹è¯•2026-01-04)
- [7. LLM å’Œ Embedding API Key åˆ†ç¦»é…ç½®](#7-llm-å’Œ-embedding-api-key-åˆ†ç¦»é…ç½®)
- [8. ç¯å¢ƒå˜é‡æ— æ³•åˆ†ç¦» LLM å’Œ Embedding API Key çš„é™åˆ¶](#8-ç¯å¢ƒå˜é‡æ— æ³•åˆ†ç¦»-llm-å’Œ-embedding-api-key-çš„é™åˆ¶)
- [9. Letta å›¾å½¢ç•Œé¢ï¼ˆWeb UIï¼‰è°ƒæŸ¥](#9-letta-å›¾å½¢ç•Œé¢web-uiè°ƒæŸ¥)
- [10. å®Œæ•´è°ƒæŸ¥æ€»ç»“](#10-å®Œæ•´è°ƒæŸ¥æ€»ç»“)
- [11. Embedding è°ƒç”¨æµç¨‹è¯¦è§£](#11-embedding-è°ƒç”¨æµç¨‹è¯¦è§£2026-01-04)
- [12. åˆ›å»º Agent çš„å®Œæ•´æµç¨‹](#12-åˆ›å»º-agent-çš„å®Œæ•´æµç¨‹2026-01-04)
- [13. Agent å‚æ•°å®Œæ•´å‚è€ƒ](#13-agent-å‚æ•°å®Œæ•´å‚è€ƒ2026-01-04)
  - [13.1 å…³äº -d å‚æ•°](#131-about--d-å‚æ•°)
  - [13.2 æ‰€æœ‰å‚æ•°åˆ—è¡¨](#132-createagent-æ‰€æœ‰å‚æ•°åˆ—è¡¨)
  - [13.3 å®Œæ•´é…ç½®ç¤ºä¾‹](#133-å®Œæ•´é…ç½®ç¤ºä¾‹)
  - [13.4 å¸¸è§é…ç½®æ¨¡å¼](#134-å¸¸è§é…ç½®æ¨¡å¼)
  - [13.5 å‚æ•°ä¼˜å…ˆçº§](#135-å‚æ•°ä¼˜å…ˆçº§)
  - [13.6 é…ç½®å»ºè®®æ€»ç»“](#136-é…ç½®å»ºè®®æ€»ç»“)
  - [13.7 å‚æ•°é€ŸæŸ¥è¡¨](#137-å‚æ•°é€ŸæŸ¥è¡¨)
  - [13.8 ä¸ºä»€ä¹ˆåºŸå¼ƒè¯¦ç»†é…ç½®ï¼Ÿ](#138-ä¸ºä»€ä¹ˆåºŸå¼ƒè¯¦ç»†é…ç½®æ·±åº¦è§£æ)
- [14. å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰ Provider](#14-å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰-provider2026-01-04)
- [15. Letta UI åˆ›å»º Agent çš„ Provider é€‰æ‹©é—®é¢˜](#15-letta-ui-åˆ›å»º-agent-çš„-provider-é€‰æ‹©é—®é¢˜2026-01-04)
- [16. OPENAI_API_HEADERS ç¯å¢ƒå˜é‡çš„çœŸç›¸](#16-openai_api_headers-ç¯å¢ƒå˜é‡çš„çœŸç›¸2026-01-05)
- [18. å‰ç«¯åˆ›å»º Agent çš„ BYOK æ¨¡å¼å®ç°](#18-å‰ç«¯åˆ›å»º-agent-çš„-byok-æ¨¡å¼å®ç°2026-01-09)
- [19. Base LLM å’Œ Embedding æ¨¡å‹æ£€ç´¢æµç¨‹](#19-base-llm-å’Œ-embedding-æ¨¡å‹æ£€ç´¢æµç¨‹2026-01-09)
- [20. Agent æ˜¾ç¤ºå’Œæ¨¡å¼åˆ¤æ–­çš„å…³é”®å‘ç°](#20-agent-æ˜¾ç¤ºå’Œæ¨¡å¼åˆ¤æ–­çš„å…³é”®å‘ç°2026-01-09) â­ æ–°å¢

## é—®é¢˜èƒŒæ™¯

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å‘ç°ï¼šå³ä½¿æä¾›çš„æ˜¯ `OPENAI_API_KEY`ï¼Œä½†å¦‚æœ model name æ˜¯ Claudeï¼Œé¡¹ç›®ä»ç„¶ä¼šå¼ºåˆ¶èµ° Anthropic çš„è¯·æ±‚æ„å»ºé€»è¾‘ã€‚

## æ ¸å¿ƒå‘ç°

**æ˜¯çš„ï¼Œä»£ç ä¸­å­˜åœ¨æ ¹æ® model ç›¸å…³ä¿¡æ¯å†³å®šä½¿ç”¨å“ªä¸ª provider çš„é€»è¾‘ã€‚**

## 0. é‡è¦æ›´æ–°ï¼šAnthropic Embedding çš„çœŸç›¸ï¼ˆ2026-01-04ï¼‰

### 0.1 å…³é”®å‘ç°

é€šè¿‡æ·±å…¥åˆ†æä»£ç ï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªé‡è¦çš„è®¾è®¡é—®é¢˜ï¼š

**âŒ Anthropic å®˜æ–¹ä¸æä¾› Embedding æœåŠ¡**

- `AnthropicClient` ä¸­**å®Œå…¨æ²¡æœ‰** embedding ç›¸å…³ä»£ç 
- æ²¡æœ‰ `request_embeddings()` æ–¹æ³•
- æ²¡æœ‰ä»»ä½• embedding endpoint è°ƒç”¨

### 0.2 ä»£ç è¯æ®

**Anthropic Client** (`letta/llm_api/anthropic_client.py`):
```python
# åªæœ‰ LLM ç›¸å…³æ–¹æ³•
- request()
- request_async()
- stream_async()
- build_request_data()
- convert_response_to_chat_completion()

# âŒ å®Œå…¨æ²¡æœ‰ embedding ç›¸å…³æ–¹æ³•
```

**å¯¹æ¯” OpenAI Client** (`letta/llm_api/openai_client.py`):
```python
# æœ‰å®Œæ•´çš„ embedding æ”¯æŒ
def _prepare_client_kwargs_embedding(self, embedding_config: EmbeddingConfig) -> dict:
    api_key = model_settings.openai_api_key or os.environ.get("OPENAI_API_KEY")
    kwargs = {"api_key": api_key, "base_url": embedding_config.embedding_endpoint}
    return kwargs

async def request_embeddings(self, inputs: List[str], embedding_config: EmbeddingConfig):
    # å®Œæ•´çš„ embedding å®ç°
```

### 0.3 Embedding å®¢æˆ·ç«¯é€‰æ‹©é€»è¾‘

**å…³é”®ä»£ç ** (`letta/llm_api/llm_client.py`):
```python
def create(provider_type: ProviderType, ...):
    match provider_type:
        case ProviderType.anthropic:
            return AnthropicClient(...)  # âŒ æ²¡æœ‰ request_embeddings
        case ProviderType.openai:
            return OpenAIClient(...)     # âœ… æœ‰ request_embeddings
        case _:
            return OpenAIClient(...)     # âœ… æœ‰ request_embeddings
```

### 0.4 EmbeddingConfig ä¸­çš„é”™è¯¯è®¾è®¡

**é—®é¢˜** (`letta/schemas/embedding_config.py:11`):
```python
embedding_endpoint_type: Literal[
    "openai",
    "anthropic",  # âŒ è¿™ä¸ªé€‰é¡¹ä¸åº”è¯¥å­˜åœ¨ï¼
    "bedrock",
    ...
]
```

**é”™è¯¯åŸå› **ï¼š
- Schema ä¸­å®šä¹‰äº† `"anthropic"` ä½œä¸º `embedding_endpoint_type`
- ä½† `AnthropicClient` **æ²¡æœ‰å®ç°** `request_embeddings()` æ–¹æ³•
- å¦‚æœç”¨æˆ·å°è¯•ä½¿ç”¨ `embedding_endpoint_type="anthropic"`ï¼Œä¼šæŠ¥é”™

**æ­£ç¡®çš„è®¾è®¡**ï¼š
- âŒ ä¸åº”è¯¥åœ¨ EmbeddingConfig ä¸­åŒ…å« `"anthropic"` é€‰é¡¹
- âœ… å› ä¸º Anthropic ä¸æä¾› embedding æœåŠ¡

### 0.5 Anthropic ç”¨æˆ·å¦‚ä½•ä½¿ç”¨ Embeddingï¼Ÿ

#### âœ… æ–¹å¼ 1ï¼šä½¿ç”¨ Letta å…è´¹ Embeddingï¼ˆæ¨èï¼‰

```bash
# .env
ANTHROPIC_API_KEY=sk-ant-xxxxx
# ä¸éœ€è¦å…¶ä»– key
```

```python
{
    "llm_config": {
        "model": "claude-sonnet-4-5-20250929",
        "model_endpoint_type": "anthropic"
    },
    "embedding_config": {
        "embedding_model": "letta",
        "embedding_endpoint": "https://embeddings.letta.com",
        "embedding_endpoint_type": "openai"  # Letta ä½¿ç”¨ OpenAI å…¼å®¹åè®®
    }
}
```

#### âœ… æ–¹å¼ 2ï¼šä½¿ç”¨ OpenAI Embedding

```bash
# .env
ANTHROPIC_API_KEY=sk-ant-xxxxx      # Anthropic LLM
OPENAI_API_KEY=sk-openai-yyyyy      # OpenAI embedding
```

```python
{
    "llm_config": {
        "model": "claude-sonnet-4-5-20250929",
        "model_endpoint_type": "anthropic"
    },
    "embedding_config": {
        "embedding_model": "text-embedding-3-small",
        "embedding_endpoint": "https://api.openai.com/v1",
        "embedding_endpoint_type": "openai"
    }
}
```

### 0.6 å…³é”®è¦ç‚¹

1. **Anthropic ä¸æä¾› embedding æœåŠ¡**
   - å®˜æ–¹åªæä¾› LLMï¼ˆClaude æ¨¡å‹ï¼‰
   - æ²¡æœ‰ä»»ä½• embedding API

2. **Embedding é…ç½®æ˜¯ç‹¬ç«‹çš„**
   - LLM å’Œ embedding ä½¿ç”¨ä¸åŒçš„ provider
   - ç¯å¢ƒå˜é‡å¯ä»¥åŒæ—¶é…ç½®å¤šä¸ª key

3. **ç¯å¢ƒå˜é‡ä¸å†²çª**
   ```bash
   ANTHROPIC_API_KEY=sk-ant-...    # ç”¨äº Claude LLM
   OPENAI_API_KEY=sk-openai-...    # ç”¨äº OpenAI embedding
   # âœ… ä¸¤ä¸ª key äº’ä¸å†²çªï¼Œç”¨äºä¸åŒçš„æœåŠ¡
   ```

4. **ä»£ç ä¸­çš„è®¾è®¡é—®é¢˜**
   - EmbeddingConfig ä¸åº”åŒ…å« "anthropic" é€‰é¡¹
   - è¿™æ˜¯ä¸€ä¸ªé—ç•™çš„è®¾è®¡é”™è¯¯

### 0.7 å¯¹æ–‡æ¡£ä¸­å…¶ä»–ç»“è®ºçš„ä¿®æ­£

**ä¹‹å‰æ–‡æ¡£ä¸­çš„ä¸å‡†ç¡®è¡¨è¿°**ï¼š
- "æ‰€æœ‰ provider éƒ½æœ‰ç›¸åŒçš„é™åˆ¶" â†’ âŒ é”™è¯¯
- "Anthropic ä¹Ÿæœ‰ embedding API key çš„é—®é¢˜" â†’ âŒ é”™è¯¯

**æ­£ç¡®çš„ç†è§£**ï¼š
- Anthropic **æ ¹æœ¬æ²¡æœ‰** embedding æœåŠ¡
- ä¸å­˜åœ¨ API key å…±äº«çš„é—®é¢˜
- Anthropic ç”¨æˆ·å¿…é¡»ä½¿ç”¨å…¶ä»– provider çš„ embedding

## 1. Provider é€‰æ‹©æœºåˆ¶

### 1.1 åŸºäº Handle æ ¼å¼çš„é€‰æ‹©

é¡¹ç›®é€šè¿‡ **handle æ ¼å¼** `{provider_name}/{model_name}` æ¥å†³å®šä½¿ç”¨å“ªä¸ª providerï¼Œè€Œä¸æ˜¯ç›´æ¥æ ¹æ® API key æˆ– model nameã€‚

**å…³é”®ä»£ç ä½ç½®**: `letta/server/server.py:1164`

```python
async def get_llm_config_from_handle_async(
    self,
    actor: User,
    handle: str,
    ...
) -> LLMConfig:
    try:
        provider_name, model_name = handle.split("/", 1)  # å…³é”®ï¼šè§£æ provider name
        provider = await self.get_provider_from_name_async(provider_name, actor)

        all_llm_configs = await provider.list_llm_models_async()
        llm_configs = [config for config in all_llm_configs if config.handle == handle]
        # ...
    except ValueError as e:
        # å¤„ç†æœ¬åœ° LLM é…ç½®
        llm_configs = [config for config in self.get_local_llm_configs() if config.handle == handle]
        # ...
```

**å·¥ä½œåŸç†**ï¼š
- ç³»ç»Ÿè§£æ handle çš„ç¬¬ä¸€éƒ¨åˆ†ï¼ˆ`/` ä¹‹å‰çš„éƒ¨åˆ†ï¼‰æ¥ç¡®å®š provider name
- ç„¶åæ ¹æ® provider name è·å– provider å®ä¾‹
- æœ€åä½¿ç”¨è¯¥ provider çš„ API å’Œè¯·æ±‚æ„å»ºé€»è¾‘

### 1.2 Model Endpoint Type çš„ä½œç”¨

æ¯ä¸ªæ¨¡å‹é…ç½®éƒ½æœ‰ä¸€ä¸ª `model_endpoint_type` å­—æ®µï¼Œå®ƒå­˜å‚¨äº†å®é™…çš„ endpoint ç±»å‹ã€‚

**å…³é”®ä»£ç ä½ç½®**: `letta/schemas/provider_model.py:45`

```python
model_endpoint_type: str = Field(
    ...,
    description="The endpoint type for the model (e.g., 'openai', 'anthropic')"
)
```

**ç”¨é€”**ï¼š
- ç¡®å®šæ¨¡å‹çš„ç‰¹æ€§æ”¯æŒï¼ˆå¦‚æ˜¯å¦æ”¯æŒæµå¼ä¼ è¾“ã€å·¥å…·è°ƒç”¨ç­‰ï¼‰
- æ„å»ºæ­£ç¡®çš„ API è¯·æ±‚
- å†³å®šä½¿ç”¨å“ªä¸ª streaming interface

**ç¤ºä¾‹ä»£ç ** (`letta/adapters/letta_llm_stream_adapter.py:57`):

```python
# æ ¹æ® model_endpoint_type é€‰æ‹© streaming interface
if self.llm_config.model_endpoint_type in [ProviderType.anthropic, ProviderType.bedrock]:
    self.interface = AnthropicStreamingInterface(...)
```

### 1.3 Provider Model å­˜å‚¨ç»“æ„

**Handle æ ¼å¼**: `provider_display_name/model_display_name`

ç¤ºä¾‹ï¼š
- `openai/gpt-4o`
- `anthropic/claude-3-5-sonnet`
- `openai-proxy/claude-3-5-sonnet` (ç”¨äº OpenRouter ç­‰ç¬¬ä¸‰æ–¹æœåŠ¡)

## 2. å®¹å™¨ç¯å¢ƒä¸‹çš„é…ç½®é€»è¾‘

### 2.1 ç¯å¢ƒå˜é‡é…ç½®

åœ¨å®¹å™¨ç¯å¢ƒä¸­ï¼Œç”¨æˆ·åªéœ€è¦åœ¨ `.env` æ–‡ä»¶ä¸­é…ç½®ï¼š

```bash
# OpenAI é…ç½®
OPENAI_API_KEY=sk-...

# Anthropic é…ç½®
ANTHROPIC_API_KEY=sk-ant-...

# å…¶ä»– provider é…ç½®
OLLAMA_BASE_URL=http://host.docker.internal:11434
VLLM_API_BASE=http://host.docker.internal:8000
```

### 2.2 Provider åˆ›å»ºå’Œæ¨¡å‹åŒæ­¥æµç¨‹

å½“ç³»ç»Ÿå¯åŠ¨æ—¶ï¼š

1. **åˆ›å»º Provider**
   - ä½¿ç”¨ç¯å¢ƒå˜é‡åˆ›å»º Provider å¯¹è±¡
   - è®¾ç½® `base_url` å’Œ `api_key`
   - Provider çš„ `default_base_url` validator ä¼šç¡®ä¿ base_url æœ‰é»˜è®¤å€¼

   **ä»£ç ä½ç½®**: `letta/schemas/providers.py:38`

   ```python
   @model_validator(mode="after")
   def default_base_url(self):
       if self.provider_type == ProviderType.openai and self.base_url is None:
           self.base_url = model_settings.openai_api_base
       return self
   ```

2. **è‡ªåŠ¨åŒæ­¥æ¨¡å‹åˆ—è¡¨**
   - åˆ›å»º provider åè‡ªåŠ¨è°ƒç”¨ `_sync_default_models_for_provider`
   - ä»å®é™…çš„ API è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨

   **ä»£ç ä½ç½®**: `letta/services/provider_manager.py:93`

   ```python
   # For BYOK providers, automatically sync available models
   if is_byok:
       await self._sync_default_models_for_provider(provider_pydantic, actor)
   ```

3. **è®¾ç½® model_endpoint_type**

   **OpenAI Provider** (`letta/schemas/providers.py:329`):
   ```python
   llm_config = LLMConfig(
       model=model_name,
       model_endpoint_type="openai",  # â† OpenAI ç±»å‹
       model_endpoint=self.base_url,
       context_window=context_window_size,
       handle=handle,
       ...
   )
   ```

   **Anthropic Provider** (`letta/schemas/providers.py:821`):
   ```python
   llm_config = LLMConfig(
       model=model["id"],
       model_endpoint_type="anthropic",  # â† Anthropic ç±»å‹
       model_endpoint=self.base_url,
       context_window=model["context_window"],
       handle=handle,
       ...
   )
   ```

4. **å­˜å‚¨åˆ°æ•°æ®åº“**
   - æ¨¡å‹é…ç½®ä¿å­˜åˆ° `provider_models` è¡¨
   - åŒ…å« `handle`ã€`model_endpoint_type`ã€`model_endpoint` ç­‰ä¿¡æ¯
   - ä¹‹åä½¿ç”¨æ—¶ç›´æ¥ä»æ•°æ®åº“è¯»å–

### 2.3 get_local_llm_configs å·²åºŸå¼ƒ

**ä»£ç ä½ç½®**: `letta/server/server.py:1266`

```python
def get_local_llm_configs(self):
    llm_models = []
    # NOTE: deprecated
    # (ä¹‹å‰ç”¨äºä» ~/.letta/llm_configs è¯»å–æœ¬åœ°é…ç½®ï¼Œç°å·²åºŸå¼ƒ)
    return llm_models  # è¿”å›ç©ºåˆ—è¡¨
```

**è¯´æ˜**ï¼šæœ¬åœ°é…ç½®æ–‡ä»¶æ–¹å¼å·²åºŸå¼ƒï¼Œç°åœ¨é€šè¿‡æ•°æ®åº“å’Œ provider åŒæ­¥æœºåˆ¶ç®¡ç†ã€‚

## 3. é—®é¢˜åœºæ™¯åˆ†æ

### 3.1 é”™è¯¯é…ç½®ç¤ºä¾‹

```bash
# .env é…ç½®
OPENAI_API_KEY=sk-openai-key
```

```python
# ä½¿ç”¨æ—¶çš„é”™è¯¯æ–¹å¼
handle = "anthropic/claude-3-5-sonnet"
```

**ç»“æœ**ï¼š
- âœ… ä» handle è§£æå‡º `provider_name="anthropic"`
- âŒ éœ€è¦æŸ¥æ‰¾æˆ–åˆ›å»º Anthropic providerï¼ˆéœ€è¦ `ANTHROPIC_API_KEY`ï¼‰
- âŒ ä½¿ç”¨æ•°æ®åº“ä¸­è¯¥æ¨¡å‹çš„é…ç½®ï¼Œå…¶ä¸­ `model_endpoint_type="anthropic"`
- âŒ ä½¿ç”¨ Anthropic çš„è¯·æ±‚æ„å»ºé€»è¾‘ï¼Œå‘ Anthropic API å‘é€è¯·æ±‚

### 3.2 æ­£ç¡®é…ç½®ç¤ºä¾‹

**åœºæ™¯ 1ï¼šä½¿ç”¨ OpenAI å®˜æ–¹ API**

```bash
# .env
OPENAI_API_KEY=sk-...
```

```python
# æ­£ç¡®çš„ handle
handle = "openai/gpt-4o"  # æˆ–å…¶ä»– OpenAI æ¨¡å‹
```

**åœºæ™¯ 2ï¼šä½¿ç”¨ OpenRouter ç­‰ç¬¬ä¸‰æ–¹æœåŠ¡ï¼ˆæä¾› Claude æ¨¡å‹ï¼‰**

```bash
# .env
OPENAI_API_KEY=sk-or-...  # OpenRouter API key
OPENAI_API_BASE=https://openrouter.ai/api/v1
```

```python
# æ­£ç¡®çš„ handle
handle = "openai-proxy/claude-3-5-sonnet"
```

**è¯´æ˜**ï¼š
- `openai-proxy` æ˜¯è‡ªå®šä¹‰ provider name
- ä½¿ç”¨ OpenAI å…¼å®¹çš„ APIï¼ˆ`model_endpoint_type="openai"`ï¼‰
- ä½†å®é™… endpoint æ˜¯ OpenRouter

## 4. å…³é”®ä»£ç æ–‡ä»¶æ±‡æ€»

| æ–‡ä»¶è·¯å¾„ | ä½œç”¨ |
|---------|------|
| `letta/server/server.py` | Handle è§£æå’Œ provider é€‰æ‹©é€»è¾‘ |
| `letta/services/provider_manager.py` | Provider åˆ›å»ºå’Œæ¨¡å‹åŒæ­¥ç®¡ç† |
| `letta/schemas/providers.py` | Provider ç±»å®šä¹‰å’Œæ¨¡å‹åˆ—è¡¨ç”Ÿæˆ |
| `letta/schemas/provider_model.py` | Provider Model æ•°æ®ç»“æ„å®šä¹‰ |
| `letta/schemas/llm_config.py` | LLMConfig é…ç½®å’ŒéªŒè¯é€»è¾‘ |
| `letta/adapters/letta_llm_stream_adapter.py` | Streaming interface é€‰æ‹© |

## 5. æ€»ç»“

### æ ¸å¿ƒè®¾è®¡åŸåˆ™

1. **Handle å†³å®š Provider**ï¼š`provider_name/model_name` æ ¼å¼ä¸­çš„ provider name æ˜¯å†³å®šæ€§å› ç´ 
2. **Provider Type å†³å®š Endpoint Type**ï¼š`model_endpoint_type` ç”± provider ç±»å‹è®¾ç½®ï¼Œä¸æ˜¯ç”± model name å­—ç¬¦ä¸²æ¨æ–­
3. **æ•°æ®åº“æŒä¹…åŒ–**ï¼šæ¨¡å‹é…ç½®å­˜å‚¨åœ¨æ•°æ®åº“ä¸­ï¼Œé¿å…é‡å¤æŸ¥è¯¢ API
4. **è‡ªåŠ¨åŒæ­¥æœºåˆ¶**ï¼šåˆ›å»º BYOK provider æ—¶è‡ªåŠ¨åŒæ­¥å¯ç”¨æ¨¡å‹åˆ—è¡¨

### å…³é”®è¦ç‚¹

- âŒ **é”™è¯¯è®¤çŸ¥**ï¼šmodel name åŒ…å« "claude" å°±ä¼šä½¿ç”¨ Anthropic é€»è¾‘
- âœ… **æ­£ç¡®é€»è¾‘**ï¼šhandle ä¸­çš„ provider name å†³å®šä½¿ç”¨å“ªä¸ª provider çš„é€»è¾‘
- âŒ **é”™è¯¯è®¤çŸ¥**ï¼šæä¾› `OPENAI_API_KEY` å°±èƒ½è®¿é—®ä»»ä½•æ¨¡å‹
- âœ… **æ­£ç¡®é€»è¾‘**ï¼šAPI key å¿…é¡»ä¸ handle ä¸­çš„ provider åŒ¹é…

### ç”¨æˆ·å»ºè®®

1. ä½¿ç”¨æ­£ç¡®çš„ handle æ ¼å¼
2. ç¡®ä¿ handle ä¸­çš„ provider name ä¸é…ç½®çš„ API key å¯¹åº”
3. å¯¹äºç¬¬ä¸‰æ–¹æœåŠ¡ï¼ˆå¦‚ OpenRouterï¼‰ï¼Œä½¿ç”¨ `openai-proxy` ä½œä¸º provider name
4. æ£€æŸ¥æ•°æ®åº“ä¸­çš„ `provider_models` è¡¨ï¼Œç¡®è®¤æ¨¡å‹çš„ `model_endpoint_type` æ˜¯å¦æ­£ç¡®

---

## 6. å®é™…éªŒè¯æµ‹è¯•ï¼ˆ2026-01-04ï¼‰

### 6.1 æµ‹è¯•ç¯å¢ƒ

- **é¡¹ç›®**: Letta v0.16.1
- **éƒ¨ç½²æ–¹å¼**: Docker Compose
- **è¿œç¨‹API**: https://lingyunapi.com/v1
- **API Key**: sk-tlegmZDKQBW5rce5sGaMdQeprOvDZgaRhr37KMhkieoiRIvh

### 6.2 é—®é¢˜çš„çœŸç›¸

#### å‘ç°çš„é”™è¯¯é…ç½®

æ•°æ®åº“ä¸­å­˜åœ¨3ä¸ªæ‰‹åŠ¨åˆ›å»ºçš„é”™è¯¯providerï¼š

| åç§° | provider_type | base_url | åˆ›å»ºæ—¶é—´ | çŠ¶æ€ |
|------|--------------|----------|---------|------|
| `anthropic-proxy` | anthropic âŒ | https://lingyunapi.com/v1 | 2026-01-03 16:02 | å·²åˆ é™¤ |
| `anthropic-sonnet45` | anthropic âŒ | https://lingyunapi.com/v1 | 2026-01-03 16:04 | å·²åˆ é™¤ |
| `anthropic-lingyun` | anthropic âŒ | https://lingyunapi.com | 2026-01-03 16:10 | å·²åˆ é™¤ |

**é”™è¯¯åŸå› **ï¼š
- `provider_type: "anthropic"` - ä½¿ç”¨ Anthropic åè®®
- ä½†å®é™…APIæ˜¯ OpenAI å…¼å®¹çš„ï¼Œä¸æ˜¯ Anthropic åè®®
- å¯¼è‡´ 401 è®¤è¯å¤±è´¥ï¼š`invalid x-api-key`

**å…³é”®å‘ç°**ï¼š
- âŒ è¿™äº›provider **ä¸æ˜¯è‡ªåŠ¨åˆ›å»ºçš„**
- âœ… æ˜¯é€šè¿‡ API æˆ– Web UI æ‰‹åŠ¨åˆ›å»ºçš„
- âŒ åˆ›å»ºæ—¶é€‰æ‹©äº†é”™è¯¯çš„ provider_type

### 6.3 ç¯å¢ƒå˜é‡é…ç½®éªŒè¯

#### å½“å‰é…ç½®

```bash
# .env
OPENAI_API_KEY=sk-tlegmZDKQBW5rce5sGaMdQeprOvDZgaRhr37KMhkieoiRIvh
OPENAI_API_BASE=https://lingyunapi.com/v1
```

#### å·¥ä½œåŸç†

ç¯å¢ƒå˜é‡åœ¨å®¹å™¨å¯åŠ¨æ—¶è¢« Letta è¯†åˆ«ï¼Œ**è‡ªåŠ¨åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ OpenAI provider**ï¼š

- âœ… ä½¿ç”¨ OpenAI åè®®
- âœ… è¿æ¥åˆ° `https://lingyunapi.com/v1`
- âœ… æˆåŠŸåŒæ­¥æ¨¡å‹åˆ—è¡¨
- âœ… æ— éœ€æŒä¹…åŒ–åˆ°æ•°æ®åº“

#### å¯ç”¨æ¨¡å‹

æ€»å…±æœ‰ **13ä¸ªæ¨¡å‹**ï¼š
- 1ä¸ª: `letta/letta-free` (Letta é»˜è®¤æ¨¡å‹)
- 12ä¸ª: `openai-proxy/claude-*` (ä½ çš„è¿œç¨‹APIæä¾›çš„æ¨¡å‹)

**æ¨¡å‹åˆ—è¡¨ç¤ºä¾‹**ï¼š
```
openai-proxy/claude-sonnet-4-5-20250929
openai-proxy/claude-opus-4-5-20251101
openai-proxy/claude-haiku-4-5-20251001
...
```

#### å…³é”®é…ç½®ä¿¡æ¯

```json
{
  "handle": "openai-proxy/claude-sonnet-4-5-20250929",
  "model": "claude-sonnet-4-5-20250929",
  "model_endpoint_type": "openai",  // âœ… ä½¿ç”¨ OpenAI åè®®
  "model_endpoint": "https://lingyunapi.com/v1",
  "provider_name": "openai-proxy",
  "provider_category": "base"  // âœ… åŸºç¡€providerï¼ˆç¯å¢ƒå˜é‡åˆ›å»ºï¼‰
}
```

### 6.4 API è°ƒç”¨æµ‹è¯•

#### æµ‹è¯•æ­¥éª¤

1. åˆ›å»ºæµ‹è¯• agent
2. å‘é€æµ‹è¯•æ¶ˆæ¯
3. éªŒè¯å“åº”

#### Agent é…ç½®

```json
{
  "name": "test-agent-lingyun",
  "llm_config": {
    "model": "claude-sonnet-4-5-20250929",
    "model_endpoint_type": "openai",
    "model_endpoint": "https://lingyunapi.com/v1",
    "provider_name": "openai-proxy",
    "provider_category": "byok",
    "context_window": 200000,
    "temperature": 0.7
  }
}
```

#### æµ‹è¯•ç»“æœ

âœ… **Agent åˆ›å»ºæˆåŠŸ**
```
ID: agent-e8056100-2e73-43e0-b52e-d3edb2c348d4
åç§°: test-agent-lingyun
æ¨¡å‹: claude-sonnet-4-5-20250929
```

âœ… **æ¶ˆæ¯å‘é€æˆåŠŸ**
```
Prompt tokens: 416
Completion tokens: 43
Total tokens: 459
Stop reason: end_turn
```

âœ… **AI å›å¤æ­£å¸¸**
> "ä½ å¥½ï¼æˆ‘æ˜¯ Claude Codeï¼ŒAnthropic å®˜æ–¹çš„ Claude å‘½ä»¤è¡Œç•Œé¢åŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºä½ æä¾›å¸®åŠ©ã€‚"

### 6.5 éªŒè¯ç»“è®º

#### âœ… ç¯å¢ƒå˜é‡é…ç½®å®Œå…¨æ­£ç¡®

ä½ çš„é…ç½® **100%æ­£ç¡®**ï¼ç¯å¢ƒå˜é‡è®© Letta èƒ½å¤Ÿï¼š

1. **æ­£ç¡®è¯†åˆ«** ä¸º OpenAI å…¼å®¹çš„ API
2. **ä½¿ç”¨æ­£ç¡®çš„åè®®**ï¼ˆOpenAIï¼‰è¿›è¡Œé€šä¿¡
3. **æˆåŠŸè°ƒç”¨** è¿œç¨‹ API è·å– Claude æ¨¡å‹çš„å“åº”
4. **æ— éœ€æ‰‹åŠ¨åˆ›å»º** provider

#### ğŸ“‹ æ­£ç¡®çš„ä½¿ç”¨æ–¹å¼

å¯¹äºä½ çš„è¿œç¨‹ APIï¼Œ**åªéœ€è¦é…ç½®ç¯å¢ƒå˜é‡**ï¼š

```bash
# .env
OPENAI_API_KEY=sk-your-api-key
OPENAI_API_BASE=https://lingyunapi.com/v1
```

**ä¸éœ€è¦**ï¼š
- âŒ æ‰‹åŠ¨åˆ›å»º provider
- âŒ è®¾ç½® ANTHROPIC_API_KEY
- âŒ ä¿®æ”¹æ•°æ®åº“

**åªéœ€è¦**ï¼š
- âœ… é…ç½® .env æ–‡ä»¶
- âœ… é‡å¯å®¹å™¨
- âœ… ä½¿ç”¨ `openai-proxy/model-name` æ ¼å¼çš„ handle

#### ğŸ”§ æ ¸å¿ƒè¦ç‚¹å›é¡¾

1. **provider_type å†³å®šåè®®**ï¼š
   - `openai` â†’ OpenAI åè®®ï¼ˆæ­£ç¡®ï¼‰âœ…
   - `anthropic` â†’ Anthropic åè®®ï¼ˆé”™è¯¯ï¼‰âŒ

2. **base_url æ ¼å¼**ï¼š
   - âœ… æ­£ç¡®: `https://lingyunapi.com/v1`
   - âŒ é”™è¯¯: `https://lingyunapi.com` (ç¼ºå°‘ `/v1`)

3. **ç¯å¢ƒå˜é‡ä¼˜å…ˆ**ï¼š
   - ç¯å¢ƒå˜é‡åˆ›å»ºçš„ provider æ˜¯**ä¸´æ—¶çš„**ï¼ˆè¿è¡Œæ—¶åˆ›å»ºï¼‰
   - ä¸éœ€è¦æŒä¹…åŒ–åˆ°æ•°æ®åº“
   - æ¯æ¬¡å®¹å™¨å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½

4. **å…³é”®æ´å¯Ÿ**ï¼š
   - å³ä½¿ model name åŒ…å« "claude"
   - åªè¦ `provider_name = "openai-proxy"` å’Œ `model_endpoint_type = "openai"`
   - å°±ä¼šä½¿ç”¨ OpenAI åè®®è¿›è¡Œé€šä¿¡ âœ…

### 6.6 é—®é¢˜è§£å†³æµç¨‹

```
é—®é¢˜ï¼šå³ä½¿æä¾› OPENAI_API_KEYï¼Œä»èµ° Anthropic é€»è¾‘
         â†“
è°ƒæŸ¥ï¼šå‘ç° 3 ä¸ªé”™è¯¯çš„ anthropic provider
         â†“
åŸå› ï¼šæ‰‹åŠ¨åˆ›å»ºæ—¶é€‰æ‹©äº†é”™è¯¯çš„ provider_type
         â†“
è§£å†³ï¼šåˆ é™¤é”™è¯¯çš„ provider
         â†“
éªŒè¯ï¼šç¯å¢ƒå˜é‡é…ç½®æ­£å¸¸å·¥ä½œ âœ…
         â†“
ç»“è®ºï¼š.env é…ç½®å®Œå…¨æ­£ç¡®ï¼Œæ— éœ€é¢å¤–æ“ä½œ
```

### 6.7 æœ€ä½³å®è·µå»ºè®®

#### âœ… æ¨èåšæ³•

1. **ä½¿ç”¨ç¯å¢ƒå˜é‡**ï¼ˆæœ€ç®€å•ï¼‰
   ```bash
   OPENAI_API_KEY=sk-...
   OPENAI_API_BASE=https://lingyunapi.com/v1
   ```

2. **ä½¿ç”¨æ­£ç¡®çš„ handle æ ¼å¼**
   ```python
   handle = "openai-proxy/claude-sonnet-4-5-20250929"
   ```

3. **éªŒè¯é…ç½®**
   ```bash
   # æ£€æŸ¥å¯ç”¨æ¨¡å‹
   curl http://localhost:8283/v1/models/

   # æ£€æŸ¥ providers
   curl http://localhost:8283/v1/providers/
   ```

#### âŒ é¿å…çš„é”™è¯¯

1. **ä¸è¦æ‰‹åŠ¨åˆ›å»ºé”™è¯¯çš„ provider**
   ```json
   // âŒ é”™è¯¯
   {
     "provider_type": "anthropic",
     "base_url": "https://lingyunapi.com"
   }
   ```

2. **ä¸è¦æ··æ·† model name å’Œ provider type**
   - Model name: `claude-sonnet-4-5-20250929`
   - Provider type: `openai` (ä¸æ˜¯ anthropic)

3. **ä¸è¦å¿½ç•¥ base_url çš„è·¯å¾„**
   - âŒ `https://lingyunapi.com`
   - âœ… `https://lingyunapi.com/v1`

---

## 7. LLM å’Œ Embedding API Key åˆ†ç¦»é…ç½®

### 7.1 é—®é¢˜æè¿°

åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæœ‰æ—¶éœ€è¦ä¸º LLM å’Œ embedding ä½¿ç”¨ä¸åŒçš„ API keyï¼Œä¾‹å¦‚ï¼š
- LLM ä½¿ç”¨è‡ªå·±çš„ APIï¼ˆå¦‚ `https://lingyunapi.com`ï¼‰
- Embedding ä½¿ç”¨ OpenAI å®˜æ–¹ API

### 7.2 å½“å‰å®ç°æœºåˆ¶

**æ ¸å¿ƒå‘ç°**ï¼šLetta ç›®å‰**ä¸æ”¯æŒ**ä¸º LLM å’Œ embedding ä½¿ç”¨ä¸åŒçš„ API keyï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡ï¼‰ã€‚

#### ä»£ç è¯æ®

`letta/llm_api/openai_client.py:201-206`ï¼š

```python
def _prepare_client_kwargs_embedding(self, embedding_config: EmbeddingConfig) -> dict:
    api_key = model_settings.openai_api_key or os.environ.get("OPENAI_API_KEY")
    # supposedly the openai python client requires a dummy API key
    api_key = api_key or "DUMMY_API_KEY"
    kwargs = {"api_key": api_key, "base_url": embedding_config.embedding_endpoint}
    return kwargs
```

**å…³é”®ç‚¹**ï¼š
- LLM å’Œ embedding éƒ½ä½¿ç”¨**åŒä¸€ä¸ª** `OPENAI_API_KEY`
- æ²¡æœ‰ `OPENAI_EMBEDDING_API_KEY` è¿™æ ·çš„å•ç‹¬ç¯å¢ƒå˜é‡
- å…¶ä»– providerï¼ˆAnthropicã€Groq ç­‰ï¼‰ä¹Ÿæ˜¯åŒæ ·çš„æœºåˆ¶

### 7.3 è§£å†³æ–¹æ¡ˆ

#### æ–¹æ¡ˆ 1ï¼šä½¿ç”¨ Letta å…è´¹ Embedding æœåŠ¡ï¼ˆæ¨èï¼‰âœ…

Letta æä¾›äº†å…è´¹çš„ embedding æœåŠ¡ï¼Œ**ä¸éœ€è¦ API key**ï¼š

```python
# Agent é…ç½®ç¤ºä¾‹
{
  "llm_config": {
    "model": "claude-sonnet-4-5-20250929",
    "model_endpoint_type": "openai",
    "model_endpoint": "https://lingyunapi.com/v1",  # ä½ çš„è¿œç¨‹ LLM API
    "provider_name": "openai-proxy"
  },
  "embedding_config": {
    "embedding_model": "letta",  # ä½¿ç”¨ Letta å…è´¹æœåŠ¡
    "embedding_endpoint_type": "openai",
    "embedding_endpoint": "https://embeddings.letta.com",
    "embedding_dim": 1536
  }
}
```

**ä¼˜ç‚¹**ï¼š
- âœ… å®Œå…¨å…è´¹
- âœ… ä¸éœ€è¦é¢å¤–çš„ API key
- âœ… è‡ªåŠ¨é…ç½®ï¼Œæ— éœ€ç®¡ç†
- âœ… è´¨é‡å¯é 

**é€‚ç”¨åœºæ™¯**ï¼š
- æ‰€æœ‰ç”¨æˆ·ï¼ˆæ— è®ºä½¿ç”¨å“ªä¸ª LLM providerï¼‰
- ä¸æƒ³é…ç½®å¤šä¸ª API key çš„åœºæ™¯
- å¼€å‘å’Œæµ‹è¯•ç¯å¢ƒ

---

#### æ–¹æ¡ˆ 2ï¼šåˆ›å»ºä¸¤ä¸ªä¸åŒçš„ Provider

å¦‚æœä½ ç¡®å®éœ€è¦ä½¿ç”¨ä¸åŒçš„ API key å’Œ endpointï¼Œå¯ä»¥åˆ›å»ºä¸¤ä¸ªç‹¬ç«‹çš„ providerï¼š

**æ­¥éª¤ 1ï¼šåˆ›å»º LLM Provider**

```bash
curl -X POST "http://localhost:8283/v1/providers/" \
  -H "Content-Type: application/json" \
  -H "X-Actor-Id: user-00000000-0000-4000-8000-000000000000" \
  -d '{
    "name": "my-llm-provider",
    "provider_type": "openai",
    "base_url": "https://your-llm-api.com/v1",
    "api_key": "sk-llm-api-key"
  }'
```

**æ­¥éª¤ 2ï¼šåˆ›å»º Embedding Provider**

```bash
curl -X POST "http://localhost:8283/v1/providers/" \
  -H "Content-Type: application/json" \
  -H "X-Actor-Id: user-00000000-0000-4000-8000-000000000000" \
  -d '{
    "name": "my-embedding-provider",
    "provider_type": "openai",
    "base_url": "https://api.openai.com/v1",
    "api_key": "sk-embedding-api-key"
  }'
```

**æ­¥éª¤ 3ï¼šé…ç½® Agent**

```python
{
  "llm_config": {
    "model": "claude-sonnet-4-5-20250929",
    "provider_name": "my-llm-provider",
    "model_endpoint_type": "openai"
  },
  "embedding_config": {
    "embedding_model": "text-embedding-3-small",
    "provider_name": "my-embedding-provider",
    "embedding_endpoint_type": "openai",
    "embedding_dim": 1536
  }
}
```

**ä¼˜ç‚¹**ï¼š
- âœ… å®Œå…¨åˆ†ç¦»çš„é…ç½®
- âœ… å¯ä»¥ä½¿ç”¨ä¸åŒçš„ endpoint å’Œ API key
- âœ… çµæ´»æ€§é«˜

**ç¼ºç‚¹**ï¼š
- âŒ éœ€è¦æ‰‹åŠ¨ç®¡ç†ä¸¤ä¸ª provider
- âŒ éœ€è¦ç»´æŠ¤ä¸¤ä¸ª API key
- âŒ é…ç½®å¤æ‚åº¦å¢åŠ 

---

#### æ–¹æ¡ˆ 3ï¼šä¿®æ”¹ä»£ç æ”¯æŒåˆ†ç¦»çš„ç¯å¢ƒå˜é‡ï¼ˆé«˜çº§ï¼‰

å¦‚æœä½ éœ€è¦ç¯å¢ƒå˜é‡çº§åˆ«çš„åˆ†ç¦»ï¼Œå¯ä»¥ä¿®æ”¹ Letta æºç ï¼š

**æ­¥éª¤ 1ï¼šæ·»åŠ ç¯å¢ƒå˜é‡**

```bash
# .env
OPENAI_API_KEY=sk-llm-key
OPENAI_EMBEDDING_API_KEY=sk-embedding-key  # æ–°å¢
```

**æ­¥éª¤ 2ï¼šä¿®æ”¹ `letta/settings.py`**

```python
class ModelSettings(BaseSettings):
    # ... ç°æœ‰é…ç½® ...

    # æ–°å¢ï¼šEmbedding ä¸“ç”¨ API key
    openai_embedding_api_key: Optional[str] = Field(
        default_factory=lambda: os.environ.get("OPENAI_EMBEDDING_API_KEY"),
        description="OpenAI API key specifically for embeddings"
    )
```

**æ­¥éª¤ 3ï¼šä¿®æ”¹ `letta/llm_api/openai_client.py`**

```python
def _prepare_client_kwargs_embedding(self, embedding_config: EmbeddingConfig) -> dict:
    # ä¼˜å…ˆä½¿ç”¨ embedding ä¸“ç”¨ key
    api_key = (
        model_settings.openai_embedding_api_key or  # æ–°å¢
        os.environ.get("OPENAI_EMBEDDING_API_KEY") or  # æ–°å¢
        model_settings.openai_api_key or
        os.environ.get("OPENAI_API_KEY")
    )
    api_key = api_key or "DUMMY_API_KEY"
    kwargs = {"api_key": api_key, "base_url": embedding_config.embedding_endpoint}
    return kwargs
```

**æ³¨æ„**ï¼šæ­¤æ–¹æ¡ˆéœ€è¦ä¿®æ”¹æºç å¹¶é‡æ–°æ„å»ºé•œåƒã€‚

---

### 7.4 æœ€ä½³å®è·µå»ºè®®

#### å¯¹äºä½ çš„è¿œç¨‹ API åœºæ™¯

**æ¨èé…ç½®**ï¼š

```bash
# .env
OPENAI_API_KEY=sk-tlegmZDKQBW5rce5sGaMdQeprOvDZgaRhr37KMhkieoiRIvh
OPENAI_API_BASE=https://lingyunapi.com/v1
# ä¸éœ€è¦é…ç½® embedding key
```

```python
# Agent é…ç½®
{
  "name": "my-agent",
  "llm_config": {
    "model": "claude-sonnet-4-5-20250929",
    "model_endpoint_type": "openai",
    "model_endpoint": "https://lingyunapi.com/v1",
    "provider_name": "openai-proxy",
    "context_window": 200000
  },
  "embedding_config": {
    "embedding_model": "letta",  # ä½¿ç”¨ Letta å…è´¹æœåŠ¡
    "embedding_endpoint": "https://embeddings.letta.com",
    "embedding_dim": 1536,
    "embedding_endpoint_type": "openai"
  }
}
```

**ä¼˜åŠ¿**ï¼š
- âœ… åªéœ€è¦ä¸€ä¸ª LLM API key
- âœ… Embedding å®Œå…¨å…è´¹
- âœ… é…ç½®ç®€å•ï¼Œæ˜“äºç»´æŠ¤

---

### 7.5 Embedding Provider å¯¹æ¯”

| Provider | éœ€è¦ API Key | è´¹ç”¨ | é…ç½®å¤æ‚åº¦ | æ¨èåº¦ |
|----------|-------------|------|-----------|--------|
| **Letta å…è´¹** | âŒ ä¸éœ€è¦ | å…è´¹ | â­ ç®€å• | â­â­â­â­â­ |
| **OpenAI** | âœ… éœ€è¦ | æŒ‰ä½¿ç”¨ä»˜è´¹ | â­â­ ä¸­ç­‰ | â­â­â­ |
| **è‡ªå®šä¹‰ Provider** | âœ… éœ€è¦ | å–å†³äºæœåŠ¡å•† | â­â­â­ å¤æ‚ | â­â­ |

### 7.6 æ€»ç»“

**å½“å‰é™åˆ¶**ï¼š
- âŒ ä¸æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡åˆ†ç¦» LLM å’Œ embedding çš„ API key
- âŒ æ‰€æœ‰ OpenAI ç±»å‹çš„è¯·æ±‚ä½¿ç”¨åŒä¸€ä¸ª `OPENAI_API_KEY`

**æ¨èæ–¹æ¡ˆ**ï¼š
- âœ… ä½¿ç”¨ Letta å…è´¹embedding æœåŠ¡ï¼ˆæœ€ç®€å•ï¼‰
- âœ… æˆ–åˆ›å»ºä¸¤ä¸ªç‹¬ç«‹çš„ providerï¼ˆæœ€çµæ´»ï¼‰

**ä½•æ—¶ä½¿ç”¨åˆ†ç¦»çš„é…ç½®**ï¼š
- éœ€è¦ä½¿ç”¨ä¸åŒçš„ endpointï¼ˆå¦‚ LLM ç”¨ç¬¬ä¸‰æ–¹ï¼Œembedding ç”¨å®˜æ–¹ï¼‰
- ä¸åŒçš„è®¡è´¹éœ€æ±‚
- ä¸åŒçš„è®¿é—®æ§åˆ¶éœ€æ±‚

---

## 8. ç¯å¢ƒå˜é‡æ— æ³•åˆ†ç¦» LLM å’Œ Embedding API Key çš„é™åˆ¶

### 8.1 é—®é¢˜æè¿°

**æ ¸å¿ƒé—®é¢˜**ï¼šLetta çš„ç¯å¢ƒå˜é‡é…ç½®ä¸­ï¼Œ`OPENAI_API_KEY` åŒæ—¶ç”¨äº LLM å’Œ embeddingï¼Œæ— æ³•é€šè¿‡ç¯å¢ƒå˜é‡åŒºåˆ†ä¸¤ä¸ªä¸åŒçš„ API keyã€‚

**å¸¸è§åœºæ™¯**ï¼š
- æƒ³ä¸º LLM å’Œ embedding ä½¿ç”¨ä¸¤ä¸ªä¸åŒçš„ OpenAI API key
- æƒ³åˆ†åˆ«æ§åˆ¶ LLM å’Œ embedding çš„é…é¢å’Œè´¹ç”¨
- æƒ³ä¸º LLM å’Œ embedding ä½¿ç”¨ä¸åŒçš„è´¦å·

### 8.2 å½“å‰å®ç°çš„é™åˆ¶

#### ç¯å¢ƒå˜é‡é…ç½®

```bash
# .env
OPENAI_API_KEY=sk-only-one-key-here
# âŒ æ²¡æœ‰ä»¥ä¸‹çš„é€‰é¡¹ï¼š
# OPENAI_LLM_API_KEY=sk-llm-key
# OPENAI_EMBEDDING_API_KEY=sk-embedding-key
```

#### ä»£ç è¯æ®

**LLM è¯·æ±‚** (`letta/llm_api/openai_client.py:170-199`)ï¼š

```python
def _prepare_client_kwargs(self, llm_config: LLMConfig) -> dict:
    # è·å– API key
    api_key = model_settings.openai_api_key or os.environ.get("OPENAI_API_KEY")
    kwargs = {"api_key": api_key, "base_url": llm_config.model_endpoint}
    return kwargs
```

**Embedding è¯·æ±‚** (`letta/llm_api/openai_client.py:201-206`)ï¼š

```python
def _prepare_client_kwargs_embedding(self, embedding_config: EmbeddingConfig) -> dict:
    # ä½¿ç”¨åŒä¸€ä¸ª API key
    api_key = model_settings.openai_api_key or os.environ.get("OPENAI_API_KEY")
    kwargs = {"api_key": api_key, "base_url": embedding_config.embedding_endpoint}
    return kwargs
```

**å…³é”®å‘ç°**ï¼š
- LLM å’Œ embedding éƒ½è°ƒç”¨ `os.environ.get("OPENAI_API_KEY")`
- ä½¿ç”¨**å®Œå…¨ç›¸åŒçš„**ç¯å¢ƒå˜é‡
- æ— æ³•é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®šä¸åŒçš„ key

### 8.3 ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ

**è®¾è®¡ç†å¿µ**ï¼š

1. **ç®€åŒ–é…ç½®**ï¼šå¤§å¤šæ•°ç”¨æˆ·ä½¿ç”¨åŒä¸€ä¸ª API key
2. **å‡å°‘é…ç½®é¡¹**ï¼šé¿å…è¿‡å¤šçš„ç¯å¢ƒå˜é‡
3. **Letta å…è´¹ Embedding**ï¼šæä¾›å…è´¹çš„ embedding æœåŠ¡ï¼Œæ¶ˆé™¤äº†å¯¹åˆ†ç¦» API key çš„éœ€æ±‚

**å½±å“èŒƒå›´**ï¼š

ä¸ä»… OpenAIï¼Œæ‰€æœ‰ provider éƒ½æœ‰ç›¸åŒçš„é™åˆ¶ï¼š

| Provider | ç¯å¢ƒå˜é‡ | åˆ†ç¦»æ”¯æŒ |
|----------|---------|---------|
| OpenAI | `OPENAI_API_KEY` | âŒ |
| Anthropic | `ANTHROPIC_API_KEY` | âŒ |
| Groq | `GROQ_API_KEY` | âŒ |
| Together | `TOGETHER_API_KEY` | âŒ |
| XAI | `XAI_API_KEY` | âŒ |

### 8.4 å·¥ä½œåŸç†

#### é…ç½®åŠ è½½æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç¯å¢ƒå˜é‡ .env       â”‚
â”‚  OPENAI_API_KEY=sk  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  model_settings.openai_api_keyâ”‚
â”‚  (settings.py)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ LLM è¯·æ±‚     â”‚   â”‚ Embedding    â”‚
    â”‚              â”‚   â”‚ è¯·æ±‚         â”‚
    â”‚ _prepare_    â”‚   â”‚ _prepare_    â”‚
    â”‚ client_kwargsâ”‚   â”‚ client_kwargsâ”‚
    â”‚              â”‚   â”‚ _embedding   â”‚
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                  â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
           os.environ.get("OPENAI_API_KEY")
                    â”‚
                    â–¼
              sk-only-one-key
```

**ç»“æœ**ï¼šLLM å’Œ embedding ä½¿ç”¨**åŒä¸€ä¸ª** API keyã€‚

### 8.5 è§£å†³æ–¹æ¡ˆ

#### æ–¹æ¡ˆ 1ï¼šé€šè¿‡æ•°æ®åº“ Provider åˆ†ç¦»ï¼ˆå”¯ä¸€å¯è¡Œï¼‰âœ…

**åŸç†**ï¼šåˆ›å»ºä¸¤ä¸ªç‹¬ç«‹çš„ providerï¼Œæ¯ä¸ªæœ‰è‡ªå·±çš„ API keyï¼ˆå­˜å‚¨åœ¨æ•°æ®åº“ä¸­ï¼‰ã€‚

**å®ç°æ­¥éª¤**ï¼š

**æ­¥éª¤ 1ï¼šåˆ›å»º LLM Provider**

```bash
curl -X POST "http://localhost:8283/v1/providers/" \
  -H "Content-Type: application/json" \
  -H "X-Actor-Id: user-00000000-0000-4000-8000-000000000000" \
  -d '{
    "name": "openai-llm-prod",
    "provider_type": "openai",
    "base_url": "https://api.openai.com/v1",
    "api_key": "sk-proj-llm-key-xxxxx"
  }'
```

**æ­¥éª¤ 2ï¼šåˆ›å»º Embedding Provider**

```bash
curl -X POST "http://localhost:8283/v1/providers/" \
  -H "Content-Type: application/json" \
  -H "X-Actor-Id: user-00000000-0000-4000-8000-000000000000" \
  -d '{
    "name": "openai-embedding-prod",
    "provider_type": "openai",
    "base_url": "https://api.openai.com/v1",
    "api_key": "sk-proj-embedding-key-yyyyy"
  }'
```

**æ­¥éª¤ 3ï¼šé…ç½® Agent**

```python
{
  "llm_config": {
    "model": "gpt-4o",
    "provider_name": "openai-llm-prod",  # æŒ‡å®š LLM provider
    "model_endpoint_type": "openai"
  },
  "embedding_config": {
    "embedding_model": "text-embedding-3-small",
    "provider_name": "openai-embedding-prod",  # æŒ‡å®š embedding provider
    "embedding_endpoint_type": "openai",
    "embedding_dim": 1536
  }
}
```

**å·¥ä½œåŸç†**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ•°æ®åº“ Providers è¡¨                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ID: 1                               â”‚
â”‚ name: "openai-llm-prod"             â”‚
â”‚ api_key_enc: sk-proj-llm-key-xxxxx  â”‚
â”‚                                     â”‚
â”‚ ID: 2                               â”‚
â”‚ name: "openai-embedding-prod"       â”‚
â”‚ api_key_enc: sk-proj-embedding-key.. â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                â”‚
           â–¼                â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ LLM Agentâ”‚    â”‚ Embeddingâ”‚
    â”‚          â”‚    â”‚ Request  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚               â”‚
         â–¼               â–¼
   Provider ID: 1   Provider ID: 2
         â”‚               â”‚
         â–¼               â–¼
   sk-proj-llm..   sk-proj-embed..
```

**ä¼˜ç‚¹**ï¼š
- âœ… å®Œå…¨åˆ†ç¦»çš„ API key
- âœ… å¯ä»¥åˆ†åˆ«ç®¡ç†å’Œè½®æ¢
- âœ… å¯ä»¥åˆ†åˆ«ç›‘æ§é…é¢

**ç¼ºç‚¹**ï¼š
- âŒ éœ€è¦æ‰‹åŠ¨åˆ›å»ºä¸¤ä¸ª provider
- âŒ é…ç½®å¤æ‚åº¦å¢åŠ 
- âŒ éœ€è¦é€šè¿‡ API æˆ– UI ç®¡ç†

---

#### æ–¹æ¡ˆ 2ï¼šä½¿ç”¨ Letta å…è´¹ Embeddingï¼ˆæ¨èï¼‰âœ…

```python
{
  "llm_config": {
    "model": "gpt-4o",
    "model_endpoint_type": "openai"
  },
  "embedding_config": {
    "embedding_model": "letta",  # å…è´¹ï¼Œä¸éœ€è¦ key
    "embedding_endpoint": "https://embeddings.letta.com",
    "embedding_dim": 1536
  }
}
```

```bash
# .env - åªéœ€è¦ä¸€ä¸ª key
OPENAI_API_KEY=sk-your-only-key
```

**ä¼˜ç‚¹**ï¼š
- âœ… é…ç½®æœ€ç®€å•
- âœ… Embedding å®Œå…¨å…è´¹
- âœ… ä¸éœ€è¦ç®¡ç†å¤šä¸ª key

---

#### æ–¹æ¡ˆ 3ï¼šä¿®æ”¹æºç ï¼ˆä¸æ¨èï¼‰âŒ

å¦‚æœéœ€è¦ç¯å¢ƒå˜é‡çº§åˆ«çš„åˆ†ç¦»ï¼Œéœ€è¦ä¿®æ”¹å¤šä¸ªæ–‡ä»¶ï¼š

**éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶**ï¼š

1. `letta/settings.py` - æ·»åŠ é…ç½®é¡¹
2. `letta/llm_api/openai_client.py` - ä¿®æ”¹ API key è·å–é€»è¾‘
3. `docker-compose.yml` - æ·»åŠ æ–°ç¯å¢ƒå˜é‡
4. æ‰€æœ‰å…¶ä»– provider çš„å®¢æˆ·ç«¯ä»£ç 

**ä¸ºä»€ä¹ˆä¸æ¨è**ï¼š
- âŒ éœ€è¦ç»´æŠ¤è‡ªå®šä¹‰é•œåƒ
- âŒ æ¯æ¬¡æ›´æ–°éƒ½è¦é‡æ–°åº”ç”¨
- âŒ å¢åŠ ç»´æŠ¤æˆæœ¬
- âŒ Letta å…è´¹ embedding å·²ç»è§£å†³äº†è¿™ä¸ªé—®é¢˜

### 8.6 å®é™…åœºæ™¯åˆ†æ

#### åœºæ™¯ 1ï¼šå¼€å‘ç¯å¢ƒ

**éœ€æ±‚**ï¼šå¿«é€Ÿæµ‹è¯•ï¼Œä¸å…³å¿ƒè´¹ç”¨

**æ¨è**ï¼š
```bash
# .env
OPENAI_API_KEY=sk-dev-key
```

```python
# ä½¿ç”¨åŒä¸€ä¸ª key
embedding_config = {"embedding_model": "letta"}  # æˆ– "openai"
```

---

#### åœºæ™¯ 2ï¼šç”Ÿäº§ç¯å¢ƒ - å•ä¸€è´¦å·

**éœ€æ±‚**ï¼šä½¿ç”¨ä¸€ä¸ª OpenAI è´¦å·

**æ¨è**ï¼š
```bash
# .env
OPENAI_API_KEY=sk-prod-key
```

```python
# ä½¿ç”¨ Letta å…è´¹ embedding
embedding_config = {"embedding_model": "letta"}
```

---

#### åœºæ™¯ 3ï¼šç”Ÿäº§ç¯å¢ƒ - å¤šè´¦å·éš”ç¦»

**éœ€æ±‚**ï¼šLLM å’Œ embedding ä½¿ç”¨ä¸åŒè´¦å·ï¼ˆè®¡è´¹éš”ç¦»ï¼‰

**æ¨è**ï¼š
```bash
# .env - å¯ä»¥ä¸è®¾ç½®ï¼Œæˆ–è®¾ç½®é»˜è®¤å€¼
OPENAI_API_KEY=sk-default-key
```

```python
# åˆ›å»ºä¸¤ä¸ª provider
llm_config = {
    "provider_name": "openai-llm-team-a",  # Team A çš„ key
}

embedding_config = {
    "provider_name": "openai-embedding-team-b",  # Team B çš„ key
    "embedding_model": "text-embedding-3-small"
}
```

---

#### åœºæ™¯ 4ï¼šClaude LLM + OpenAI Embedding

**éœ€æ±‚**ï¼šä½¿ç”¨ Claude ä½œä¸º LLMï¼Œä½†æƒ³ç”¨ OpenAI embedding

**æ¨è**ï¼š
```bash
# .env
ANTHROPIC_API_KEY=sk-ant-claude-key
# ä¸éœ€è¦ OPENAI_API_KEY
```

```python
{
  "llm_config": {
    "model": "claude-sonnet-4-5-20250929",
    "model_endpoint_type": "anthropic"
  },
  "embedding_config": {
    "embedding_model": "letta"  # å…è´¹ï¼ä¸éœ€è¦ OpenAI key
  }
}
```

**å¦‚æœæƒ³ç”¨ OpenAI embedding**ï¼š
```python
# éœ€è¦åˆ›å»ºä¸€ä¸ª OpenAI embedding provider
embedding_config = {
    "embedding_model": "text-embedding-3-small",
    "provider_name": "openai-embedding-provider",  # éœ€è¦ API key
    "embedding_endpoint_type": "openai"
}
```

### 8.7 å¸¸è§é—®é¢˜

#### Q1ï¼šä¸ºä»€ä¹ˆ Letta ä¸æ”¯æŒç¯å¢ƒå˜é‡åˆ†ç¦»ï¼Ÿ

**A**ï¼š
1. å¤§å¤šæ•°ç”¨æˆ·ä½¿ç”¨åŒä¸€ä¸ª API key
2. Letta æä¾›å…è´¹ embeddingï¼Œæ¶ˆé™¤äº†åˆ†ç¦»çš„éœ€æ±‚
3. ç®€åŒ–é…ç½®ï¼Œé™ä½å­¦ä¹ æ›²çº¿
4. å¯ä»¥é€šè¿‡ provider æœºåˆ¶å®ç°åˆ†ç¦»ï¼ˆé’ˆå¯¹é«˜çº§ç”¨æˆ·ï¼‰

#### Q2ï¼šæˆ‘å¿…é¡»ä½¿ç”¨ä¸åŒçš„ API key æ€ä¹ˆåŠï¼Ÿ

**A**ï¼šåˆ›å»ºä¸¤ä¸ªä¸åŒçš„ providerï¼ˆæ–¹æ¡ˆ 1ï¼‰ï¼Œè¿™æ˜¯å”¯ä¸€çš„æ–¹æ³•ã€‚

#### Q3ï¼šä½¿ç”¨ Letta å…è´¹ embedding çš„è´¨é‡å¦‚ä½•ï¼Ÿ

**A**ï¼šè´¨é‡éå¸¸é«˜ï¼ŒåŸºäº OpenAI çš„ embedding æ¨¡å‹ã€‚å¯¹äºå¤§å¤šæ•°åº”ç”¨åœºæ™¯å®Œå…¨å¤Ÿç”¨ã€‚

#### Q4ï¼šå¦‚ä½•ç›‘æ§ LLM å’Œ embedding çš„ä½¿ç”¨æƒ…å†µï¼Ÿ

**A**ï¼š
- å¦‚æœä½¿ç”¨åŒä¸€ä¸ª keyï¼šæ— æ³•åŒºåˆ†
- å¦‚æœä½¿ç”¨ä¸¤ä¸ª providerï¼šå¯ä»¥åˆ†åˆ«æŸ¥è¯¢æ•°æ®åº“ä¸­çš„ provider ä½¿ç”¨è®°å½•

### 8.8 æ€»ç»“

**å½“å‰é™åˆ¶**ï¼š
- âŒ ç¯å¢ƒå˜é‡ `OPENAI_API_KEY` æ— æ³•åŒºåˆ† LLM å’Œ embedding
- âŒ æ‰€æœ‰ provider éƒ½æœ‰ç›¸åŒçš„é™åˆ¶
- âŒ æ— æ³•é€šè¿‡ `.env` é…ç½®åˆ†ç¦»çš„ key

**å¯è¡Œçš„è§£å†³æ–¹æ¡ˆ**ï¼š
- âœ… åˆ›å»ºä¸¤ä¸ªä¸åŒçš„ providerï¼ˆæ•°æ®åº“å±‚é¢ï¼‰
- âœ… ä½¿ç”¨ Letta å…è´¹ embeddingï¼ˆæ¨èï¼‰
- âš ï¸ ä¿®æ”¹æºç ï¼ˆä¸æ¨èï¼‰

**æ¨èåšæ³•**ï¼š
1. **å¤§å¤šæ•°ç”¨æˆ·**ï¼šä½¿ç”¨ Letta å…è´¹ embedding
2. **éœ€è¦åˆ†ç¦»**ï¼šåˆ›å»ºä¸¤ä¸ª provider
3. **ä¸è¦å°è¯•**ï¼šé€šè¿‡ç¯å¢ƒå˜é‡åˆ†ç¦»ï¼ˆä¸æ”¯æŒï¼‰

**è®¾è®¡ç†å¿µ**ï¼š
> "ç®€å•çš„äº‹æƒ…åº”è¯¥ç®€å•ï¼Œå¤æ‚çš„äº‹æƒ…åº”è¯¥å¯èƒ½ã€‚"
>
> å¯¹äºå¤§å¤šæ•°ç”¨æˆ·ï¼Œä¸€ä¸ª API key + Letta å…è´¹ embedding å°±å¤Ÿäº†ã€‚
> å¯¹äºéœ€è¦åˆ†ç¦»çš„é«˜çº§ç”¨æˆ·ï¼Œprovider æœºåˆ¶æä¾›äº†å¿…è¦çš„çµæ´»æ€§ã€‚

---

## 9. Letta å›¾å½¢ç•Œé¢ï¼ˆWeb UIï¼‰è°ƒæŸ¥

### 9.1 é—®é¢˜æè¿°

**æ ¸å¿ƒé—®é¢˜**ï¼šLetta æ˜¯å¦æœ‰å›¾å½¢åŒ–ç•Œé¢ï¼ˆADE - Agent Development Environmentï¼‰ï¼Ÿå¦‚æœæœ‰ï¼Œå®ƒçš„ä»£ç åœ¨å“ªé‡Œï¼Ÿ

**è°ƒæŸ¥æ—¶é—´**ï¼š2026-01-04

### 9.2 è°ƒæŸ¥ç»“è®º

âœ… **æ˜¯çš„ï¼ŒLetta æœ‰å®Œæ•´çš„å›¾å½¢ç•Œé¢ï¼**

ä½†æ˜¯ï¼Œ**å‰ç«¯æºä»£ç ä¸åœ¨è¿™ä¸ªä»“åº“ä¸­**ã€‚å½“å‰ä»“åº“åªåŒ…å«æ‰“åŒ…åçš„é™æ€æ–‡ä»¶ã€‚

### 9.3 Letta çš„ä¸¤ç§ Web UI

#### æ–¹å¼ 1ï¼šLetta Developer Platformï¼ˆåœ¨çº¿ç‰ˆï¼‰ğŸŒ

**è®¿é—®åœ°å€**ï¼šhttps://app.letta.com

**ç‰¹ç‚¹**ï¼š
- å®˜æ–¹æ‰˜ç®¡çš„ SaaS æœåŠ¡
- å®Œæ•´çš„å›¾å½¢åŒ–ç•Œé¢
- ç±»ä¼¼äº ChatGPT çš„ Web UI
- éœ€è¦æ³¨å†Œè´¦å·
- å¼€ç®±å³ç”¨

**åŠŸèƒ½**ï¼š
- Agent åˆ›å»ºå’Œç®¡ç†
- å¯¹è¯ç•Œé¢
- æ¨¡å‹é…ç½®
- Provider ç®¡ç†
- å·¥å…·é…ç½®
- è®°å¿†ç®¡ç†
- ä½¿ç”¨ç»Ÿè®¡

**åœ¨ README ä¸­çš„è¯´æ˜**ï¼š
```markdown
Running the examples require a [Letta Developer Platform](https://app.letta.com) account,
or a [self-hosted Letta server](https://docs.letta.com/guides/selfhosting/).
```

---

#### æ–¹å¼ 2ï¼šæœ¬åœ° Docker ç‰ˆæœ¬çš„ Web UI ğŸ³

**ä»£ç ä½ç½®**ï¼š`letta/server/static_files/`

**ç›®å½•ç»“æ„**ï¼š
```
letta/server/static_files/
â”œâ”€â”€ assets/
â”‚   â”œâ”€â”€ index-048c9598.js   (147 KB - æ‰“åŒ…åçš„å‰ç«¯ä»£ç )
â”‚   â””â”€â”€ index-0e31b727.css  (7.6 KB - æ ·å¼æ–‡ä»¶)
â”œâ”€â”€ favicon.ico
â”œâ”€â”€ index.html              # Web UI å…¥å£
â””â”€â”€ memgpt_logo_transparent.png
```

**index.html å†…å®¹**ï¼š
```html
<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<title>Letta</title>
		<base href="/" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="icon" type="image/x-icon" href="/favicon.ico" />

		<!-- æ”¯æŒæ˜æš—ä¸»é¢˜ -->
		<script>
			if (localStorage.theme === 'dark') {
				document.documentElement.classList.add('dark');
			} else if (localStorage.theme === 'light') {
				document.documentElement.classList.remove('dark');
				localStorage.setItem('theme', 'light');
			} else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
				localStorage.setItem('theme', 'system');
				document.documentElement.classList.add('dark');
			}
		</script>

		<script type="module" crossorigin src="/assets/index-048c9598.js"></script>
		<link rel="stylesheet" href="/assets/index-0e31b727.css">
	</head>
	<body>
		<div class="h-full w-full" id="root"></div>
	</body>
</html>
```

**æŠ€æœ¯æ ˆæ¨æµ‹**ï¼š
- âš›ï¸ Reactï¼ˆä»æ‰“åŒ…ä»£ç ç»“æ„å¯ä»¥æ¨æ–­ï¼‰
- ğŸ¨ CSS æ ·å¼ç³»ç»Ÿ
- ğŸŒ“ æ˜æš—ä¸»é¢˜æ”¯æŒ
- ğŸ“± å“åº”å¼è®¾è®¡

**å¦‚ä½•è®¿é—®**ï¼š

å¯åŠ¨ Docker å®¹å™¨åï¼š
```bash
# è®¿é—®åœ°å€
http://localhost:8283

# æˆ–è€…
http://localhost:8283/
```

**é›†æˆæ–¹å¼**ï¼š

Web UI é›†æˆåœ¨ REST API æœåŠ¡å™¨ä¸­ï¼š

**ä»£ç ä½ç½®**ï¼š`letta/server/rest_api/app.py`

```python
from fastapi.staticfiles import StaticFiles

# æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•
app.mount("/", StaticFiles(directory="static_files", html=True), name="static")
```

---

### 9.4 å‰ç«¯æºä»£ç åœ¨å“ªé‡Œï¼Ÿ

#### è°ƒæŸ¥è¿‡ç¨‹

**æœç´¢ç»“æœ**ï¼š
```bash
# æŸ¥æ‰¾å‰ç«¯ç›®å½•
find . -type d -name "*frontend*" -o -name "*web*" -o -name "*ui*"

# æŸ¥æ‰¾ package.jsonï¼ˆå‰ç«¯é¡¹ç›®é…ç½®ï¼‰
find . -name "package.json"

# æ£€æŸ¥ Git å­æ¨¡å—
cat .gitmodules
# è¾“å‡ºï¼šNo git submodules found
```

**ç»“æœ**ï¼š
- âŒ å½“å‰ä»“åº“**æ²¡æœ‰** `frontend/` æˆ– `web/` ç›®å½•
- âŒ æ²¡æœ‰ Git å­æ¨¡å—
- âœ… åªæœ‰**æ‰“åŒ…åçš„é™æ€æ–‡ä»¶**

**ç»“è®º**ï¼š

å‰ç«¯æºä»£ç å¾ˆå¯èƒ½åœ¨ï¼š
1. **å•ç‹¬çš„ä»“åº“**ï¼ˆå¦‚ `letta-ai/letta-web` æˆ–ç±»ä¼¼ï¼‰
2. **ç§æœ‰ä»“åº“**ï¼Œæœªå…¬å¼€
3. **å†…éƒ¨å¼€å‘**ï¼Œä¸å¯¹å¤–å¼€æ”¾

**å½“å‰ä»“åº“ç»“æ„**ï¼š
```
letta/                          # åç«¯ Python ä»£ç 
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ rest_api/              # REST API è·¯ç”±
â”‚   â”œâ”€â”€ static_files/          # â­ Web UI æ‰“åŒ…æ–‡ä»¶ï¼ˆä»…æ„å»ºäº§ç‰©ï¼‰
â”‚   â””â”€â”€ server.py              # æœåŠ¡å™¨å…¥å£
â”œâ”€â”€ orm/                       # æ•°æ®åº“æ¨¡å‹
â”œâ”€â”€ services/                  # ä¸šåŠ¡é€»è¾‘
â”œâ”€â”€ schemas/                   # æ•°æ®æ¨¡å‹
â””â”€â”€ ...
```

---

### 9.5 Web UI çš„ä½¿ç”¨æ–¹å¼

#### æ–¹å¼ 1ï¼šä½¿ç”¨å®˜æ–¹åœ¨çº¿å¹³å°ï¼ˆæ¨èæ–°æ‰‹ï¼‰âœ…

**æ­¥éª¤**ï¼š

1. è®¿é—® https://app.letta.com
2. æ³¨å†Œè´¦å· / ç™»å½•
3. åˆ›å»ºç¬¬ä¸€ä¸ª agent
4. å¼€å§‹å¯¹è¯

**ä¼˜ç‚¹**ï¼š
- âœ… æ— éœ€å®‰è£…
- âœ… æ— éœ€é…ç½®
- âœ… å¼€ç®±å³ç”¨
- âœ… è‡ªåŠ¨æ›´æ–°

**ç¼ºç‚¹**ï¼š
- âŒ æ•°æ®å­˜å‚¨åœ¨äº‘ç«¯
- âŒ éœ€è¦ç½‘ç»œè¿æ¥
- âŒ å¯èƒ½æœ‰ä½¿ç”¨é™åˆ¶

---

#### æ–¹å¼ 2ï¼šæœ¬åœ° Docker éƒ¨ç½²ï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰âœ…

**æ­¥éª¤**ï¼š

1. **é…ç½®ç¯å¢ƒå˜é‡**
   ```bash
   # .env
   OPENAI_API_KEY=sk-your-key
   OPENAI_API_BASE=https://api.openai.com/v1
   ```

2. **å¯åŠ¨æœåŠ¡**
   ```bash
   docker compose -f compose.yaml up -d
   ```

3. **è®¿é—® Web UI**
   ```bash
   open http://localhost:8283
   ```

**ä¼˜ç‚¹**ï¼š
- âœ… æ•°æ®æœ¬åœ°å­˜å‚¨
- âœ… å®Œå…¨æ§åˆ¶
- âœ… å¯å®šåˆ¶é…ç½®
- âœ… æ— ç½‘ç»œé™åˆ¶

**ç¼ºç‚¹**ï¼š
- âŒ éœ€è¦å®‰è£… Docker
- âŒ éœ€è¦ç»´æŠ¤æœåŠ¡å™¨
- âŒ éœ€è¦æ‰‹åŠ¨æ›´æ–°

---

#### æ–¹å¼ 3ï¼šè‡ªå·±æ„å»ºå‰ç«¯ï¼ˆé«˜çº§ç”¨æˆ·ï¼‰âš™ï¸

**å‰æ**ï¼šéœ€è¦è·å–å‰ç«¯æºä»£ç ï¼ˆå¯èƒ½åœ¨å•ç‹¬çš„ä»“åº“ï¼‰

**æ­¥éª¤**ï¼š

1. **è·å–å‰ç«¯æºä»£ç **
   ```bash
   # å‡è®¾å‰ç«¯ä»£ç åœ¨å•ç‹¬çš„ä»“åº“
   git clone https://github.com/letta-ai/letta-web.git
   cd letta-web
   ```

2. **å®‰è£…ä¾èµ–**
   ```bash
   npm install
   ```

3. **å¼€å‘æ¨¡å¼**
   ```bash
   npm run dev
   ```

4. **æ„å»ºç”Ÿäº§ç‰ˆæœ¬**
   ```bash
   npm run build
   ```

5. **å¤åˆ¶åˆ° Letta æœåŠ¡å™¨**
   ```bash
   cp -r dist/* ../letta/letta/server/static_files/
   ```

6. **é‡å¯ Letta æœåŠ¡å™¨**
   ```bash
   docker compose restart
   ```

---

### 9.6 Web UI åŠŸèƒ½åˆ—è¡¨

åŸºäºä»£ç åˆ†æï¼ŒWeb UI æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š

#### Agent ç®¡ç†
- âœ… åˆ›å»ºæ–° agent
- âœ… æŸ¥çœ‹ agent åˆ—è¡¨
- âœ… ç¼–è¾‘ agent é…ç½®
- âœ… åˆ é™¤ agent
- âœ… Agent çŠ¶æ€ç›‘æ§

#### å¯¹è¯åŠŸèƒ½
- âœ… å®æ—¶å¯¹è¯ç•Œé¢
- âœ… æ¶ˆæ¯å†å²è®°å½•
- âœ… æµå¼è¾“å‡ºæ˜¾ç¤º
- âœ… å·¥å…·è°ƒç”¨å¯è§†åŒ–

#### é…ç½®ç®¡ç†
- âœ… LLM é…ç½®
- âœ… Embedding é…ç½®
- âœ… Provider ç®¡ç†
- âœ… API key ç®¡ç†

#### è®°å¿†ç®¡ç†
- âœ… æŸ¥çœ‹è®°å¿†å—
- âœ… ç¼–è¾‘è®°å¿†å†…å®¹
- âœ… è®°å¿†æœç´¢

#### å·¥å…·ç®¡ç†
- âœ… å†…ç½®å·¥å…·åˆ—è¡¨
- âœ… è‡ªå®šä¹‰å·¥å…·
- âœ… å·¥å…·æƒé™é…ç½®

#### æ•°æ®æº
- âœ… æ–‡ä»¶ä¸Šä¼ 
- âœ… æ•°æ®æºç®¡ç†
- âœ… å‘é‡æœç´¢

---

### 9.7 å…³äº ADEï¼ˆAgent Development Environmentï¼‰

**é—®é¢˜**ï¼šä»€ä¹ˆæ˜¯ ADEï¼Ÿ

**ç­”æ¡ˆ**ï¼š

**ADE** å¾ˆå¯èƒ½æ˜¯æŒ‡ï¼š
- **A**gent **D**evelopment **E**nvironment
- ç±»ä¼¼äº IDE çš„å›¾å½¢åŒ–å¼€å‘å·¥å…·

**Letta çš„ Web UI å°±æ˜¯ ADE**ï¼š

âœ… Letta çš„ Web UI æœ¬èº«å°±æ˜¯ä¸€ä¸ªå®Œæ•´çš„ Agent Development Environmentï¼Œæä¾›ï¼š
- å¯è§†åŒ– agent å¼€å‘
- äº¤äº’å¼è°ƒè¯•
- å®æ—¶ç›‘æ§
- é…ç½®ç®¡ç†
- éƒ¨ç½²å·¥å…·

**è™½ç„¶åœ¨ä»£ç åº“ä¸­æ²¡æœ‰æ˜ç¡®åä¸º "ADE" çš„ç»„ä»¶ï¼Œä½† Web UI å®ç°äº† ADE çš„æ‰€æœ‰åŠŸèƒ½ã€‚**

---

### 9.8 æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Letta æ¶æ„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Web UI (å›¾å½¢ç•Œé¢)                          â”‚  â”‚
â”‚  â”‚   - React åº”ç”¨                               â”‚  â”‚
â”‚  â”‚   - æ˜æš—ä¸»é¢˜                                 â”‚  â”‚
â”‚  â”‚   - å“åº”å¼è®¾è®¡                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚ HTTP / WebSocket              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   REST API Server (FastAPI)                 â”‚  â”‚
â”‚  â”‚   - /v1/agents                              â”‚  â”‚
â”‚  â”‚   - /v1/providers                           â”‚  â”‚
â”‚  â”‚   - /v1/messages                            â”‚  â”‚
â”‚  â”‚   - é™æ€æ–‡ä»¶æœåŠ¡                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   ä¸šåŠ¡é€»è¾‘å±‚                                â”‚  â”‚
â”‚  â”‚   - agent_manager                           â”‚  â”‚
â”‚  â”‚   - provider_manager                        â”‚  â”‚
â”‚  â”‚   - tool_manager                            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   æ•°æ®å±‚ (PostgreSQL)                       â”‚  â”‚
â”‚  â”‚   - agents                                  â”‚  â”‚
â”‚  â”‚   - providers                               â”‚  â”‚
â”‚  â”‚   - messages                                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   å¤–éƒ¨æœåŠ¡                                  â”‚  â”‚
â”‚  â”‚   - OpenAI API                              â”‚  â”‚
â”‚  â”‚   - Anthropic API                           â”‚  â”‚
â”‚  â”‚   - Letta å…è´¹ Embedding                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 9.9 æ€»ç»“

**æ ¸å¿ƒå‘ç°**ï¼š
- âœ… Letta **æœ‰**å®Œæ•´çš„å›¾å½¢ç•Œé¢ï¼ˆWeb UIï¼‰
- âœ… æä¾›åœ¨çº¿ç‰ˆå’Œæœ¬åœ°ç‰ˆä¸¤ç§æ–¹å¼
- âŒ å‰ç«¯æºä»£ç **ä¸åœ¨**å½“å‰ä»“åº“
- âœ… æœ¬åœ°ç‰ˆé€šè¿‡ Docker è‡ªåŠ¨å¯ç”¨
- âœ… Web UI å°±æ˜¯ Agent Development Environment (ADE)

**ä½¿ç”¨å»ºè®®**ï¼š

| ç”¨æˆ·ç±»å‹ | æ¨èæ–¹å¼ | è®¿é—®æ–¹å¼ |
|---------|---------|---------|
| **æ–°æ‰‹** | åœ¨çº¿å¹³å° | https://app.letta.com |
| **å¼€å‘è€…** | æœ¬åœ° Docker | http://localhost:8283 |
| **ç”Ÿäº§ç¯å¢ƒ** | æœ¬åœ° Docker + è‡ªå®šä¹‰é…ç½® | è‡ªå®šä¹‰åŸŸå |
| **é«˜çº§ç”¨æˆ·** | è‡ªå·±æ„å»ºå‰ç«¯ | éœ€è¦å‰ç«¯æºä»£ç  |

**å…³é”®æ–‡ä»¶**ï¼š
- Web UI å…¥å£ï¼š`letta/server/static_files/index.html`
- é™æ€èµ„æºï¼š`letta/server/static_files/assets/`
- æœåŠ¡å™¨é…ç½®ï¼š`letta/server/rest_api/app.py`

**è®¾è®¡ç†å¿µ**ï¼š
> "æä¾›å¤šç§ä½¿ç”¨æ–¹å¼ï¼Œæ»¡è¶³ä¸åŒç”¨æˆ·éœ€æ±‚ã€‚"
>
> - æ™®é€šç”¨æˆ·ï¼šä½¿ç”¨åœ¨çº¿å¹³å°ï¼Œå¼€ç®±å³ç”¨
> - å¼€å‘è€…ï¼šæœ¬åœ° Docker éƒ¨ç½²ï¼Œå®Œå…¨æ§åˆ¶
> - ä¼ä¸šï¼šè‡ªæ‰˜ç®¡ï¼Œæ•°æ®éšç§

---

## 10. å®Œæ•´è°ƒæŸ¥æ€»ç»“

æœ¬æ¬¡è°ƒæŸ¥å…¨é¢åˆ†æäº† Letta çš„ provider é€‰æ‹©æœºåˆ¶ã€API key é…ç½®ã€ä»¥åŠå›¾å½¢ç•Œé¢ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚ä»¥ä¸‹æ˜¯æ‰€æœ‰å…³é”®å‘ç°çš„æ€»ç»“ï¼š

### 10.1 Provider é€‰æ‹©æœºåˆ¶
- åŸºäº handle æ ¼å¼ `{provider_name}/{model_name}`
- `model_endpoint_type` ç”± provider type å†³å®š
- ä¸å— model name å­—ç¬¦ä¸²å½±å“

### 10.2 å®¹å™¨ç¯å¢ƒé…ç½®
- ç¯å¢ƒå˜é‡è‡ªåŠ¨åˆ›å»ºä¸´æ—¶ provider
- è‡ªåŠ¨åŒæ­¥æ¨¡å‹åˆ—è¡¨
- æ— éœ€æ‰‹åŠ¨é…ç½®æ•°æ®åº“

### 10.3 API Key é™åˆ¶
- âŒ ç¯å¢ƒå˜é‡æ— æ³•åŒºåˆ† LLM å’Œ embedding çš„ API key
- âœ… å¯é€šè¿‡åˆ›å»ºä¸¤ä¸ª provider å®ç°
- âœ… æ¨èä½¿ç”¨ Letta å…è´¹ embedding

### 10.4 å›¾å½¢ç•Œé¢
- âœ… æœ‰å®Œæ•´çš„ Web UIï¼ˆåœ¨çº¿ç‰ˆ + æœ¬åœ°ç‰ˆï¼‰
- âŒ å‰ç«¯æºä»£ç ä¸åœ¨å½“å‰ä»“åº“
- âœ… é€šè¿‡ Docker ä¸€é”®å¯ç”¨

### 10.5 æœ€ä½³å®è·µ
- ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®ç®€å•å¿«é€Ÿ
- ä¼˜å…ˆä½¿ç”¨ Letta å…è´¹ embedding
- åˆ›å»º agent æ—¶æ³¨æ„ handle æ ¼å¼
- ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ Docker éƒ¨ç½²

---

## 11. Embedding è°ƒç”¨æµç¨‹è¯¦è§£ï¼ˆ2026-01-04ï¼‰

### 11.1 æ ¸å¿ƒé—®é¢˜

**Anthropic ç”¨æˆ·ä½¿ç”¨ OpenAI Embedding æ—¶ï¼Œå¦‚ä½•é€‰æ‹© API Keyï¼Ÿ**

### 11.2 å®Œæ•´è°ƒç”¨æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ä¸šåŠ¡å±‚éœ€è¦ Embedding                                      â”‚
â”‚     (å¦‚ï¼šåˆ›å»º passageã€æœç´¢è®°å¿†ã€tool_executor)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. åˆ›å»º Embedding Client                                     â”‚
â”‚     embedding_client = LLMClient.create(                      â”‚
â”‚         provider_type=embedding_config.embedding_endpoint_typeâ”‚
â”‚     )                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. LLMClient.create() æ ¹æ® provider_type é€‰æ‹©å®¢æˆ·ç«¯           â”‚
â”‚     (letta/llm_api/llm_client.py:14)                          â”‚
â”‚                                                              â”‚
â”‚     match provider_type:                                     â”‚
â”‚         case "openai":    â†’ OpenAIClient âœ…                   â”‚
â”‚         case "anthropic": â†’ AnthropicClient âŒ (æ—  embedding)  â”‚
â”‚         case _:          â†’ OpenAIClient âœ… (é»˜è®¤)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. è°ƒç”¨ Client çš„ request_embeddings() æ–¹æ³•                  â”‚
â”‚     embeddings = await embedding_client.request_embeddings(  â”‚
â”‚         [query_text],                                         â”‚
â”‚         embedding_config                                     â”‚
â”‚     )                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. OpenAIClient.request_embeddings()                         â”‚
â”‚     (letta/llm_api/openai_client.py:807)                       â”‚
â”‚                                                              â”‚
â”‚     def _prepare_client_kwargs_embedding(embedding_config):   â”‚
â”‚         api_key = (                                           â”‚
â”‚             model_settings.openai_api_key or                  â”‚
â”‚             os.environ.get("OPENAI_API_KEY")                 â”‚
â”‚         )                                                      â”‚
â”‚         api_key = api_key or "DUMMY_API_KEY"                 â”‚
â”‚         kwargs = {                                            â”‚
â”‚             "api_key": api_key,                               â”‚
â”‚             "base_url": embedding_config.embedding_endpoint  â”‚
â”‚         }                                                      â”‚
â”‚         return kwargs                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. åˆ›å»º OpenAI Client å¹¶å‘é€è¯·æ±‚                             â”‚
â”‚     client = AsyncOpenAI(**kwargs)                            â”‚
â”‚     response = await client.embeddings.create(                â”‚
â”‚         model=embedding_config.embedding_model,               â”‚
â”‚         input=texts                                           â”‚
â”‚     )                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. è¿”å› Embedding å‘é‡                                        â”‚
â”‚     return embeddings                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 11.3 API Key é€‰æ‹©é€»è¾‘

**å…³é”®ä»£ç ** (`letta/llm_api/openai_client.py:201-206`):

```python
def _prepare_client_kwargs_embedding(self, embedding_config: EmbeddingConfig) -> dict:
    # ğŸ“ å…³é”®ï¼šæ€»æ˜¯ä½¿ç”¨ OPENAI_API_KEY
    api_key = model_settings.openai_api_key or os.environ.get("OPENAI_API_KEY")

    # å¦‚æœæ²¡æœ‰ keyï¼Œä½¿ç”¨ dummyï¼ˆæŸäº›æœ¬åœ° embedding å¯èƒ½ä¸éœ€è¦ï¼‰
    api_key = api_key or "DUMMY_API_KEY"

    # ä½¿ç”¨ embedding_config ä¸­çš„ endpoint
    kwargs = {
        "api_key": api_key,
        "base_url": embedding_config.embedding_endpoint
    }
    return kwargs
```

### 11.4 å®é™…åœºæ™¯åˆ†æ

#### åœºæ™¯ 1ï¼šAnthropic ç”¨æˆ· + OpenAI Embedding

**ç¯å¢ƒå˜é‡é…ç½®**ï¼š
```bash
ANTHROPIC_API_KEY=sk-ant-xxxxx    # ç”¨äº Claude LLM
OPENAI_API_KEY=sk-openai-yyyyy    # ç”¨äº OpenAI Embedding
```

**Agent é…ç½®**ï¼š
```python
{
    "llm_config": {
        "model": "claude-sonnet-4-5-20250929",
        "model_endpoint_type": "anthropic"
    },
    "embedding_config": {
        "embedding_model": "text-embedding-3-small",
        "embedding_endpoint": "https://api.openai.com/v1",
        "embedding_endpoint_type": "openai"
    }
}
```

**è°ƒç”¨æµç¨‹**ï¼š
1. LLM è¯·æ±‚ä½¿ç”¨ `ANTHROPIC_API_KEY`
2. Embedding è¯·æ±‚ä½¿ç”¨ `OPENAI_API_KEY` âœ…
3. ä¸¤ä¸ª key **äº’ä¸å†²çª**

#### åœºæ™¯ 2ï¼šAnthropic ç”¨æˆ· + Letta å…è´¹ Embedding

**ç¯å¢ƒå˜é‡é…ç½®**ï¼š
```bash
ANTHROPIC_API_KEY=sk-ant-xxxxx
# ä¸éœ€è¦å…¶ä»– key
```

**Agent é…ç½®**ï¼š
```python
{
    "llm_config": {
        "model": "claude-sonnet-4-5-20250929",
        "model_endpoint_type": "anthropic"
    },
    "embedding_config": {
        "embedding_model": "letta",
        "embedding_endpoint": "https://embeddings.letta.com",
        "embedding_endpoint_type": "openai"
    }
}
```

**è°ƒç”¨æµç¨‹**ï¼š
1. LLM è¯·æ±‚ä½¿ç”¨ `ANTHROPIC_API_KEY`
2. Embedding è¯·æ±‚ä½¿ç”¨ `DUMMY_API_KEY`ï¼ˆLetta å…è´¹ embedding ä¸éœ€è¦ keyï¼‰
3. è¯·æ±‚å‘é€åˆ° `https://embeddings.letta.com`

### 11.5 å…³é”®è¦ç‚¹

1. **Embedding æ€»æ˜¯ä½¿ç”¨ `OPENAI_API_KEY`**
   - ä¸ç®¡ LLM ç”¨ä»€ä¹ˆ providerï¼ˆAnthropicã€OpenAIã€å…¶ä»–ï¼‰
   - Embedding åªçœ‹ `embedding_endpoint_type`
   - å¦‚æœæ˜¯ `"openai"`ï¼Œå°±ç”¨ `OPENAI_API_KEY`

2. **Embedding å’Œ LLM çš„ API Key å®Œå…¨ç‹¬ç«‹**
   - LLM keyï¼šç”± `llm_config.model_endpoint_type` å†³å®š
   - Embedding keyï¼šæ€»æ˜¯ä½¿ç”¨ `OPENAI_API_KEY`ï¼ˆå¦‚æœæ˜¯ OpenAI ç±»å‹ï¼‰

3. **ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ**
   - Embedding API å¤§å¤šå…¼å®¹ OpenAI æ ¼å¼
   - ç®€åŒ–é…ç½®ï¼šåªéœ€è¦ä¸€ä¸ª `OPENAI_API_KEY` å°±èƒ½ç”¨äºå¤šä¸ªåœºæ™¯
   - Anthropic æ ¹æœ¬ä¸æä¾› embeddingï¼Œæ‰€ä»¥ä¸å­˜åœ¨åˆ†ç¦»çš„é—®é¢˜

4. **å¦‚æœä½ æƒ³ç”¨ä¸åŒçš„ embedding key**
   - âŒ ç¯å¢ƒå˜é‡ï¼šæ— æ³•åŒºåˆ†
   - âœ… æ•°æ®åº“ Providerï¼šåˆ›å»ºç‹¬ç«‹çš„ embedding provider

### 11.6 è°ƒç”¨ç¤ºä¾‹ä»£ç 

**ç¤ºä¾‹ 1ï¼šåˆ›å»º passage æ—¶è°ƒç”¨ embedding**

```python
# letta/services/passage_manager.py:476
async def create_agent_passage_async(...):
    # 1. åˆ›å»º embedding client
    embedding_client = LLMClient.create(
        provider_type=agent_state.embedding_config.embedding_endpoint_type,
        actor=actor,
    )

    # 2. è¯·æ±‚ embedding
    embeddings = await embedding_client.request_embeddings(
        [text],
        agent_state.embedding_config
    )

    # 3. å­˜å‚¨ passage
    passage = PydanticPassage(
        text=text,
        embedding=embeddings[0],  # âœ… ä½¿ç”¨è¿”å›çš„ embedding
        ...
    )
```

**ç¤ºä¾‹ 2ï¼šæœç´¢è®°å¿†æ—¶è°ƒç”¨ embedding**

```python
# letta/services/helpers/agent_manager_helper.py:878
async def query_agent_sources(...):
    # 1. åˆ›å»º embedding client
    embedding_client = LLMClient.create(
        provider_type=embedding_config.embedding_endpoint_type,
        actor=actor,
    )

    # 2. å°†æŸ¥è¯¢æ–‡æœ¬è½¬ä¸º embedding
    embeddings = await embedding_client.request_embeddings(
        [query_text],
        embedding_config
    )

    # 3. ä½¿ç”¨ embedding è¿›è¡Œå‘é‡æœç´¢
    embedded_text = np.array(embeddings[0])
    # ... æœç´¢é€»è¾‘
```

### 11.7 é”™è¯¯é…ç½®ç¤ºä¾‹

#### âŒ é”™è¯¯ 1ï¼šAnthropic ç”¨æˆ·å°è¯•ä½¿ç”¨ Anthropic Embedding

```python
{
    "embedding_config": {
        "embedding_endpoint_type": "anthropic",  # âŒ é”™è¯¯ï¼
        "embedding_model": "claude-3-5-sonnet"
    }
}
```

**ç»“æœ**ï¼šä¼šæŠ¥é”™ï¼Œå› ä¸º `AnthropicClient` æ²¡æœ‰ `request_embeddings()` æ–¹æ³•

#### âŒ é”™è¯¯ 2ï¼šåªé…ç½® ANTHROPIC_API_KEYï¼Œå´æƒ³ç”¨ OpenAI Embedding

```bash
# .env
ANTHROPIC_API_KEY=sk-ant-xxxxx
# âŒ ç¼ºå°‘ OPENAI_API_KEY
```

```python
{
    "embedding_config": {
        "embedding_endpoint_type": "openai",  # éœ€è¦ OPENAI_API_KEY
        "embedding_model": "text-embedding-3-small"
    }
}
```

**ç»“æœ**ï¼šä¼šä½¿ç”¨ `DUMMY_API_KEY`ï¼Œè¯·æ±‚ä¼šå¤±è´¥ï¼ˆé™¤éæ˜¯ Letta å…è´¹ embeddingï¼‰

---

## 12. åˆ›å»º Agent çš„å®Œæ•´æµç¨‹ï¼ˆ2026-01-04ï¼‰

### 12.1 æµç¨‹æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç”¨æˆ·è¯·æ±‚åˆ›å»º Agent                                           â”‚
â”‚  POST /v1/agents                                            â”‚
â”‚  {                                                           â”‚
â”‚    "name": "my-agent",                                       â”‚
â”‚    "llm_config": {...},                                      â”‚
â”‚    "embedding_config": {...},                                â”‚
â”‚    ...                                                       â”‚
â”‚  }                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. REST API æ¥æ”¶è¯·æ±‚                                         â”‚
â”‚     letta/server/rest_api/routers/v1/agents.py              â”‚
â”‚     async def create_agent(...)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. è°ƒç”¨ Agent Manager                                        â”‚
â”‚     agent = await server.agent_manager.create_agent_async(  â”‚
â”‚         agent_create,                                        â”‚
â”‚         actor                                                â”‚
â”‚     )                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. éªŒè¯å’Œå‡†å¤‡é…ç½®                                            â”‚
â”‚     letta/services/agent_manager.py:330                     â”‚
â”‚                                                              â”‚
â”‚     3.1 éªŒè¯å¿…å¡«å­—æ®µ                                          â”‚
â”‚         if not llm_config or not embedding_config:           â”‚
â”‚             raise ValueError(...)                            â”‚
â”‚                                                              â”‚
â”‚     3.2 åº”ç”¨æ¨ç†è®¾ç½®ï¼ˆé’ˆå¯¹ v1 agentsï¼‰                        â”‚
â”‚         if agent_type == letta_v1_agent:                     â”‚
â”‚             apply_reasoning_setting_to_config()              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. å¤„ç† Memory Blocksï¼ˆè®°å¿†å—ï¼‰                               â”‚
â”‚                                                              â”‚
â”‚     4.1 è·å–ç”¨æˆ·æä¾›çš„ blocks                                 â”‚
â”‚         pydantic_blocks = [                                 â”‚
â”‚             PydanticBlock(**b.model_dump())                 â”‚
â”‚             for b in agent_create.memory_blocks             â”‚
â”‚         ]                                                    â”‚
â”‚                                                              â”‚
â”‚     4.2 ä¸ºé»˜è®¤ blocks æ³¨å…¥æè¿°                                â”‚
â”‚         default_blocks = {                                   â”‚
â”‚             "persona": ...,                                  â”‚
â”‚             "human": ...,                                    â”‚
â”‚             "instructions": ...                              â”‚
â”‚         }                                                    â”‚
â”‚                                                              â”‚
â”‚     4.3 æ‰¹é‡åˆ›å»º blocks                                      â”‚
â”‚         created_blocks = await block_manager                 â”‚
â”‚             .batch_create_blocks_async(                     â”‚
â”‚                 pydantic_blocks,                             â”‚
â”‚                 actor                                        â”‚
â”‚             )                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. å¤„ç†å·¥å…·ï¼ˆToolsï¼‰                                         â”‚
â”‚                                                              â”‚
â”‚     5.1 æ”¶é›†å·¥å…·åç§°                                          â”‚
â”‚         tool_names = set(agent_create.tools or [])          â”‚
â”‚                                                              â”‚
â”‚     5.2 æ ¹æ® agent_type æ·»åŠ åŸºç¡€å·¥å…·                          â”‚
â”‚         if include_base_tools:                               â”‚
â”‚             if agent_type == letta_v1_agent:                â”‚
â”‚                 tool_names |= BASE_TOOLS_V2                 â”‚
â”‚             else:                                            â”‚
â”‚                 tool_names |= BASE_TOOLS                    â”‚
â”‚                                                              â”‚
â”‚     5.3 è§£æå·¥å…·ï¼ˆname â†’ id æ˜ å°„ï¼‰                            â”‚
â”‚         name_to_id, id_to_name = await _resolve_tools_async(â”‚
â”‚             tool_names,                                      â”‚
â”‚             supplied_ids                                     â”‚
â”‚         )                                                    â”‚
â”‚                                                              â”‚
â”‚     5.4 è®¾ç½®å·¥å…·è§„åˆ™                                          â”‚
â”‚         tool_rules = [                                       â”‚
â”‚             TerminalToolRule(tool_name="send_message"),      â”‚
â”‚             ContinueToolRule(tool_name="conversation_search"),â”‚
â”‚             ...                                               â”‚
â”‚         ]                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. å¤„ç†æ•°æ®æºï¼ˆSourcesï¼‰                                      â”‚
â”‚                                                              â”‚
â”‚     if include_default_source:                               â”‚
â”‚         default_source = PydanticSource(                     â”‚
â”‚             name=f"{agent_name} External Data Source",       â”‚
â”‚             embedding_config=agent_create.embedding_config  â”‚
â”‚         )                                                    â”‚
â”‚         created_source = await source_manager               â”‚
â”‚             .create_source(default_source, actor)           â”‚
â”‚         source_ids.append(created_source.id)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. åˆ›å»º Agent æ•°æ®åº“è®°å½•                                      â”‚
â”‚     async with db_registry.async_session() as session:       â”‚
â”‚         new_agent = AgentModel(                              â”‚
â”‚             name=agent_create.name,                          â”‚
â”‚             system=derive_system_message(...),              â”‚
â”‚             agent_type=agent_create.agent_type,              â”‚
â”‚             llm_config=agent_create.llm_config,              â”‚
â”‚             embedding_config=agent_create.embedding_config,  â”‚
â”‚             organization_id=actor.organization_id,           â”‚
â”‚             tool_rules=tool_rules,                           â”‚
â”‚             ...                                               â”‚
â”‚         )                                                    â”‚
â”‚         session.add(new_agent)                               â”‚
â”‚         await session.flush()                                â”‚
â”‚         aid = new_agent.id                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  8. å…³è”æ•°æ®ï¼ˆé€šè¿‡ä¸­é—´è¡¨ï¼‰                                      â”‚
â”‚                                                              â”‚
â”‚     8.1 å…³è” Blocks                                           â”‚
â”‚         await _bulk_insert_pivot_async(                      â”‚
â”‚             session,                                         â”‚
â”‚             AgentsBlocks,                                    â”‚
â”‚             agent_id=aid,                                    â”‚
â”‚             block_ids=block_ids                              â”‚
â”‚         )                                                    â”‚
â”‚                                                              â”‚
â”‚     8.2 å…³è” Tools                                           â”‚
â”‚         await _bulk_insert_pivot_async(                      â”‚
â”‚             session,                                         â”‚
â”‚             AgentsTools,                                     â”‚
â”‚             agent_id=aid,                                    â”‚
â”‚             tool_ids=tool_ids                                â”‚
â”‚         )                                                    â”‚
â”‚                                                              â”‚
â”‚     8.3 å…³è” Sources                                         â”‚
â”‚         await _bulk_insert_pivot_async(                      â”‚
â”‚             session,                                         â”‚
â”‚             SourcesAgents,                                   â”‚
â”‚             agent_id=aid,                                    â”‚
â”‚             source_ids=source_ids                            â”‚
â”‚         )                                                    â”‚
â”‚                                                              â”‚
â”‚     8.4 å…³è” Identities                                       â”‚
â”‚         if identity_ids:                                     â”‚
â”‚             await _bulk_insert_pivot_async(...)             â”‚
â”‚                                                              â”‚
â”‚     8.5 å…³è” Tags                                             â”‚
â”‚         if tag_values:                                       â”‚
â”‚             await _bulk_insert_pivot_async(...)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  9. åˆå§‹åŒ– Agentï¼ˆå¦‚æœéœ€è¦ï¼‰                                    â”‚
â”‚                                                              â”‚
â”‚     if not _init_with_no_messages:                           â”‚
â”‚         # 9.1 åˆ›å»ºåˆå§‹æ¶ˆæ¯åºåˆ—                                 â”‚
â”‚             initial_messages = (                              â”‚
â”‚                 agent_create.initial_message_sequence or     â”‚
â”‚                 get_default_initial_message_sequence(...)    â”‚
â”‚             )                                                â”‚
â”‚                                                              â”‚
â”‚         # 9.2 å‘é€åˆå§‹æ¶ˆæ¯                                     â”‚
â”‚             for msg in initial_messages:                     â”‚
â”‚                 await self._submit_message_to_agent_async(  â”‚
â”‚                     session,                                 â”‚
â”‚                     agent_state=new_agent,                   â”‚
â”‚                     actor=actor,                             â”‚
â”‚                     message=msg                              â”‚
â”‚                 )                                            â”‚
â”‚                                                              â”‚
â”‚         # 9.3 æ›´æ–° agent state                                â”‚
â”‚             await new_agent.reload_state(session)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  10. è¿”å›åˆ›å»ºçš„ Agent                                          â”‚
â”‚      return PydanticAgentState(**new_agent.model_dump())     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 12.2 å…³é”®æ­¥éª¤è¯¦è§£

#### æ­¥éª¤ 1ï¼šé…ç½®éªŒè¯

**ä»£ç ä½ç½®**ï¼š`letta/services/agent_manager.py:338-359`

```python
async def create_agent_async(...):
    # âœ… éªŒè¯å¿…å¡«é…ç½®
    if not agent_create.llm_config or not agent_create.embedding_config:
        raise ValueError("llm_config and embedding_config are required")

    # âœ… åº”ç”¨æ¨ç†è®¾ç½®ï¼ˆé’ˆå¯¹æ”¯æŒæ¨ç†çš„æ¨¡å‹ï¼‰
    if agent_create.agent_type == AgentType.letta_v1_agent:
        default_reasoning = (
            LLMConfig.is_anthropic_reasoning_model(agent_create.llm_config) or
            LLMConfig.is_openai_reasoning_model(agent_create.llm_config)
        )
        agent_create.llm_config = LLMConfig.apply_reasoning_setting_to_config(
            agent_create.llm_config,
            agent_create.reasoning or default_reasoning,
            agent_create.agent_type,
        )
```

**å…³é”®ç‚¹**ï¼š
- `llm_config` å’Œ `embedding_config` éƒ½æ˜¯å¿…å¡«çš„
- æ¨ç†æ¨¡å‹ï¼ˆClaude 3.7/4ã€OpenAI o1/o3ï¼‰ä¼šè‡ªåŠ¨å¯ç”¨æ¨ç†åŠŸèƒ½

#### æ­¥éª¤ 2ï¼šåˆ›å»º Memory Blocks

**ä»£ç ä½ç½®**ï¼š`letta/services/agent_manager.py:361-379`

```python
# å¤„ç†ç”¨æˆ·æä¾›çš„ blocks
if agent_create.memory_blocks:
    pydantic_blocks = [PydanticBlock(**b.model_dump()) for b in ...]

    # ä¸ºé»˜è®¤ blocks æ³¨å…¥æè¿°
    default_blocks = {block.label: block for block in DEFAULT_BLOCKS}
    for block in pydantic_blocks:
        if block.label in default_blocks and not block.description:
            block.description = default_blocks[block.label].description

    # æ‰¹é‡åˆ›å»º
    created_blocks = await self.block_manager.batch_create_blocks_async(
        pydantic_blocks, actor
    )
    block_ids.extend([blk.id for blk in created_blocks])
```

**é»˜è®¤ Blocks**ï¼š
- `persona`ï¼šAgent çš„è§’è‰²å’Œæ€§æ ¼
- `human`ï¼šå¯¹ç”¨æˆ·çš„æè¿°
- `instructions`ï¼šç³»ç»ŸæŒ‡ä»¤

#### æ­¥éª¤ 3ï¼šé…ç½®å·¥å…·

**ä»£ç ä½ç½®**ï¼š`letta/services/agent_manager.py:381-479`

```python
# æ”¶é›†å·¥å…·åç§°
tool_names = set(agent_create.tools or [])

# æ ¹æ®ç±»å‹æ·»åŠ åŸºç¡€å·¥å…·
if agent_create.include_base_tools:
    if agent_create.agent_type == AgentType.letta_v1_agent:
        tool_names |= calculate_base_tools(is_v2=True)
    else:
        tool_names |= calculate_base_tools(is_v2=False)

# è§£æå·¥å…·ï¼ˆåç§° â†’ ID æ˜ å°„ï¼‰
name_to_id, id_to_name, requires_approval = await self._resolve_tools_async(
    session, tool_names, supplied_ids, actor.organization_id
)

# è®¾ç½®å·¥å…·è§„åˆ™
tool_rules = []
if should_add_base_tool_rules:
    for tool_name in tool_names:
        if tool_name in {"send_message", ...}:
            tool_rules.append(TerminalToolRule(tool_name=tool_name))
        elif tool_name in BASE_TOOLS:
            tool_rules.append(ContinueToolRule(tool_name=tool_name))
```

**åŸºç¡€å·¥å…·ç¤ºä¾‹**ï¼š
- `send_message`ï¼šå‘é€æ¶ˆæ¯ç»™ç”¨æˆ·
- `conversation_search`ï¼šæœç´¢å¯¹è¯å†å²
- `archival_memory_search`ï¼šæœç´¢é•¿æœŸè®°å¿†
- `core_memory_append`ï¼šè¿½åŠ æ ¸å¿ƒè®°å¿†
- `core_memory_replace`ï¼šæ›¿æ¢æ ¸å¿ƒè®°å¿†

#### æ­¥éª¤ 4ï¼šåˆ›å»ºæ•°æ®æº

**ä»£ç ä½ç½®**ï¼š`letta/services/agent_manager.py:427-433`

```python
if agent_create.include_default_source:
    # åˆ›å»ºé»˜è®¤æ•°æ®æºï¼ˆä½¿ç”¨ agent çš„ embedding é…ç½®ï¼‰
    default_source = PydanticSource(
        name=f"{agent_create.name} External Data Source",
        embedding_config=agent_create.embedding_config  # âœ… å…³é”®
    )
    created_source = await self.source_manager.create_source(
        default_source, actor
    )
    source_ids.append(created_source.id)
```

**é‡è¦**ï¼šæ•°æ®æºçš„ embedding_config å¿…é¡»ä¸ agent ä¸€è‡´ï¼Œè¿™æ ·æ‰èƒ½æ­£ç¡®æœç´¢ã€‚

#### æ­¥éª¤ 5ï¼šå­˜å‚¨åˆ°æ•°æ®åº“

**ä»£ç ä½ç½®**ï¼š`letta/services/agent_manager.py:484-525`

```python
async with db_registry.async_session() as session:
    async with session.begin():
        # åˆ›å»º Agent è®°å½•
        new_agent = AgentModel(
            name=agent_create.name,
            system=derive_system_message(
                agent_type=agent_create.agent_type,
                system=agent_create.system
            ),
            agent_type=agent_create.agent_type,
            llm_config=agent_create.llm_config,          # âœ… JSON å­˜å‚¨
            embedding_config=agent_create.embedding_config,  # âœ… JSON å­˜å‚¨
            organization_id=actor.organization_id,
            tool_rules=tool_rules,
            ...
        )

        session.add(new_agent)
        await session.flush()  # è·å– ID
        aid = new_agent.id
```

**æ•°æ®è¡¨ç»“æ„**ï¼š
- `agents` è¡¨ï¼šå­˜å‚¨ agent åŸºæœ¬ä¿¡æ¯
- `agents_blocks`ï¼šagent ä¸ blocks çš„å¤šå¯¹å¤šå…³ç³»
- `agents_tools`ï¼šagent ä¸ tools çš„å¤šå¯¹å¤šå…³ç³»
- `sources_agents`ï¼šagent ä¸ sources çš„å¤šå¯¹å¤šå…³ç³»

#### æ­¥éª¤ 6ï¼šåˆå§‹åŒ– Agent

**ä»£ç ä½ç½®**ï¼š`letta/services/agent_manager.py:540-580`

```python
if not _init_with_no_messages:
    # è·å–åˆå§‹æ¶ˆæ¯åºåˆ—
    initial_messages = (
        agent_create.initial_message_sequence or
        get_default_initial_message_sequence(agent_type)
    )

    # å‘é€åˆå§‹æ¶ˆæ¯
    for msg in initial_messages:
        await self._submit_message_to_agent_async(
            session,
            agent_state=new_agent,
            actor=actor,
            message=msg
        )

    # é‡æ–°åŠ è½½ agent state
    await new_agent.reload_state(session)
```

**é»˜è®¤åˆå§‹æ¶ˆæ¯**ï¼š
- V1 agentsï¼šç©ºåºåˆ—ï¼ˆ`[]`ï¼‰
- å…¶ä»– agentsï¼šåŒ…å«ç³»ç»Ÿæç¤ºå’Œåˆå§‹åŒ–æ¶ˆæ¯

### 12.3 é…ç½®å­˜å‚¨æ ¼å¼

#### LLM Config

```json
{
    "model": "claude-sonnet-4-5-20250929",
    "model_endpoint_type": "anthropic",
    "model_endpoint": "https://api.anthropic.com",
    "context_window": 200000,
    "temperature": 0.7,
    "max_tokens": 4096
}
```

#### Embedding Config

```json
{
    "embedding_model": "letta",
    "embedding_endpoint": "https://embeddings.letta.com",
    "embedding_endpoint_type": "openai",
    "embedding_dim": 1536,
    "embedding_chunk_size": 300
}
```

### 12.4 å®é™…è°ƒç”¨ç¤ºä¾‹

**HTTP è¯·æ±‚**ï¼š
```bash
curl -X POST "http://localhost:8283/v1/agents" \
  -H "Content-Type: application/json" \
  -H "X-Actor-Id: user-00000000-0000-4000-8000-000000000000" \
  -d '{
    "name": "my-agent",
    "agent_type": "letta_v1_agent",
    "system": "You are a helpful assistant.",
    "llm_config": {
      "model": "claude-sonnet-4-5-20250929",
      "model_endpoint_type": "anthropic"
    },
    "embedding_config": {
      "embedding_model": "letta",
      "embedding_endpoint": "https://embeddings.letta.com",
      "embedding_endpoint_type": "openai",
      "embedding_dim": 1536
    },
    "memory_blocks": [
      {
        "label": "persona",
        "value": "You are Claude, a helpful AI assistant."
      },
      {
        "label": "human",
        "value": "The user is asking for help."
      }
    ],
    "include_base_tools": true,
    "include_default_source": true
  }'
```

**å“åº”**ï¼š
```json
{
  "id": "agent-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx",
  "name": "my-agent",
  "agent_type": "letta_v1_agent",
  "system": "You are a helpful assistant.",
  "llm_config": {...},
  "embedding_config": {...},
  "created_at": "2026-01-04T12:00:00Z",
  ...
}
```

### 12.5 å…³é”®è¦ç‚¹æ€»ç»“

1. **é…ç½®éªŒè¯**
   - `llm_config` å’Œ `embedding_config` å¿…å¡«
   - æ¨ç†æ¨¡å‹è‡ªåŠ¨å¯ç”¨æ¨ç†åŠŸèƒ½

2. **Memory Blocks**
   - é»˜è®¤ blocksï¼š`persona`ã€`human`ã€`instructions`
   - ç”¨æˆ·å¯è‡ªå®šä¹‰ blocks

3. **å·¥å…·é…ç½®**
   - åŸºç¡€å·¥å…·æ ¹æ® `agent_type` è‡ªåŠ¨æ·»åŠ 
   - å·¥å…·è§„åˆ™æ§åˆ¶å·¥å…·è°ƒç”¨è¡Œä¸º

4. **æ•°æ®æº**
   - å¯é€‰çš„é»˜è®¤æ•°æ®æº
   - ä½¿ç”¨ agent çš„ embedding_config

5. **æ•°æ®åº“å­˜å‚¨**
   - Agent ä¸»è¡¨å­˜å‚¨é…ç½®ï¼ˆJSON æ ¼å¼ï¼‰
   - ä¸­é—´è¡¨å…³è” blocksã€toolsã€sources

6. **åˆå§‹åŒ–**
   - é»˜è®¤å‘é€åˆå§‹æ¶ˆæ¯åºåˆ—
   - å¯é€šè¿‡ `_init_with_no_messages` è·³è¿‡

---

## 13. Agent å‚æ•°å®Œæ•´å‚è€ƒï¼ˆ2026-01-04ï¼‰

### 13.1 å…³äº -d å‚æ•°

**é—®é¢˜**ï¼šåˆ›å»º agent æ—¶ï¼Œé€šè¿‡ `-d` å‚æ•°å¯ä»¥è®¾ç½® agent çš„æ‰€æœ‰å‚æ•°å—ï¼Ÿ

**ç­”æ¡ˆ**ï¼š

#### å½“å‰ `letta_agent_manager.py` çš„çŠ¶æ€

æ‚¨çš„è„šæœ¬**ä¸æ”¯æŒ** `-d` å‚æ•°ã€‚å½“å‰ä½¿ç”¨çš„æ˜¯ç‹¬ç«‹å‘½ä»¤è¡Œå‚æ•°ï¼š

```bash
python letta_agent_manager.py create <name> \
  --model <model> \
  --provider <provider> \
  --endpoint <endpoint> \
  --human "<info>" \
  --persona "<persona>" \
  --system "<prompt>" \
  --temperature 0.7 \
  --context-window 200000
```

#### å¦‚æœæƒ³æ·»åŠ  `-d` å‚æ•°æ”¯æŒ

å¯ä»¥ä¿®æ”¹è„šæœ¬ï¼Œæ·»åŠ ä» JSON æ–‡ä»¶è¯»å–é…ç½®çš„åŠŸèƒ½ï¼š

```python
# åœ¨ create_agent æ–¹æ³•ä¹‹å‰æ·»åŠ 
def create_agent_from_file(self, config_file: str) -> Optional[Dict[str, Any]]:
    """ä» JSON æ–‡ä»¶åˆ›å»º agent"""
    try:
        with open(config_file, 'r') as f:
            config = json.load(f)

        print(f"\nğŸ”¨ ä»é…ç½®æ–‡ä»¶åˆ›å»º Agent...")
        print(f"   é…ç½®æ–‡ä»¶: {config_file}")
        print(f"   Agent åç§°: {config.get('name', 'N/A')}")
        print()

        resp = self._make_request("POST", "/v1/agents/", json=config)

        if resp.status_code == 200:
            agent = resp.json()
            print(f"âœ… Agent åˆ›å»ºæˆåŠŸ!")
            print(f"   Agent ID: {agent['id']}")
            return agent
        else:
            print(f"âŒ åˆ›å»ºå¤±è´¥:")
            print(f"   {resp.text}")
            return None
    except Exception as e:
        print(f"âŒ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return None
```

ç„¶ååœ¨å‘½ä»¤è¡Œå‚æ•°ä¸­æ·»åŠ ï¼š

```python
create_parser.add_argument(
    "-d", "--data",
    dest="config_file",
    help="ä» JSON æ–‡ä»¶è¯»å–é…ç½®"
)
```

### 13.2 CreateAgent æ‰€æœ‰å‚æ•°åˆ—è¡¨

#### åŸºæœ¬ä¿¡æ¯
- **name** (str): Agent åç§°
  - å¿…å¡«ï¼Œå¦‚æœä¸æä¾›ä¼šè‡ªåŠ¨ç”Ÿæˆéšæœºåç§°
  - ä¾‹å¦‚ï¼š`"my-assistant"`

- **description** (str, optional): Agent æè¿°
  - ä¾‹å¦‚ï¼š`"A helpful coding assistant"`

- **agent_type** (AgentType, optional): Agent ç±»å‹
  - é»˜è®¤ï¼š`"letta_v1_agent"`
  - å¯é€‰å€¼ï¼š
    - `letta_v1_agent` - æœ€æ–°ç‰ˆæœ¬çš„ Letta agent
    - `memgpt_v2_agent` - MemGPT v2 agent
    - `react_agent` - ReAct agent
    - `workflow_agent` - å·¥ä½œæµ agent
    - `voice_agent` - è¯­éŸ³ agent
    - ç­‰

#### è®°å¿†é…ç½®

**memory_blocks** (List[CreateBlock], optional): è¦åˆ›å»ºçš„è®°å¿†å—åˆ—è¡¨
```python
[
  {
    "label": "human",              # å¿…å¡«ï¼šè®°å¿†å—æ ‡ç­¾
    "value": "User info",          # å¿…å¡«ï¼šè®°å¿†å—å†…å®¹
    "limit": 2000,                 # å¯é€‰ï¼štoken é™åˆ¶
    "description": "About the user" # å¯é€‰ï¼šæè¿°
  },
  {
    "label": "persona",
    "value": "You are a helpful assistant."
  }
]
```

**é»˜è®¤è®°å¿†å—**ï¼š
- `human` - ç”¨æˆ·ä¿¡æ¯
- `persona` - AI äººæ ¼
- `instructions` - ç³»ç»ŸæŒ‡ä»¤

**block_ids** (List[str], optional): å·²å­˜åœ¨çš„ block IDs
- å¦‚æœä½ æƒ³ä½¿ç”¨å·²åˆ›å»ºçš„è®°å¿†å—

**initial_message_sequence** (List[MessageCreate], optional): åˆå§‹æ¶ˆæ¯åºåˆ—
- é»˜è®¤ä¼šå‘é€ä¸€äº›åˆå§‹åŒ–æ¶ˆæ¯
- è®¾ä¸ºç©ºåˆ—è¡¨ `[]` å¯ä»¥è·³è¿‡

#### å·¥å…·é…ç½®

**tools** (List[str], optional): å·¥å…·åç§°åˆ—è¡¨
- âš ï¸ å·²åºŸå¼ƒï¼Œå»ºè®®ä½¿ç”¨ `tool_ids`

**tool_ids** (List[str], optional): å·¥å…· IDs åˆ—è¡¨
- ä¾‹å¦‚ï¼š`["tool-id-1", "tool-id-2"]`

**include_base_tools** (bool): æ˜¯å¦åŒ…å«åŸºç¡€å·¥å…·
- é»˜è®¤ï¼š`True`
- åŸºç¡€å·¥å…·åŒ…æ‹¬ï¼š
  - `core_memory_append` - è¿½åŠ æ ¸å¿ƒè®°å¿†
  - `core_memory_replace` - æ›¿æ¢æ ¸å¿ƒè®°å¿†
  - `conversation_search` - æœç´¢å¯¹è¯å†å²
  - `archival_memory_search` - æœç´¢é•¿æœŸè®°å¿†
  - `send_message` - å‘é€æ¶ˆæ¯ç»™ç”¨æˆ·

**include_multi_agent_tools** (bool): æ˜¯å¦åŒ…å«å¤š agent å·¥å…·
- é»˜è®¤ï¼š`False`
- åŒ…æ‹¬ï¼š`send_message_to_agent` ç­‰

**include_base_tool_rules** (bool, optional): æ˜¯å¦æ·»åŠ åŸºç¡€å·¥å…·è§„åˆ™
- æ§åˆ¶å·¥å…·è°ƒç”¨çš„è¡Œä¸º

**tool_rules** (List[ToolRule], optional): å·¥å…·è§„åˆ™åˆ—è¡¨
- ä¾‹å¦‚ï¼š
  ```python
  [
    {"type": "terminal", "tool_name": "send_message"},
    {"type": "continue", "tool_name": "conversation_search"}
  ]
  ```

#### æ•°æ®æºé…ç½®

**source_ids** (List[str], optional): æ•°æ®æº IDs
- âš ï¸ å·²åºŸå¼ƒï¼Œä½¿ç”¨ `folder_ids` ä»£æ›¿

**folder_ids** (List[str], optional): æ–‡ä»¶å¤¹ IDs
- å…³è”æ–‡ä»¶å¤¹ä½œä¸ºæ•°æ®æº

**include_default_source** (bool): æ˜¯å¦åˆ›å»ºé»˜è®¤æ•°æ®æº
- é»˜è®¤ï¼š`False`
- âš ï¸ å·²åºŸå¼ƒ

#### ç³»ç»Ÿé…ç½®

**system** (str, optional): ç³»ç»Ÿæç¤ºè¯
- ä¾‹å¦‚ï¼š`"You are a helpful AI assistant. Be concise and friendly."`

#### æ¨¡å‹é…ç½®ï¼ˆæ¨èä½¿ç”¨ï¼‰âœ…

**model** (str, optional): æ¨¡å‹ handle
- æ ¼å¼ï¼š`provider/model-name`
- ä¾‹å¦‚ï¼š
  - `"openai/gpt-4o"`
  - `"anthropic/claude-3-5-sonnet-20250929"`
  - `"my-custom-provider/claude-sonnet-4-5-20250929"`

**embedding** (str, optional): Embedding æ¨¡å‹ handle
- æ ¼å¼ï¼š`provider/model-name`
- ä¾‹å¦‚ï¼š
  - `"letta/letta-free"` - Letta å…è´¹ embedding
  - `"openai/text-embedding-3-small"`
  - `"openai/text-embedding-3-large"`

**model_settings** (ModelSettingsUnion, optional): æ¨¡å‹è®¾ç½®
- é«˜çº§é…ç½®

#### æ¨¡å‹é…ç½®ï¼ˆè¯¦ç»†ï¼Œå·²åºŸå¼ƒï¼‰âš ï¸

**llm_config** (LLMConfig, optional): LLM é…ç½®
- âš ï¸ å·²åºŸå¼ƒï¼Œä½¿ç”¨ `model` ä»£æ›¿
```python
{
  "model": "claude-3-5-sonnet-20250929",
  "model_endpoint_type": "anthropic",  # or "openai"
  "model_endpoint": "https://api.anthropic.com",
  "provider_name": "anthropic",
  "provider_category": "byok",         # or "base"
  "context_window": 200000,
  "temperature": 0.7,
  "max_tokens": 4096
}
```

**embedding_config** (EmbeddingConfig, optional): Embedding é…ç½®
- âš ï¸ å·²åºŸå¼ƒï¼Œä½¿ç”¨ `embedding` ä»£æ›¿
```python
{
  "embedding_model": "letta-free",
  "embedding_endpoint_type": "openai",
  "embedding_endpoint": "https://embeddings.letta.com",
  "embedding_dim": 1536,
  "embedding_chunk_size": 300
}
```

#### é«˜çº§é…ç½®

**compaction_settings** (CompactionSettings, optional): å‹ç¼©è®¾ç½®
- æ§åˆ¶è®°å¿†å‹ç¼©ç­–ç•¥

**context_window_limit** (int, optional): ä¸Šä¸‹æ–‡çª—å£é™åˆ¶
- é™åˆ¶ agent ä½¿ç”¨çš„ä¸Šä¸‹æ–‡å¤§å°

#### æ¨ç†é…ç½®

**reasoning** (bool, optional): æ˜¯å¦å¯ç”¨æ¨ç†
- âš ï¸ å·²åºŸå¼ƒï¼Œç”± `model` å‚æ•°è‡ªåŠ¨åˆ¤æ–­

**enable_reasoner** (bool, optional): æ˜¯å¦å¯ç”¨æ¨ç†æ­¥éª¤
- é»˜è®¤ï¼š`True`
- âš ï¸ å·²åºŸå¼ƒ

**max_reasoning_tokens** (int, optional): æœ€å¤§æ¨ç† tokens
- âš ï¸ å·²åºŸå¼ƒ

**max_tokens** (int, optional): æœ€å¤§ç”Ÿæˆ tokens
- âš ï¸ å·²åºŸå¼ƒ

#### å…¶ä»–é…ç½®

**tags** (List[str], optional): Agent æ ‡ç­¾
- ä¾‹å¦‚ï¼š`["assistant", "coding", "production"]`

**metadata** (Dict, optional): å…ƒæ•°æ®
- è‡ªå®šä¹‰çš„é”®å€¼å¯¹
- ä¾‹å¦‚ï¼š`{"version": "1.0", "author": "Alice"}`

**secrets** (Dict[str, str], optional): Agent ä¸“å±çš„ç¯å¢ƒå˜é‡/å¯†é’¥
- ä¾‹å¦‚ï¼š
  ```python
  {
    "API_KEY": "sk-...",
    "DATABASE_URL": "postgresql://..."
  }
  ```

**project_id** (str, optional): é¡¹ç›® ID
- âš ï¸ å·²åºŸå¼ƒ

### 13.3 å®Œæ•´é…ç½®ç¤ºä¾‹

#### æœ€ç®€å•çš„é…ç½® âœ…

```python
{
  "name": "my-agent",
  "model": "openai/gpt-4o",
  "embedding": "letta/letta-free"
}
```

#### æ¨èé…ç½®ï¼ˆä½¿ç”¨ handle æ ¼å¼ï¼‰âœ…

```python
{
  # åŸºæœ¬ä¿¡æ¯
  "name": "my-assistant",
  "description": "A helpful coding assistant",
  "agent_type": "letta_v1_agent",

  # æ¨¡å‹é…ç½®ï¼ˆæ¨èï¼‰
  "model": "anthropic/claude-3-5-sonnet-20250929",
  "embedding": "letta/letta-free",

  # è®°å¿†é…ç½®
  "memory_blocks": [
    {
      "label": "human",
      "value": "The user is named Alice, a software engineer."
    },
    {
      "label": "persona",
      "value": "You are a helpful coding assistant specialized in Python."
    }
  ],

  # ç³»ç»Ÿæç¤º
  "system": "You are a helpful AI assistant. Be concise and friendly.",

  # å·¥å…·é…ç½®
  "include_base_tools": True,
  "include_base_tool_rules": True,

  # æ ‡ç­¾å’Œå…ƒæ•°æ®
  "tags": ["assistant", "coding"],
  "metadata": {
    "version": "1.0",
    "author": "Alice"
  }
}
```

#### ä½¿ç”¨è¯¦ç»†é…ç½®ï¼ˆä¸æ¨èï¼‰âš ï¸

```python
{
  "name": "my-agent",

  # ä½¿ç”¨è¯¦ç»†çš„ LLM é…ç½®ï¼ˆå·²åºŸå¼ƒï¼‰
  "llm_config": {
    "model": "claude-3-5-sonnet-20240620",
    "model_endpoint_type": "anthropic",
    "model_endpoint": "https://api.anthropic.com",
    "provider_name": "anthropic",
    "provider_category": "byok",
    "context_window": 200000,
    "temperature": 0.7,
    "max_tokens": 4096
  },

  # ä½¿ç”¨è¯¦ç»†çš„ Embedding é…ç½®ï¼ˆå·²åºŸå¼ƒï¼‰
  "embedding_config": {
    "embedding_model": "letta-free",
    "embedding_endpoint_type": "openai",
    "embedding_endpoint": "https://embeddings.letta.com",
    "embedding_dim": 1536,
    "embedding_chunk_size": 300
  },

  "memory_blocks": [
    {"label": "human", "value": "A user"},
    {"label": "persona", "value": "Helpful assistant"}
  ],

  "system": "You are a helpful assistant."
}
```

### 13.4 å¸¸è§é…ç½®æ¨¡å¼

#### æ¨¡å¼ 1ï¼šä½¿ç”¨ Letta å…è´¹ Embeddingï¼ˆæœ€ç®€å•ï¼‰âœ…

**é…ç½®**ï¼š
```python
{
  "name": "simple-agent",
  "model": "anthropic/claude-3-5-sonnet-20250929",
  "embedding": "letta/letta-free",
  "memory_blocks": [
    {"label": "human", "value": "A user"},
    {"label": "persona", "value": "Helpful assistant"}
  ]
}
```

**ç¯å¢ƒå˜é‡**ï¼š
```bash
ANTHROPIC_API_KEY=sk-ant-xxxxx
# ä¸éœ€è¦å…¶ä»– key
```

#### æ¨¡å¼ 2ï¼šä½¿ç”¨ OpenAI LLM + OpenAI Embedding

**é…ç½®**ï¼š
```python
{
  "name": "openai-agent",
  "model": "openai/gpt-4o",
  "embedding": "openai/text-embedding-3-small",
  "memory_blocks": [...]
}
```

**ç¯å¢ƒå˜é‡**ï¼š
```bash
OPENAI_API_KEY=sk-openai-xxxxx
```

#### æ¨¡å¼ 3ï¼šAnthropic LLM + OpenAI Embedding

**é…ç½®**ï¼š
```python
{
  "name": "hybrid-agent",
  "model": "anthropic/claude-3-5-sonnet-20250929",
  "embedding": "openai/text-embedding-3-small",
  "memory_blocks": [...]
}
```

**ç¯å¢ƒå˜é‡**ï¼š
```bash
ANTHROPIC_API_KEY=sk-ant-xxxxx      # Anthropic LLM
OPENAI_API_KEY=sk-openai-yyyyy      # OpenAI Embedding
```

**å…³é”®ç‚¹**ï¼š
- âœ… ä¸¤ä¸ª key äº’ä¸å†²çª
- âœ… LLM ä½¿ç”¨ `ANTHROPIC_API_KEY`
- âœ… Embedding ä½¿ç”¨ `OPENAI_API_KEY`

#### æ¨¡å¼ 4ï¼šä½¿ç”¨è‡ªå®šä¹‰ Providerï¼ˆå¦‚ lingyunapi.comï¼‰

**æ­¥éª¤ 1**ï¼šå…ˆåˆ›å»º Providerï¼ˆé€šè¿‡ API æˆ–æ•°æ®åº“ï¼‰

```python
# åˆ›å»º Provider
POST /v1/providers/
{
  "name": "lingyun-proxy",
  "provider_type": "openai",  # ä½¿ç”¨ OpenAI åè®®
  "base_url": "https://lingyunapi.com/v1",
  "provider_category": "byok"
}
```

**æ­¥éª¤ 2**ï¼šä½¿ç”¨ Provider åˆ›å»º Agent

```python
{
  "name": "custom-agent",
  "model": "lingyun-proxy/claude-sonnet-4-5-20250929",
  "embedding": "letta/letta-free",
  "memory_blocks": [...]
}
```

**ç¯å¢ƒå˜é‡**ï¼š
```bash
OPENAI_API_KEY=sk-tlegmZDKQBW5rce5sGaMdQeprOvDZgaRhr37KMhkieoiRIvh
# è¿™ä¸ª key ä¼šè¢« lingyun-proxy ä½¿ç”¨
```

### 13.5 å‚æ•°ä¼˜å…ˆçº§

å½“åŒä¸€ä¸ªå‚æ•°æœ‰å¤šç§è®¾ç½®æ–¹å¼æ—¶ï¼Œä¼˜å…ˆçº§å¦‚ä¸‹ï¼š

1. **handle æ ¼å¼å‚æ•°** (`model`, `embedding`) - æœ€æ¨è âœ…
   - ä¾‹å¦‚ï¼š`model="anthropic/claude-3-5-sonnet-20250929"`
   - è‡ªåŠ¨ä»æ•°æ®åº“è·å–é…ç½®
   - ç®€æ´ä¸”ä¸æ˜“å‡ºé”™

2. **è¯¦ç»†é…ç½®å‚æ•°** (`llm_config`, `embedding_config`) - å·²åºŸå¼ƒ âš ï¸
   - ä»…ç”¨äºå‘åå…¼å®¹
   - æ–°ä»£ç ä¸åº”è¯¥ä½¿ç”¨

3. **ç¯å¢ƒå˜é‡** - ä½œä¸º fallback
   - ä¾‹å¦‚ï¼š`ANTHROPIC_API_KEY`, `OPENAI_API_KEY`

### 13.6 é…ç½®å»ºè®®æ€»ç»“

#### âœ… æ¨èåšæ³•

1. **ä½¿ç”¨ handle æ ¼å¼**
   ```python
   {
     "model": "anthropic/claude-3-5-sonnet-20250929",
     "embedding": "letta/letta-free"
   }
   ```

2. **é…ç½®è®°å¿†å—**
   ```python
   "memory_blocks": [
     {"label": "human", "value": "..."},
     {"label": "persona", "value": "..."}
   ]
   ```

3. **ä½¿ç”¨ç¯å¢ƒå˜é‡ç®¡ç†å¯†é’¥**
   ```bash
   ANTHROPIC_API_KEY=sk-...
   OPENAI_API_KEY=sk-...
   ```

4. **ä¼˜å…ˆä½¿ç”¨ Letta å…è´¹ embedding**
   ```python
   "embedding": "letta/letta-free"
   ```

#### âŒ ä¸æ¨èåšæ³•

1. **ä½¿ç”¨å·²åºŸå¼ƒçš„ `llm_config` å’Œ `embedding_config`**
   ```python
   # âŒ ä¸æ¨è
   {
     "llm_config": {
       "model": "...",
       "model_endpoint": "...",
       ...
     }
   }
   ```

2. **ç¡¬ç¼–ç  API keys**
   ```python
   # âŒ ä¸æ¨è
   "secrets": {
     "API_KEY": "sk-hardcoded-key"
   }
   ```

3. **æ··åˆä½¿ç”¨æ–°æ—§å‚æ•°**
   ```python
   # âŒ æ··ä¹±
   {
     "model": "anthropic/claude-3-5-sonnet-20250929",
     "llm_config": {...}  # å†²çªï¼
   }
   ```

### 13.7 å‚æ•°é€ŸæŸ¥è¡¨

| å‚æ•° | ç±»å‹ | å¿…å¡« | é»˜è®¤å€¼ | æ¨èåº¦ |
|------|------|------|--------|--------|
| **åŸºæœ¬ä¿¡æ¯** |
| `name` | str | âŒ | éšæœºç”Ÿæˆ | â­â­â­ |
| `description` | str | âŒ | - | â­â­ |
| `agent_type` | str | âŒ | `letta_v1_agent` | â­â­ |
| **è®°å¿†** |
| `memory_blocks` | List | âŒ | - | â­â­â­ |
| `block_ids` | List[str] | âŒ | - | â­â­ |
| **æ¨¡å‹** |
| `model` | str | âŒ | - | â­â­â­ï¼ˆå¼ºçƒˆæ¨èï¼‰ |
| `embedding` | str | âŒ | - | â­â­â­ï¼ˆå¼ºçƒˆæ¨èï¼‰ |
| `llm_config` | object | âŒ | - | âŒ å·²åºŸå¼ƒ |
| `embedding_config` | object | âŒ | - | âŒ å·²åºŸå¼ƒ |
| **å·¥å…·** |
| `include_base_tools` | bool | âŒ | `True` | â­â­â­ |
| `include_base_tool_rules` | bool | âŒ | `True` | â­â­ |
| `tool_ids` | List[str] | âŒ | - | â­â­â­ |
| `tools` | List[str] | âŒ | - | âŒ å·²åºŸå¼ƒ |
| **ç³»ç»Ÿ** |
| `system` | str | âŒ | - | â­â­â­ |
| **æ•°æ®æº** |
| `folder_ids` | List[str] | âŒ | - | â­â­ |
| **å…¶ä»–** |
| `tags` | List[str] | âŒ | - | â­â­ |
| `metadata` | Dict | âŒ | - | â­â­ |
| `secrets` | Dict[str,str]| âŒ | - | â­â­ |

### 13.8 ä¸ºä»€ä¹ˆåºŸå¼ƒè¯¦ç»†é…ç½®ï¼Ÿï¼ˆæ·±åº¦è§£æï¼‰

#### é—®é¢˜

ä¸ºä»€ä¹ˆ `llm_config` å’Œ `embedding_config` è¢«æ ‡è®°ä¸ºåºŸå¼ƒï¼Ÿ

#### æ ¸å¿ƒåŸå› ï¼šä»"æ‰‹åŠ¨é…ç½®"åˆ°"æ•°æ®åº“é©±åŠ¨"çš„æ¶æ„è½¬å˜

##### âŒ æ—§æ–¹å¼ï¼šæ‰‹åŠ¨è¯¦ç»†é…ç½®ï¼ˆå·²åºŸå¼ƒï¼‰

```python
{
  "llm_config": {
    "model": "claude-3-5-sonnet-20250929",
    "model_endpoint_type": "anthropic",
    "model_endpoint": "https://api.anthropic.com",    # æ‰‹åŠ¨æŒ‡å®š
    "provider_name": "anthropic",                      # æ‰‹åŠ¨æŒ‡å®š
    "provider_category": "byok",                       # æ‰‹åŠ¨æŒ‡å®š
    "context_window": 200000,                          # æ‰‹åŠ¨æŒ‡å®š
    "temperature": 0.7,
    "max_tokens": 4096
  },
  "embedding_config": {
    "embedding_model": "letta-free",
    "embedding_endpoint_type": "openai",
    "embedding_endpoint": "https://embeddings.letta.com",  # æ‰‹åŠ¨æŒ‡å®š
    "embedding_dim": 1536,                                 # æ‰‹åŠ¨æŒ‡å®š
    "embedding_chunk_size": 300
  }
}
```

**å­˜åœ¨çš„é—®é¢˜**ï¼š
1. âŒ **é…ç½®å¤æ‚**ï¼šéœ€è¦å¡«å†™ 10+ ä¸ªå‚æ•°
2. âŒ **å®¹æ˜“å‡ºé”™**ï¼šæ‰‹åŠ¨è¾“å…¥ endpointã€ç±»å‹ç­‰å®¹æ˜“å‡ºé”™
3. âŒ **API key æš´éœ²**ï¼šé…ç½®ä¸­å¯èƒ½ç¡¬ç¼–ç æ•æ„Ÿä¿¡æ¯
4. âŒ **éš¾ä»¥ç»´æŠ¤**ï¼šæ›´æ–°é…ç½®éœ€è¦ä¿®æ”¹æ‰€æœ‰ agent
5. âŒ **é‡å¤é…ç½®**ï¼šæ¯ä¸ª agent éƒ½è¦é‡å¤ç›¸åŒçš„é…ç½®

##### âœ… æ–°æ–¹å¼ï¼šHandle æ ¼å¼ï¼ˆæ¨èï¼‰

```python
{
  "model": "anthropic/claude-3-5-sonnet-20250929",
  "embedding": "letta/letta-free"
}
```

**å·¥ä½œåŸç†**ï¼š

```
ç”¨æˆ·æä¾› handle: "anthropic/claude-3-5-sonnet-20250929"
                        â†“
         è§£æä¸º: provider_name="anthropic"
                  model_name="claude-3-5-sonnet-20250929"
                        â†“
    ä»æ•°æ®åº“æŸ¥è¯¢ Provider (letta/orm/provider.py)
    - provider_type: "anthropic"
    - base_url: "https://api.anthropic.com"
    - api_key: (ä»ç¯å¢ƒå˜é‡æˆ–åŠ å¯†å­˜å‚¨è·å–)
                        â†“
    ä»æ•°æ®åº“æŸ¥è¯¢ ProviderModel (letta/orm/provider_model.py)
    - handle: "anthropic/claude-3-5-sonnet-20250929"
    - context_window: 200000
    - max_tokens: 4096
    - model_endpoint_type: "anthropic"
                        â†“
         è‡ªåŠ¨å¡«å……å®Œæ•´çš„ llm_config
```

**ä¼˜åŠ¿**ï¼š
1. âœ… **æç®€é…ç½®**ï¼šåªéœ€ 1 ä¸ªå­—ç¬¦ä¸²
2. âœ… **é›†ä¸­ç®¡ç†**ï¼šæ‰€æœ‰é…ç½®å­˜å‚¨åœ¨æ•°æ®åº“
3. âœ… **åŠ¨æ€æ›´æ–°**ï¼šä¿®æ”¹ Providerï¼Œæ‰€æœ‰ agent è‡ªåŠ¨ç”Ÿæ•ˆ
4. âœ… **å®‰å…¨æ€§é«˜**ï¼šAPI key åŠ å¯†å­˜å‚¨
5. âœ… **ä¸æ˜“å‡ºé”™**ï¼šé¿å…æ‰‹åŠ¨é…ç½®é”™è¯¯
6. âœ… **æ˜“äºç»´æŠ¤**ï¼šä¸€æ¬¡ä¿®æ”¹ï¼Œå…¨å±€ç”Ÿæ•ˆ

#### ä»£ç è¯æ®

**åˆ›å»º Agent æ—¶çš„å¤„ç†** (`letta/server/server.py:437-490`):

```python
async def create_agent_async(self, request: CreateAgent, actor: User):
    # å¦‚æœæ²¡æœ‰æä¾› llm_configï¼ˆæ¨èæ–¹å¼ï¼‰
    if request.llm_config is None:
        # æ£€æŸ¥æ˜¯å¦æä¾›äº† model handle
        if request.model is None:
            raise LettaInvalidArgumentError(
                "Must specify either model or llm_config"
            )
        else:
            # ä» handle è‡ªåŠ¨è·å–å®Œæ•´é…ç½®
            handle = request.model  # "anthropic/claude-3-5-sonnet-20250929"

            llm_config = await self.get_llm_config_from_handle_async(
                actor=actor,
                handle=handle,
                context_window_limit=request.context_window_limit,
                max_tokens=request.max_tokens,
                ...
            )
```

**Handle è§£æé€»è¾‘** (`letta/server/server.py:1154-1194`):

```python
async def get_llm_config_from_handle_async(
    self,
    actor: User,
    handle: str,  # "anthropic/claude-3-5-sonnet-20250929"
    ...
) -> LLMConfig:
    # 1. è§£æ handle
    provider_name, model_name = handle.split("/", 1)

    # 2. ä»æ•°æ®åº“è·å– Provider
    provider = await self.get_provider_from_name_async(
        provider_name, actor
    )

    # 3. ä» Provider è·å–æ‰€æœ‰æ¨¡å‹é…ç½®
    all_llm_configs = await provider.list_llm_models_async()

    # 4. æŸ¥æ‰¾åŒ¹é…çš„æ¨¡å‹
    llm_configs = [
        config for config in all_llm_configs
        if config.handle == handle
    ]

    # 5. è¿”å›å®Œæ•´çš„ LLMConfig
    return llm_configs[0]
```

#### æ•°æ®åº“ç»“æ„

**Provider è¡¨** (`letta/orm/provider.py:15-46`):
```python
class Provider(SqlalchemyBase):
    __tablename__ = "providers"

    name: str                    # "anthropic", "openai", "lingyun-proxy"
    provider_type: str           # "anthropic", "openai", "bedrock"
    base_url: str                # "https://api.anthropic.com"
    api_key_enc: str             # åŠ å¯†å­˜å‚¨çš„ API key
    provider_category: str       # "base" or "byok"
    region: str                  # AWS region (for Bedrock)
    ...
```

**ProviderModel è¡¨** (`letta/orm/provider_model.py:14-70`):
```python
class ProviderModel(SqlalchemyBase):
    __tablename__ = "provider_models"

    handle: str                  # "anthropic/claude-3-5-sonnet-20250929"
    model: str                   # "claude-3-5-sonnet-20250929"
    provider_id: str             # å…³è”åˆ° Provider
    model_type: str              # "llm" or "embedding"
    context_window: int          # 200000
    max_tokens: int              # 4096
    model_endpoint_type: str     # "anthropic", "openai", etc.
    ...
```

#### å®é™…åœºæ™¯å¯¹æ¯”

##### åœºæ™¯ï¼šä½ æœ‰ 10 ä¸ª agent éƒ½ä½¿ç”¨ Anthropic Claude

**âŒ æ—§æ–¹å¼ï¼ˆå·²åºŸå¼ƒï¼‰**ï¼š

```python
# Agent 1
{
  "name": "agent-1",
  "llm_config": {
    "model": "claude-3-5-sonnet-20250929",
    "model_endpoint": "https://api.anthropic.com",
    "provider_name": "anthropic",
    "provider_category": "byok",
    "model_endpoint_type": "anthropic",
    "context_window": 200000,
    "temperature": 0.7
  }
}

# Agent 2 - é‡å¤æ‰€æœ‰é…ç½®
{
  "name": "agent-2",
  "llm_config": {
    "model": "claude-3-5-sonnet-20250929",
    "model_endpoint": "https://api.anthropic.com",
    "provider_name": "anthropic",
    "provider_category": "byok",
    "model_endpoint_type": "anthropic",
    "context_window": 200000,
    "temperature": 0.7
  }
}

# ... Agent 3-10 å…¨éƒ¨é‡å¤ç›¸åŒçš„é…ç½®
```

**é—®é¢˜**ï¼š
- å¦‚æœè¦ä¿®æ”¹ endpointï¼ˆä¾‹å¦‚åˆ‡æ¢åˆ°é•œåƒç«™ï¼‰ï¼Œéœ€è¦ä¿®æ”¹ 10 ä¸ª agentï¼
- å¦‚æœè¦æ›´æ–° API keyï¼Œéœ€è¦ä¿®æ”¹ 10 ä¸ª agentï¼
- é…ç½®é‡å¤ï¼Œç»´æŠ¤å›°éš¾

**âœ… æ–°æ–¹å¼ï¼ˆæ¨èï¼‰**ï¼š

```python
# æ‰€æœ‰ 10 ä¸ª agentï¼ˆé…ç½®å®Œå…¨ç›¸åŒï¼‰
{
  "name": "agent-1",
  "model": "anthropic/claude-3-5-sonnet-20250929"
}

{
  "name": "agent-2",
  "model": "anthropic/claude-3-5-sonnet-20250929"
}

# ... Agent 3-10
```

**ä¼˜åŠ¿**ï¼š
- âœ… é…ç½®ç®€æ´ï¼šæ¯ä¸ª agent åªéœ€ 1 è¡Œ
- âœ… ç»Ÿä¸€ç®¡ç†ï¼šä¿®æ”¹ Providerï¼Œæ‰€æœ‰ agent ç«‹å³ç”Ÿæ•ˆ
- âœ… æ˜“äºç»´æŠ¤ï¼šä¸éœ€è¦é€ä¸ªä¿®æ”¹ agent

##### åœºæ™¯ï¼šä½¿ç”¨è‡ªå®šä¹‰ Providerï¼ˆå¦‚ lingyunapi.comï¼‰

**æ­¥éª¤ 1**ï¼šåˆ›å»º Providerï¼ˆä¸€æ¬¡ï¼‰

```python
POST /v1/providers/
{
  "name": "lingyun-proxy",
  "provider_type": "openai",           # ä½¿ç”¨ OpenAI åè®®
  "base_url": "https://lingyunapi.com/v1",
  "provider_category": "byok"
}
```

**æ­¥éª¤ 2**ï¼šç³»ç»Ÿè‡ªåŠ¨åŒæ­¥æ¨¡å‹åˆ—è¡¨

Letta ä¼šè‡ªåŠ¨ä»è¯¥ Provider è·å–å¯ç”¨çš„æ¨¡å‹ï¼Œå¹¶å­˜å‚¨åˆ° `provider_models` è¡¨ï¼š
```
lingyun-proxy/claude-sonnet-4-5-20250929
lingyun-proxy/gpt-4o
lingyun-proxy/gemini-pro
...
```

**æ­¥éª¤ 3**ï¼šæ‰€æœ‰ agent éƒ½å¯ä»¥ä½¿ç”¨

```python
# Agent 1
{
  "name": "agent-1",
  "model": "lingyun-proxy/claude-sonnet-4-5-20250929"
}

# Agent 2
{
  "name": "agent-2",
  "model": "lingyun-proxy/gpt-4o"
}

# Agent 3
{
  "name": "agent-3",
  "model": "lingyun-proxy/claude-sonnet-4-5-20250929"
}
```

**ä¼˜åŠ¿**ï¼š
- âœ… é…ç½®é›†ä¸­ï¼šæ‰€æœ‰ agent å…±äº«åŒä¸€ä¸ª Provider é…ç½®
- âœ… åŠ¨æ€æ›´æ–°ï¼šä¿®æ”¹ Provider çš„ base_urlï¼Œæ‰€æœ‰ agent è‡ªåŠ¨åˆ‡æ¢
- âœ… å®‰å…¨æ€§ï¼šAPI key åŠ å¯†å­˜å‚¨åœ¨æ•°æ®åº“ä¸­

#### æ¶æ„ä¼˜åŠ¿å¯¹æ¯”

| æ–¹é¢ | è¯¦ç»†é…ç½®ï¼ˆå·²åºŸå¼ƒï¼‰ | Handle æ ¼å¼ï¼ˆæ¨èï¼‰ |
|------|------------------|-------------------|
| **é…ç½®é•¿åº¦** | 10+ è¡Œ | 1 è¡Œ |
| **å‡ºé”™æ¦‚ç‡** | é«˜ï¼ˆæ‰‹åŠ¨è¾“å…¥ï¼‰ | ä½ï¼ˆæ•°æ®åº“éªŒè¯ï¼‰ |
| **æ›´æ–°æ–¹å¼** | ä¿®æ”¹æ¯ä¸ª agent | ä¿®æ”¹ Provider ä¸€æ¬¡ |
| **API Key å®‰å…¨** | å¯èƒ½æš´éœ²åœ¨é…ç½®ä¸­ | åŠ å¯†å­˜å‚¨ |
| **å¯ç»´æŠ¤æ€§** | å·®ï¼ˆé‡å¤é…ç½®ï¼‰ | ä¼˜ï¼ˆé›†ä¸­ç®¡ç†ï¼‰ |
| **çµæ´»æ€§** | ä½ï¼ˆç¡¬ç¼–ç ï¼‰ | é«˜ï¼ˆåŠ¨æ€è°ƒæ•´ï¼‰ |
| **æ‰©å±•æ€§** | å·® | ä¼˜ï¼ˆæ˜“äºæ·»åŠ æ–°æ¨¡å‹ï¼‰ |
| **ä¸€è‡´æ€§** | éš¾ä»¥ä¿è¯ | è‡ªåŠ¨ä¿è¯ |

#### è¿ç§»å»ºè®®

**ä»æ—§é…ç½®è¿ç§»åˆ°æ–°é…ç½®**ï¼š

```python
# âŒ æ—§é…ç½®
{
  "llm_config": {
    "model": "claude-3-5-sonnet-20250929",
    "model_endpoint": "https://api.anthropic.com",
    "provider_name": "anthropic",
    "provider_category": "byok",
    "model_endpoint_type": "anthropic",
    "context_window": 200000
  }
}

# âœ… æ–°é…ç½®ï¼ˆç­‰ä»·ï¼‰
{
  "model": "anthropic/claude-3-5-sonnet-20250929"
}
```

**æ³¨æ„äº‹é¡¹**ï¼š
1. âœ… ç¡®ä¿æ•°æ®åº“ä¸­å·²æœ‰å¯¹åº”çš„ Provider
2. âœ… ç¡®ä¿æ¨¡å‹å·²åŒæ­¥åˆ° `provider_models` è¡¨
3. âœ… å¦‚æœéœ€è¦è‡ªå®šä¹‰å‚æ•°ï¼ˆå¦‚ temperatureï¼‰ï¼Œä½¿ç”¨ `model_settings`

**é«˜çº§ç”¨æ³•**ï¼šå¦‚æœç¡®å®éœ€è¦è‡ªå®šä¹‰æŸäº›å‚æ•°

```python
{
  "model": "anthropic/claude-3-5-sonnet-20250929",
  "model_settings": {
    "temperature": 0.8,  # è¦†ç›–é»˜è®¤å€¼
    "max_tokens": 2048   # è¦†ç›–é»˜è®¤å€¼
  }
}
```

#### æ€»ç»“

**åºŸå¼ƒè¯¦ç»†é…ç½®çš„åŸå› **ï¼š
1. **æ¶æ„å‡çº§**ï¼šä»ç¡¬ç¼–ç é…ç½®è½¬å‘æ•°æ®åº“é©±åŠ¨
2. **ç®€åŒ–ä½¿ç”¨**ï¼šå‡å°‘é…ç½®å¤æ‚åº¦ï¼Œé™ä½å‡ºé”™æ¦‚ç‡
3. **é›†ä¸­ç®¡ç†**ï¼šç»Ÿä¸€ç®¡ç† Provider å’Œæ¨¡å‹é…ç½®
4. **æé«˜å®‰å…¨æ€§**ï¼šAPI key åŠ å¯†å­˜å‚¨
5. **å¢å¼ºå¯ç»´æŠ¤æ€§**ï¼šä¸€æ¬¡ä¿®æ”¹ï¼Œå…¨å±€ç”Ÿæ•ˆ
6. **æ”¯æŒåŠ¨æ€æ›´æ–°**ï¼šæ— éœ€é‡å¯æœåŠ¡å³å¯æ›´æ–°é…ç½®

**è¿™æ˜¯ Letta é¡¹ç›®çš„ä¸€ä¸ªé‡è¦æ¶æ„æ”¹è¿›ï¼Œä½“ç°äº†ä»"é…ç½®é©±åŠ¨"åˆ°"æ•°æ®é©±åŠ¨"çš„è®¾è®¡ç†å¿µè½¬å˜ã€‚**

---

## 14. å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰ Providerï¼ˆ2026-01-04ï¼‰

### 14.1 é—®é¢˜

1. å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰ Providerï¼Ÿ
2. ä½¿ç”¨ç¬¬ä¸‰æ–¹èšåˆå¹³å°ï¼Œåœ¨åˆ›å»º agent ä¹‹å‰è¦å…ˆåˆ›å»ºå¥½ provider å—ï¼Ÿ
3. åˆ›å»º provider éœ€è¦æä¾› API KEY å—ï¼Ÿ

### 14.2 åˆ›å»º Provider çš„ä¸¤ç§æ–¹å¼

#### æ–¹å¼ 1ï¼šé€šè¿‡ç¯å¢ƒå˜é‡ï¼ˆæœ€ç®€å•ï¼Œæ¨èï¼‰âœ…

**é€‚ç”¨äº**ï¼šå¿«é€Ÿæµ‹è¯•ã€æœ¬åœ°å¼€å‘

**é…ç½®ç¤ºä¾‹**ï¼š
```bash
# .env æ–‡ä»¶
OPENAI_API_KEY=sk-tlegmZDKQBW5rce5sGaMdQeprOvDZgaRhr37KMhkieoiRIvh
OPENAI_API_BASE=https://lingyunapi.com/v1
```

**å·¥ä½œåŸç†**ï¼š
- Letta å¯åŠ¨æ—¶ä¼š**è‡ªåŠ¨åˆ›å»º**ä¸´æ—¶çš„ provider
- Provider åç§°ï¼š`openai-proxy`
- æ— éœ€æ‰‹åŠ¨åˆ›å»º

**ä¼˜ç‚¹**ï¼š
- âœ… æœ€ç®€å•
- âœ… ä¸éœ€è¦è°ƒç”¨ API
- âœ… é€‚åˆå¿«é€Ÿæµ‹è¯•

**ä½¿ç”¨æ–¹å¼**ï¼š
```bash
# 1. é…ç½®ç¯å¢ƒå˜é‡
cat > .env << EOF
OPENAI_API_KEY=sk-tlegmZDKQBW5rce5sGaMdQeprOvDZgaRhr37KMhkieoiRIvh
OPENAI_API_BASE=https://lingyunapi.com/v1
EOF

# 2. å¯åŠ¨ Letta
docker compose up -d

# 3. ç›´æ¥åˆ›å»º agentï¼ˆæ— éœ€æ‰‹åŠ¨åˆ›å»º providerï¼‰
curl -X POST "http://localhost:8283/v1/agents/" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "my-agent",
    "model": "openai-proxy/claude-sonnet-4-5-20250929",
    "embedding": "letta/letta-free"
  }'
```

#### æ–¹å¼ 2ï¼šé€šè¿‡ API æ‰‹åŠ¨åˆ›å»ºï¼ˆæŒä¹…åŒ–ï¼‰âœ…

**é€‚ç”¨äº**ï¼šç”Ÿäº§ç¯å¢ƒã€å¤šä¸ª agent å…±äº«

**HTTP è¯·æ±‚**ï¼š
```bash
curl -X POST "http://localhost:8283/v1/providers/" \
  -H "Content-Type: application/json" \
  -H "X-Actor-Id: user-00000000-0000-4000-8000-000000000000" \
  -d '{
    "name": "lingyun-proxy",
    "provider_type": "openai",
    "base_url": "https://lingyunapi.com/v1",
    "api_key": "sk-tlegmZDKQBW5rce5sGaMdQeprOvDZgaRhr37KMhkieoiRIvh"
  }'
```

**Python SDK**ï¼š
```python
from letta_client import Letta

client = Letta(base_url="http://localhost:8283")

provider = client.providers.create(
    name="lingyun-proxy",
    provider_type="openai",
    base_url="https://lingyunapi.com/v1",
    api_key="sk-tlegmZDKQBW5rce5sGaMdQeprOvDZgaRhr37KMhkieoiRIvh"
)

print(f"Provider created: {provider.id}")
```

### 14.3 å¿…é¡»å…ˆåˆ›å»º Provider å—ï¼Ÿ

#### ç­”æ¡ˆï¼šå–å†³äºä½¿ç”¨æ–¹å¼

##### åœºæ™¯ Aï¼šä½¿ç”¨ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰

**ä¸éœ€è¦æ‰‹åŠ¨åˆ›å»ºï¼** âœ…

Letta ä¼šè‡ªåŠ¨ä»ç¯å¢ƒå˜é‡åˆ›å»ºä¸´æ—¶ providerï¼Œæµç¨‹å¦‚ä¸‹ï¼š

```bash
# æ­¥éª¤ 1ï¼šé…ç½®ç¯å¢ƒå˜é‡
OPENAI_API_KEY=sk-...
OPENAI_API_BASE=https://lingyunapi.com/v1

# æ­¥éª¤ 2ï¼šå¯åŠ¨ Letta
docker compose up

# æ­¥éª¤ 3ï¼šç›´æ¥åˆ›å»º agent
# ç³»ç»Ÿä¼šè‡ªåŠ¨ä½¿ç”¨ç¯å¢ƒå˜é‡åˆ›å»ºçš„ä¸´æ—¶ provider
```

##### åœºæ™¯ Bï¼šä½¿ç”¨æ•°æ®åº“æŒä¹…åŒ–ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰

**éœ€è¦å…ˆåˆ›å»ºï¼** âœ…

å®Œæ•´æµç¨‹ï¼š

```
æ­¥éª¤ 1ï¼šåˆ›å»º Provider
   â†“
æ­¥éª¤ 2ï¼šç³»ç»Ÿè‡ªåŠ¨åŒæ­¥æ¨¡å‹åˆ—è¡¨
   â†“
æ­¥éª¤ 3ï¼šåˆ›å»º Agentï¼ˆä½¿ç”¨ Provider çš„æ¨¡å‹ï¼‰
```

**è¯¦ç»†æ­¥éª¤**ï¼š

```bash
# æ­¥éª¤ 1ï¼šåˆ›å»º Provider
curl -X POST "http://localhost:8283/v1/providers/" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "lingyun-proxy",
    "provider_type": "openai",
    "base_url": "https://lingyunapi.com/v1",
    "api_key": "sk-..."
  }'

# æ­¥éª¤ 2ï¼šç³»ç»Ÿè‡ªåŠ¨åŒæ­¥å¯ç”¨æ¨¡å‹
# Letta ä¼šè‡ªåŠ¨è°ƒç”¨ Provider çš„ APIï¼Œè·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
# å¹¶å­˜å‚¨åˆ° provider_models è¡¨

# æ­¥éª¤ 3ï¼šæŸ¥çœ‹å¯ç”¨æ¨¡å‹
curl http://localhost:8283/v1/providers/lingyun-proxy/models

# æ­¥éª¤ 4ï¼šåˆ›å»º Agentï¼ˆä½¿ç”¨ handle æ ¼å¼ï¼‰
curl -X POST "http://localhost:8283/v1/agents/" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "my-agent",
    "model": "lingyun-proxy/claude-sonnet-4-5-20250929",
    "embedding": "letta/letta-free"
  }'
```

### 14.4 éœ€è¦æä¾› API KEY å—ï¼Ÿ

#### ç­”æ¡ˆï¼š**æ˜¯çš„ï¼Œéœ€è¦ï¼** âœ…

##### é€šè¿‡ API åˆ›å»ºï¼ˆå¿…å¡«ï¼‰

**ProviderCreate Schema** (`letta/schemas/providers/base.py:241-248`):

```python
class ProviderCreate(ProviderBase):
    name: str = Field(..., description="The name of the provider.")
    provider_type: ProviderType = Field(..., description="The type of the provider.")
    api_key: str = Field(..., description="API key or secret key...")  # â† å¿…å¡«ï¼
    base_url: str | None = Field(None, description="Base URL...")
    ...
```

**å…³é”®ç‚¹**ï¼š
- `api_key` æ˜¯**å¿…å¡«å­—æ®µ**ï¼ˆ`...` è¡¨ç¤ºå¿…å¡«ï¼‰
- API key ä¼šè¢«**åŠ å¯†å­˜å‚¨**åˆ°æ•°æ®åº“

##### åŠ å¯†å­˜å‚¨æµç¨‹

**ä»£ç è¯æ®** (`letta/services/provider_manager.py:82-85`):

```python
# æ˜æ–‡ API key
request.api_key = "sk-..."

# è‡ªåŠ¨åŠ å¯†å­˜å‚¨
provider.api_key_enc = Secret.from_plaintext(request.api_key)

# å­˜å‚¨åˆ°æ•°æ®åº“ï¼ˆåŠ å¯†åï¼‰
new_provider = ProviderModel(...)
await new_provider.create_async(session)
```

**å…³é”®ç‚¹**ï¼š
- âœ… API key ä¼šè¢«åŠ å¯†
- âœ… ä¸ä¼šä»¥æ˜æ–‡å½¢å¼å­˜å‚¨åœ¨æ•°æ®åº“ä¸­
- âœ… ä½¿ç”¨æ—¶è‡ªåŠ¨è§£å¯†

##### ä½†æ˜¯ï¼å¦‚æœä½¿ç”¨ç¯å¢ƒå˜é‡...

**ä¸éœ€è¦åœ¨åˆ›å»º provider æ—¶æä¾› API key** âœ…

```bash
# ç¯å¢ƒå˜é‡æ–¹å¼
OPENAI_API_KEY=sk-...  # API key åœ¨è¿™é‡Œ
OPENAI_API_BASE=https://lingyunapi.com/v1

# å¯åŠ¨åï¼ŒLetta ä¼šè‡ªåŠ¨åˆ›å»ºä¸´æ—¶ provider
# è¿™ä¸ª provider ä¼šè‡ªåŠ¨ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„ API key
```

### 14.5 Provider å‚æ•°è¯¦è§£

#### å¿…å¡«å‚æ•°

**ProviderCreate** çš„å¿…å¡«å‚æ•°ï¼š

```python
{
  "name": "lingyun-proxy",        # å¿…å¡«ï¼šProvider åç§°
  "provider_type": "openai",       # å¿…å¡«ï¼šProvider ç±»å‹
  "api_key": "sk-..."              # å¿…å¡«ï¼šAPI key
}
```

#### å¯é€‰å‚æ•°

```python
{
  "name": "lingyun-proxy",
  "provider_type": "openai",
  "api_key": "sk-...",
  "base_url": "https://lingyunapi.com/v1",  # å¯é€‰ï¼šè‡ªå®šä¹‰ endpoint
  "region": "us-east-1",                     # å¯é€‰ï¼šAWS region (Bedrock)
  "api_version": "2023-05-15"                # å¯é€‰ï¼šAPI version
}
```

#### Provider ç±»å‹é€‰é¡¹

**ProviderType** æšä¸¾å€¼ï¼š
- `"openai"` - OpenAI åè®®
- `"anthropic"` - Anthropic åè®®
- `"google_ai"` - Google AI åè®®
- `"google_vertex"` - Google Vertex AI åè®®
- `"bedrock"` - AWS Bedrock åè®®
- `"azure"` - Azure OpenAI åè®®
- `"together"` - Together AI åè®®
- `"xai"` - xAI (Grok) åè®®
- `"groq"` - Groq åè®®
- `"deepseek"` - DeepSeek åè®®

### 14.6 ç¬¬ä¸‰æ–¹èšåˆå¹³å°é…ç½®ç¤ºä¾‹

#### ç¤ºä¾‹ 1ï¼šä½¿ç”¨ OpenAI å…¼å®¹åè®®ï¼ˆå¦‚ lingyunapi.comï¼‰

```bash
# æ–¹å¼ Aï¼šç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰
cat > .env << EOF
OPENAI_API_KEY=sk-tlegmZDKQBW5rce5sGaMdQeprOvDZgaRhr37KMhkieoiRIvh
OPENAI_API_BASE=https://lingyunapi.com/v1
EOF

# åˆ›å»º agent
{
  "model": "openai-proxy/claude-sonnet-4-5-20250929",
  "embedding": "letta/letta-free"
}
```

```bash
# æ–¹å¼ Bï¼šAPI åˆ›å»ºï¼ˆæŒä¹…åŒ–ï¼‰
curl -X POST "http://localhost:8283/v1/providers/" \
  -d '{
    "name": "lingyun",
    "provider_type": "openai",  # ä½¿ç”¨ OpenAI åè®®
    "base_url": "https://lingyunapi.com/v1",
    "api_key": "sk-tlegmZDKQBW5rce5sGaMdQeprOvDZgaRhr37KMhkieoiRIvh"
  }'

# ä½¿ç”¨è¯¥ provider åˆ›å»º agent
{
  "model": "lingyun/claude-sonnet-4-5-20250929",
  "embedding": "letta/letta-free"
}
```

#### ç¤ºä¾‹ 2ï¼šåŒæ—¶ä½¿ç”¨å¤šä¸ª Provider

```bash
# .env é…ç½®
OPENAI_API_KEY=sk-openai-xxxxx
OPENAI_API_BASE=https://api.openai.com/v1

ANTHROPIC_API_KEY=sk-ant-xxxxx

# è‡ªå®šä¹‰ providerï¼ˆé€šè¿‡ APIï¼‰
curl -X POST "http://localhost:8283/v1/providers/" \
  -d '{
    "name": "lingyun",
    "provider_type": "openai",
    "base_url": "https://lingyunapi.com/v1",
    "api_key": "sk-lingyun-xxxxx"
  }'
```

**åˆ›å»º agent æ—¶å¯ä»¥é€‰æ‹©ä¸åŒçš„ provider**ï¼š

```python
# ä½¿ç”¨å®˜æ–¹ OpenAI
{
  "model": "openai/gpt-4o",
  "embedding": "openai/text-embedding-3-small"
}

# ä½¿ç”¨å®˜æ–¹ Anthropic
{
  "model": "anthropic/claude-3-5-sonnet-20250929",
  "embedding": "letta/letta-free"
}

# ä½¿ç”¨è‡ªå®šä¹‰ lingyun
{
  "model": "lingyun/claude-sonnet-4-5-20250929",
  "embedding": "letta/letta-free"
}
```

### 14.7 Provider åˆ›å»ºåçš„è‡ªåŠ¨åŒæ­¥

**é‡è¦ç‰¹æ€§**ï¼šåˆ›å»º Provider åï¼ŒLetta ä¼š**è‡ªåŠ¨åŒæ­¥**å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ã€‚

**ä»£ç è¯æ®** (`letta/services/provider_manager.py:92-93`):

```python
# For BYOK providers, automatically sync available models
if is_byok:
    await self._sync_default_models_for_provider(provider_pydantic, actor)
```

**åŒæ­¥è¿‡ç¨‹**ï¼š
```
åˆ›å»º Provider
    â†“
Letta è°ƒç”¨ Provider çš„ API
    â†“
è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
    â†“
å­˜å‚¨åˆ° provider_models è¡¨
    â†“
åˆ›å»º agent æ—¶å¯ä»¥ä½¿ç”¨
```

**æŸ¥çœ‹åŒæ­¥çš„æ¨¡å‹**ï¼š

```bash
# æ–¹å¼ 1ï¼šé€šè¿‡ API
curl http://localhost:8283/v1/providers/{provider_id}/models

# æ–¹å¼ 2ï¼šé€šè¿‡ SDK
models = client.providers.models.list(provider_id)
for model in models:
    print(f"{model.handle} - {model.model}")
```

### 14.8 å®Œæ•´å·¥ä½œæµç¨‹ç¤ºä¾‹

#### ç¤ºä¾‹ï¼šä½¿ç”¨ lingyunapi.com åˆ›å»ºå®Œæ•´çš„ Agent

```python
from letta_client import Letta

client = Letta(base_url="http://localhost:8283")

# ========== æ–¹å¼ 1ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡ï¼ˆæ¨èï¼‰ ==========

# æ­¥éª¤ 1ï¼šé…ç½® .env
# OPENAI_API_KEY=sk-tlegmZDKQBW5rce5sGaMdQeprOvDZgaRhr37KMhkieoiRIvh
# OPENAI_API_BASE=https://lingyunapi.com/v1

# æ­¥éª¤ 2ï¼šå¯åŠ¨ Letta
# docker compose up

# æ­¥éª¤ 3ï¼šç›´æ¥åˆ›å»º agentï¼ˆæ— éœ€æ‰‹åŠ¨åˆ›å»º providerï¼‰
agent = client.agents.create(
    name="my-agent",
    model="openai-proxy/claude-sonnet-4-5-20250929",
    embedding="letta/letta-free",
    memory_blocks=[
        {"label": "human", "value": "A software engineer"},
        {"label": "persona", "value": "Helpful coding assistant"}
    ]
)

print(f"âœ… Agent created: {agent.id}")


# ========== æ–¹å¼ 2ï¼šä½¿ç”¨ API åˆ›å»ºï¼ˆæŒä¹…åŒ–ï¼‰ ==========

# æ­¥éª¤ 1ï¼šåˆ›å»º Provider
provider = client.providers.create(
    name="lingyun",
    provider_type="openai",  # ä½¿ç”¨ OpenAI åè®®
    base_url="https://lingyunapi.com/v1",
    api_key="sk-tlegmZDKQBW5rce5sGaMdQeprOvDZgaRhr37KMhkieoiRIvh"
)

print(f"âœ… Provider created: {provider.id}")
print(f"âœ… Provider name: {provider.name}")

# æ­¥éª¤ 2ï¼šæŸ¥çœ‹è‡ªåŠ¨åŒæ­¥çš„æ¨¡å‹åˆ—è¡¨
models = client.providers.models.list(provider.id)
print(f"\nâœ… Available models ({len(models)}):")
for model in models:
    print(f"  - {model.handle}")

# æ­¥éª¤ 3ï¼šåˆ›å»º Agentï¼ˆä½¿ç”¨ Provider çš„æ¨¡å‹ï¼‰
agent = client.agents.create(
    name="my-agent",
    model="lingyun/claude-sonnet-4-5-20250929",  # ä½¿ç”¨ Provider çš„æ¨¡å‹
    embedding="letta/letta-free",
    memory_blocks=[
        {"label": "human", "value": "A software engineer"},
        {"label": "persona", "value": "Helpful coding assistant"}
    ]
)

print(f"\nâœ… Agent created: {agent.id}")

# æ­¥éª¤ 4ï¼šä¸ Agent å¯¹è¯
response = client.agents.messages.create(
    agent_id=agent.id,
    messages=[{"role": "user", "content": "Hello!"}]
)

print(f"\nâœ… Response: {response.messages}")
```

### 14.9 å…³é”®è¦ç‚¹æ€»ç»“

#### é—®é¢˜ 1ï¼šéœ€è¦å…ˆåˆ›å»º Provider å—ï¼Ÿ

| ä½¿ç”¨æ–¹å¼ | æ˜¯å¦éœ€è¦å…ˆåˆ›å»º |
|---------|--------------|
| ç¯å¢ƒå˜é‡ | âŒ ä¸éœ€è¦ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰ |
| API åˆ›å»º | âœ… éœ€è¦ï¼ˆæ‰‹åŠ¨åˆ›å»ºï¼‰ |

#### é—®é¢˜ 2ï¼šéœ€è¦æä¾› API KEY å—ï¼Ÿ

| ä½¿ç”¨æ–¹å¼ | API KEY ä½ç½® |
|---------|------------|
| ç¯å¢ƒå˜é‡ | åœ¨ `.env` æ–‡ä»¶ä¸­ |
| API åˆ›å»º | åœ¨åˆ›å»ºè¯·æ±‚ä¸­ï¼ˆå¿…å¡«ï¼‰ |

#### é—®é¢˜ 3ï¼šæ¨èå“ªç§æ–¹å¼ï¼Ÿ

| åœºæ™¯ | æ¨èæ–¹å¼ | åŸå›  |
|-----|---------|------|
| æœ¬åœ°å¼€å‘ | ç¯å¢ƒå˜é‡ | æœ€ç®€å•ï¼Œå¿«é€Ÿå¼€å§‹ |
| ç”Ÿäº§ç¯å¢ƒ | API åˆ›å»º | æŒä¹…åŒ–ï¼Œä¾¿äºç®¡ç† |
| å¤šä¸ª Agent | API åˆ›å»º | å…±äº«é…ç½®ï¼Œç»Ÿä¸€ç®¡ç† |
| ä¸´æ—¶æµ‹è¯• | ç¯å¢ƒå˜é‡ | æ— éœ€æŒä¹…åŒ– |

### 14.10 å¸¸è§é—®é¢˜

#### Q1ï¼šå¦‚ä½•çŸ¥é“ Provider åˆ›å»ºæˆåŠŸï¼Ÿ

```bash
# æŸ¥çœ‹æ‰€æœ‰ providers
curl http://localhost:8283/v1/providers/

# æŸ¥çœ‹ç‰¹å®š provider
curl http://localhost:8283/v1/providers/{provider_id}

# æŸ¥çœ‹å¯ç”¨çš„æ¨¡å‹
curl http://localhost:8283/v1/providers/{provider_id}/models
```

#### Q2ï¼šå¦‚ä½•ä¿®æ”¹ Provider çš„ API keyï¼Ÿ

```python
provider = client.providers.update(
    provider_id="provider-xxx",
    api_key="new-api-key"
)
```

#### Q3ï¼šå¦‚ä½•åˆ é™¤ Providerï¼Ÿ

```python
client.providers.delete(provider_id="provider-xxx")
```

#### Q4ï¼šç¯å¢ƒå˜é‡æ–¹å¼åˆ›å»ºçš„ Provider å¯ä»¥æŒä¹…åŒ–å—ï¼Ÿ

**ç­”æ¡ˆ**ï¼šä¸å¯ä»¥ã€‚ç¯å¢ƒå˜é‡åˆ›å»ºçš„æ˜¯**ä¸´æ—¶ provider**ï¼Œé‡å¯åéœ€è¦é‡æ–°åˆ›å»ºã€‚

å¦‚æœéœ€è¦æŒä¹…åŒ–ï¼Œè¯·ä½¿ç”¨ API åˆ›å»ºã€‚

#### Q5ï¼šä¸€ä¸ª Agent å¯ä»¥ä½¿ç”¨å¤šä¸ª Provider å—ï¼Ÿ

**ç­”æ¡ˆ**ï¼šä¸å¯ä»¥ã€‚ä¸€ä¸ª Agent åªèƒ½ä½¿ç”¨ä¸€ä¸ª LLM Provider å’Œä¸€ä¸ª Embedding Providerã€‚

ä½†æ˜¯ï¼Œä¸åŒçš„ Agent å¯ä»¥ä½¿ç”¨ä¸åŒçš„ Providerã€‚

### 14.11 æœ€ä½³å®è·µå»ºè®®

#### âœ… æ¨èåšæ³•

1. **æœ¬åœ°å¼€å‘**ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡
   ```bash
   OPENAI_API_KEY=sk-...
   OPENAI_API_BASE=https://lingyunapi.com/v1
   ```

2. **ç”Ÿäº§ç¯å¢ƒ**ï¼šä½¿ç”¨ API åˆ›å»º
   ```python
   provider = client.providers.create(
       name="lingyun",
       provider_type="openai",
       base_url="https://lingyunapi.com/v1",
       api_key="sk-..."
   )
   ```

3. **æµ‹è¯•è¿æ¥**ï¼šåˆ›å»ºåéªŒè¯
   ```bash
   # æŸ¥çœ‹ provider
   curl http://localhost:8283/v1/providers/

   # æŸ¥çœ‹å¯ç”¨æ¨¡å‹
   curl http://localhost:8283/v1/providers/{provider_id}/models
   ```

#### âŒ é¿å…çš„é”™è¯¯

1. **ä¸è¦æ··æ·†åè®®ç±»å‹**
   ```python
   # âŒ é”™è¯¯ï¼šAnthropic æ¨¡å‹ä½¿ç”¨ Anthropic åè®®
   {
     "provider_type": "anthropic",
     "base_url": "https://lingyunapi.com/v1"  # è¿™ä¸ª API ä½¿ç”¨ OpenAI åè®®ï¼
   }

   # âœ… æ­£ç¡®ï¼šä½¿ç”¨ OpenAI åè®®
   {
     "provider_type": "openai",
     "base_url": "https://lingyunapi.com/v1"
   }
   ```

2. **ä¸è¦å¿˜è®° base_url çš„è·¯å¾„**
   ```python
   # âŒ é”™è¯¯
   "base_url": "https://lingyunapi.com"

   # âœ… æ­£ç¡®
   "base_url": "https://lingyunapi.com/v1"
   ```

3. **ä¸è¦åœ¨ç¯å¢ƒå˜é‡ä¸­ç¡¬ç¼–ç å¤šä¸ª endpoint**
   ```bash
   # âŒ æ··ä¹±
   OPENAI_API_KEY=sk-xxx
   ANTHROPIC_API_KEY=sk-yyy
   CUSTOM_API_KEY=sk-zzz  # è¿™ä¸ªå¯èƒ½ä¸ä¼šç”Ÿæ•ˆ

   # âœ… æ¸…æ™°
   # å¯¹äºè‡ªå®šä¹‰ endpointï¼Œä½¿ç”¨ API åˆ›å»º Provider
   ```

---

## 15. Letta UI åˆ›å»º Agent çš„ Provider é€‰æ‹©é—®é¢˜ï¼ˆ2026-01-04ï¼‰

### 15.1 é—®é¢˜æè¿°

**ç”¨æˆ·ç¯å¢ƒ**ï¼š
```bash
# .env ä¸­åªé…ç½®äº†
OPENAI_API_KEY=sk-...
OPENAI_API_BASE=https://lingyunapi.com/v1

# æ²¡æœ‰é…ç½® ANTHROPIC_API_KEY
```

**é—®é¢˜**ï¼š
é€šè¿‡ Letta å®˜æ–¹ UI åˆ›å»º agent æ—¶ï¼ŒUI é»˜è®¤é€‰æ‹©äº† **Anthropic å®˜æ–¹** ä½œä¸º providerï¼Œè€Œä¸æ˜¯ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­é…ç½®çš„ `lingyunapi.com`ã€‚

### 15.2 é—®é¢˜å¤ç°

#### å®é™…æ¡ˆä¾‹

**ç”¨æˆ·åˆ›å»ºçš„ agentï¼ˆscratch-agent_copyï¼‰**ï¼š
```json
{
  "model": "claude-sonnet-4-5-20250929",
  "model_endpoint_type": "anthropic",        // âŒ é”™è¯¯ï¼
  "model_endpoint": "https://api.anthropic.com/v1",  // âŒ ä½¿ç”¨ Anthropic å®˜æ–¹
  "provider_name": "anthropic",              // âŒ
  "provider_category": "base"
}
```

**ç¯å¢ƒå®é™…æƒ…å†µ**ï¼š
- âŒ ç”¨æˆ·**æ²¡æœ‰**æä¾› `ANTHROPIC_API_KEY`
- âœ… ç”¨æˆ·åªæä¾›äº† `OPENAI_API_KEY` å’Œ `OPENAI_API_BASE`
- âœ… æœŸæœ›ä½¿ç”¨ `https://lingyunapi.com/v1`

#### æµ‹è¯•ç»“æœ

```bash
# ä½¿ç”¨ scratch-agent_copy å‘é€æ¶ˆæ¯
curl -X POST "http://localhost:8283/v1/agents/agent-57f6e9db-3904-4b6c-b938-9d61d289b295/messages" \
  -d '{"messages":[{"role":"user","content":"Hello"}]}'

# è¿”å›é”™è¯¯
{
  "error": {
    "message": "Could not resolve authentication method. Expected either
                api_key or auth_token to be set..."
  }
}
```

### 15.3 æ ¹æœ¬åŸå› åˆ†æ

#### Letta çš„ Base Provider æœºåˆ¶

**ä»€ä¹ˆæ˜¯ Base Provider**ï¼Ÿ

Letta å†…ç½®äº†ä¸€äº›"base providers"ï¼Œè¿™äº›æ˜¯ Letta å®˜æ–¹æä¾›çš„ã€é¢„é…ç½®çš„ providerï¼š

1. **letta** - Letta è‡ªå·±çš„æ¨ç†æœåŠ¡
2. **openai** - OpenAI å®˜æ–¹ï¼ˆå¯è¢«ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
3. **anthropic** - Anthropic å®˜æ–¹ï¼ˆéœ€è¦ ANTHROPIC_API_KEYï¼‰
4. å…¶ä»–...

**ç¯å¢ƒå˜é‡å¦‚ä½•å½±å“ Base Provider**ï¼š

```bash
# ç”¨æˆ·é…ç½®
OPENAI_API_KEY=sk-...
OPENAI_API_BASE=https://lingyunapi.com/v1

# Letta è‡ªåŠ¨åˆ›å»º base provider
provider_name: "openai"
provider_category: "base"
model_endpoint: "https://lingyunapi.com/v1"  # âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡
```

**éªŒè¯**ï¼š
```bash
# æŸ¥çœ‹å¯ç”¨æ¨¡å‹
curl http://localhost:8283/v1/models/

# ç»“æœï¼š
{
  "handle": "openai-proxy/claude-sonnet-4-5-20250929",
  "provider_name": "openai",
  "provider_category": "base",  // â† base provider
  "model_endpoint": "https://lingyunapi.com/v1"  // âœ… ä½¿ç”¨ç”¨æˆ·çš„é…ç½®
}
```

#### UI çš„é—®é¢˜

**UI åˆ›å»º agent æ—¶çš„è¡Œä¸º**ï¼š

1. âŒ **é»˜è®¤é€‰æ‹© Anthropic å®˜æ–¹**
   - å³ä½¿ç¯å¢ƒå˜é‡ä¸­æ²¡æœ‰ `ANTHROPIC_API_KEY`
   - å³ä½¿ç¯å¢ƒå˜é‡ä¸­é…ç½®äº† `OPENAI_API_BASE`

2. âŒ **ä¸æ£€æŸ¥ Provider å¯ç”¨æ€§**
   - ä¸æ£€æŸ¥é€‰æ‹©çš„ provider æ˜¯å¦æœ‰ API key
   - ä¸æ£€æŸ¥ provider æ˜¯å¦å¯è¾¾
   - ç›´æ¥åˆ›å»ºé…ç½®

3. âŒ **å¿½ç•¥ç¯å¢ƒå˜é‡é…ç½®**
   - åº”è¯¥ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®çš„ provider
   - æˆ–è€…è‡³å°‘æç¤ºç”¨æˆ·æœ‰å¯ç”¨çš„æ›¿ä»£é€‰é¡¹

### 15.4 å¯¹æ¯”åˆ†æ

#### âœ… æ­£ç¡®çš„é…ç½®ï¼ˆKleo agentï¼‰

```json
{
  "model_endpoint_type": "openai",        // âœ…
  "model_endpoint": "https://lingyunapi.com/v1",  // âœ…
  "provider_name": "openai",               // âœ…
  "handle": "openai-proxy/claude-sonnet-4-5-20250929"  // âœ…
}
```

**å·¥ä½œåŸç†**ï¼š
```bash
# 1. æ£€æŸ¥ model_endpoint_type = "openai"
# 2. ä½¿ç”¨ OpenAIClient
# 3. è¯»å–ç¯å¢ƒå˜é‡ OPENAI_API_KEY
# 4. ä½¿ç”¨ç¯å¢ƒå˜é‡ OPENAI_API_BASE
# 5. å‘é€è¯·æ±‚åˆ° https://lingyunapi.com/v1 âœ…
```

#### âŒ é”™è¯¯çš„é…ç½®ï¼ˆscratch-agent_copyï¼‰

```json
{
  "model_endpoint_type": "anthropic",     // âŒ
  "model_endpoint": "https://api.anthropic.com/v1",  // âŒ
  "provider_name": "anthropic",           // âŒ
  "handle": "anthropic/claude-sonnet-4-5-20250929"  // âŒ
}
```

**æ‰§è¡Œæµç¨‹**ï¼š
```bash
# 1. æ£€æŸ¥ model_endpoint_type = "anthropic"
# 2. ä½¿ç”¨ AnthropicClient
# 3. å°è¯•è¯»å– ANTHROPIC_API_KEY
# 4. âŒ æ‰¾ä¸åˆ° ANTHROPIC_API_KEY
# 5. âŒ è®¤è¯å¤±è´¥
```

### 15.5 æ•°æ®åº“éªŒè¯

**æŸ¥çœ‹æ•°æ®åº“ä¸­çš„ providers è¡¨**ï¼š

```bash
docker exec root_letta-server_1 psql -U letta -d letta \
  -c 'SELECT name, provider_type, provider_category, base_url FROM providers;'

# ç»“æœï¼š
# (0 rows)
```

**ç»“è®º**ï¼š
- âœ… æ²¡æœ‰æ‰‹åŠ¨åˆ›å»ºçš„æŒä¹…åŒ– providers
- âœ… ä½¿ç”¨çš„éƒ½æ˜¯ Letta çš„å†…ç½® base providers
- âœ… `openai` base provider è¢«ç¯å¢ƒå˜é‡è¦†ç›–ï¼ŒæŒ‡å‘ `lingyunapi.com`
- âŒ `anthropic` base provider æ²¡æœ‰è¢«è¦†ç›–ï¼ˆå› ä¸ºç¼ºå°‘ `ANTHROPIC_API_KEY`ï¼‰

### 15.6 å¯ç”¨æ¨¡å‹åˆ—è¡¨

**æŸ¥è¯¢å¯ç”¨æ¨¡å‹**ï¼š

```bash
curl http://localhost:8283/v1/models/
```

**è¿”å›ç»“æœåˆ†æ**ï¼š

âœ… **å¯ç”¨çš„æ¨¡å‹**ï¼ˆä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®ï¼‰ï¼š
```json
{
  "handle": "openai-proxy/claude-sonnet-4-5-20250929",
  "provider_name": "openai",
  "provider_category": "base",
  "model_endpoint": "https://lingyunapi.com/v1"
}
```

âŒ **ä¸å¯ç”¨çš„æ¨¡å‹**ï¼ˆUI é»˜è®¤é€‰æ‹©ï¼‰ï¼š
```json
{
  "handle": "anthropic/claude-sonnet-4-5-20250929",  // â† ä¸åœ¨åˆ—è¡¨ä¸­ï¼
  "provider_name": "anthropic",
  "model_endpoint": "https://api.anthropic.com/v1"
}
```

**å…³é”®å‘ç°**ï¼š
- `/v1/models/` åˆ—è¡¨ä¸­**æ²¡æœ‰** `anthropic/` å¼€å¤´çš„æ¨¡å‹
- UI å´å…è®¸é€‰æ‹©å¹¶ä¸å­˜åœ¨çš„æ¨¡å‹
- è¿™å¯¼è‡´äº†åˆ›å»ºæ— æ³•ä½¿ç”¨çš„ agent

### 15.7 è§£å†³æ–¹æ¡ˆ

#### æ–¹æ¡ˆ 1ï¼šé€šè¿‡ UI åˆ›å»ºï¼ˆæ‰‹åŠ¨é€‰æ‹©æ­£ç¡®æ¨¡å‹ï¼‰âœ…

åœ¨ UI ä¸­åˆ›å»º agent æ—¶ï¼š
1. âœ… **æ‰‹åŠ¨é€‰æ‹©** `openai-proxy/claude-sonnet-4-5-20250929`
2. âŒ **ä¸è¦**ä¾èµ– UI çš„é»˜è®¤é€‰æ‹©
3. âœ… ç¡®è®¤é€‰æ‹©çš„æ¨¡å‹åœ¨ `/v1/models/` åˆ—è¡¨ä¸­

#### æ–¹æ¡ˆ 2ï¼šé€šè¿‡ API åˆ›å»ºï¼ˆæ¨èï¼‰âœ…

ä½¿ç”¨æ˜ç¡®çš„ model handleï¼š

```bash
curl -X POST "http://localhost:8283/v1/agents/" \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "my-agent",
    "model": "openai-proxy/claude-sonnet-4-5-20250929",
    "embedding": "letta/letta-free",
    "memory_blocks": [
      {"label": "human", "value": "User info"},
      {"label": "persona", "value": "Helpful assistant"}
    ]
  }'
```

**Python SDK**ï¼š
```python
from letta_client import Letta

client = Letta(base_url="http://localhost:8283")

agent = client.agents.create(
    name="my-agent",
    model="openai-proxy/claude-sonnet-4-5-20250929",  # âœ… ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®
    embedding="letta/letta-free",
    memory_blocks=[
        {"label": "human", "value": "User info"},
        {"label": "persona", "value": "Helpful assistant"}
    ]
)
```

#### æ–¹æ¡ˆ 3ï¼šåˆ é™¤é”™è¯¯çš„ Agent

```bash
# åˆ é™¤æ— æ³•ä½¿ç”¨çš„ agent
curl -X DELETE \
  "http://localhost:8283/v1/agents/agent-57f6e9db-3904-4b6c-b938-9d61d289b295" \
  -H "Authorization: Bearer YOUR_TOKEN"
```

### 15.8 UI çš„è¡Œä¸ºç¼ºé™·æ€»ç»“

| é—®é¢˜ | æè¿° | å½±å“ |
|------|------|------|
| **é»˜è®¤é€‰æ‹©ä¸å½“** | UI é»˜è®¤é€‰æ‹© Anthropic å®˜æ–¹ï¼Œè€Œä¸æ˜¯ç¯å¢ƒå˜é‡é…ç½®çš„ provider | åˆ›å»ºæ— æ³•ä½¿ç”¨çš„ agent |
| **ç¼ºå°‘éªŒè¯** | ä¸æ£€æŸ¥é€‰æ‹©çš„ provider æ˜¯å¦å¯ç”¨ï¼ˆæ˜¯å¦æœ‰ API keyï¼‰ | åˆ›å»ºåæ‰å‘ç°æ— æ³•ä½¿ç”¨ |
| **å¿½ç•¥ç¯å¢ƒå˜é‡** | ä¸ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®çš„ provider | ç”¨æˆ·é…ç½®è¢«å¿½ç•¥ |
| **ç¼ºå°‘æç¤º** | ä¸æç¤ºç”¨æˆ·æœ‰å¯ç”¨çš„æ›¿ä»£é€‰é¡¹ | ç”¨æˆ·ä¸çŸ¥é“æ­£ç¡®åšæ³• |
| **æ¨¡å‹åˆ—è¡¨ä¸åŒæ­¥** | `/v1/models/` åˆ—è¡¨å’Œ UI é€‰é¡¹ä¸ä¸€è‡´ | å¯ä»¥é€‰æ‹©ä¸å¯ç”¨çš„æ¨¡å‹ |

### 15.9 æœ€ä½³å®è·µå»ºè®®

#### âœ… æ¨èåšæ³•

1. **åˆ›å»ºå‰å…ˆæŸ¥çœ‹å¯ç”¨æ¨¡å‹**
   ```bash
   # æŸ¥çœ‹ /v1/models/ åˆ—è¡¨
   curl http://localhost:8283/v1/models/

   # åªé€‰æ‹©åˆ—è¡¨ä¸­å­˜åœ¨çš„æ¨¡å‹
   ```

2. **ä¼˜å…ˆä½¿ç”¨ API/SDK åˆ›å»º**
   - æ›´å¯æ§
   - ä¸ä¼šå‡ºç° UI çš„é»˜è®¤é€‰æ‹©é—®é¢˜
   - å¯ä»¥æ˜ç¡®æŒ‡å®šæ‰€æœ‰å‚æ•°

3. **éªŒè¯ Agent é…ç½®**
   ```bash
   # åˆ›å»ºåæ£€æŸ¥ agent é…ç½®
   curl http://localhost:8283/v1/agents/{agent_id}

   # ç¡®è®¤ model_endpoint_type å’Œ model_endpoint
   ```

4. **æµ‹è¯• Agent**
   ```bash
   # å‘é€æµ‹è¯•æ¶ˆæ¯
   curl -X POST "http://localhost:8283/v1/agents/{agent_id}/messages" \
     -d '{"messages":[{"role":"user","content":"test"}]}'
   ```

#### âŒ é¿å…çš„åšæ³•

1. **ä¸è¦ç›²ç›®ä½¿ç”¨ UI çš„é»˜è®¤é€‰æ‹©**
   - UI é»˜è®¤é€‰æ‹©çš„æ¨¡å‹å¯èƒ½ä¸å¯ç”¨
   - éœ€è¦æ‰‹åŠ¨ç¡®è®¤

2. **ä¸è¦å‡è®¾é…ç½®ä¼šè‡ªåŠ¨åº”ç”¨**
   - ç¯å¢ƒå˜é‡åªå½±å“ç‰¹å®šçš„ base providers
   - ä¸ä¼šè¦†ç›–æ‰€æœ‰ providers

3. **ä¸è¦å¿½ç•¥é”™è¯¯ä¿¡æ¯**
   - "authentication method" é”™è¯¯ = provider é…ç½®é—®é¢˜
   - æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„ API key

### 15.10 ä»£ç è¯æ®

#### å¯ç”¨æ¨¡å‹åˆ—è¡¨é€»è¾‘

**ä»£ç ä½ç½®**ï¼š`letta/server/server.py`

Letta ä¼šåœ¨å¯åŠ¨æ—¶ï¼š
1. è¯»å–ç¯å¢ƒå˜é‡
2. åˆ›å»º/æ›´æ–° base providers
3. åŒæ­¥å¯ç”¨æ¨¡å‹åˆ°æ•°æ®åº“

```python
# ä¼ªä»£ç 
if OPENAI_API_BASE:
    create_base_provider(
        name="openai",
        base_url=OPENAI_API_BASE,  # lingyunapi.com/v1
        api_key=OPENAI_API_KEY
    )

sync_models_to_database()
```

#### Agent åˆ›å»ºé€»è¾‘

**ä»£ç ä½ç½®**ï¼š`letta/server/rest_api/routers/v1/agents.py`

```python
async def create_agent(agent: CreateAgentRequest):
    # UI è°ƒç”¨æ­¤æ¥å£
    # agent.model å¯èƒ½æ˜¯ "claude-sonnet-4-5-20250929"
    # UI éœ€è¦å†³å®šä½¿ç”¨å“ªä¸ª provider

    # âŒ é—®é¢˜ï¼šUI é»˜è®¤é€‰æ‹©äº† anthropic provider
    # è€Œä¸æ˜¯æ£€æŸ¥æ˜¯å¦æœ‰ ANTHROPIC_API_KEY
```

### 15.11 æ€»ç»“

**æ ¸å¿ƒé—®é¢˜**ï¼š
- Letta UI åˆ›å»º agent æ—¶ï¼Œé»˜è®¤é€‰æ‹©äº† Anthropic å®˜æ–¹ provider
- ä½†ç”¨æˆ·ç¯å¢ƒå˜é‡ä¸­åªé…ç½®äº† `OPENAI_API_KEY` å’Œ `OPENAI_API_BASE`
- å¯¼è‡´åˆ›å»ºçš„ agent æ— æ³•ä½¿ç”¨ï¼ˆè®¤è¯å¤±è´¥ï¼‰

**æ ¹æœ¬åŸå› **ï¼š
1. UI çš„é»˜è®¤é€‰æ‹©é€»è¾‘ä¸å½“
2. æ²¡æœ‰æ£€æŸ¥ provider å¯ç”¨æ€§
3. æ²¡æœ‰ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. âœ… ä½¿ç”¨ `openai-proxy/*` æ ¼å¼çš„æ¨¡å‹ï¼ˆä¸ç¯å¢ƒå˜é‡é…ç½®ä¸€è‡´ï¼‰
2. âœ… ä¼˜å…ˆä½¿ç”¨ API/SDK åˆ›å»º agent
3. âœ… åˆ›å»ºå‰æŸ¥çœ‹ `/v1/models/` åˆ—è¡¨
4. âœ… åˆ›å»ºåéªŒè¯ agent é…ç½®

**è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„å‘ç°**ï¼Œæ­ç¤ºäº† Letta UI åœ¨ provider é€‰æ‹©ä¸Šçš„è®¾è®¡ç¼ºé™·ï¼Œä»¥åŠç¯å¢ƒå˜é‡ä¸ base provider çš„äº¤äº’æœºåˆ¶ã€‚

---

## 16. OPENAI_API_HEADERS ç¯å¢ƒå˜é‡çš„çœŸç›¸ï¼ˆ2026-01-05ï¼‰

### 16.1 é—®é¢˜èƒŒæ™¯

åœ¨ç”Ÿäº§ç¯å¢ƒçš„ `.env` æ–‡ä»¶ä¸­å‘ç°äº†ä»¥ä¸‹é…ç½®ï¼š

```bash
OPENAI_API_KEY=sk-tlegmZDKQBW5rce5sGaMdQeprOvDZgaRhr37KMhkieoiRIvh
OPENAI_API_BASE=https://lingyunapi.com/v1
OPENAI_API_HEADERS={"Authorization":"Bearer ${OPENAI_API_KEY}","Content-Type":"application/json","Accept":"*/*","Host":"lingyunapi.com","Connection":"keep-alive","User-Agent":"Apifox/1.0.0"}
```

**ç”¨æˆ·æŠ¥å‘Š**ï¼šä¹‹å‰ Kleo agent å‡ºç°è®¤è¯å¤±è´¥ï¼Œæ·»åŠ  `OPENAI_API_HEADERS` åé—®é¢˜è§£å†³ã€‚

**ç–‘é—®**ï¼šè¿™ä¸ªç¯å¢ƒå˜é‡çœŸçš„æœ‰ç”¨å—ï¼Ÿè¿˜æ˜¯å·§åˆï¼Ÿ

### 16.2 æ·±å…¥è°ƒæŸ¥

#### 16.2.1 Letta ä»£ç ä¸­æ˜¯å¦æ”¯æŒ OPENAI_API_HEADERSï¼Ÿ

**æœç´¢ç»“æœ**ï¼š

```bash
# æœç´¢æ‰€æœ‰ä¸ OPENAI_API_HEADERS ç›¸å…³çš„ä»£ç 
grep -r "OPENAI_API_HEADERS" letta/
# ç»“æœï¼šæ— åŒ¹é…
```

**ç»“è®º**ï¼šLetta ä»£ç ä¸­**å®Œå…¨æ²¡æœ‰**è¯»å– `OPENAI_API_HEADERS` ç¯å¢ƒå˜é‡çš„é€»è¾‘ã€‚

#### 16.2.2 OpenAI Client å¦‚ä½•å¤„ç† headersï¼Ÿ

**ä»£ç ä½ç½®**ï¼š`letta/llm_api/openai_client.py`

**OpenAI Client çš„ header å¤„ç†**ï¼š

```python
def _prepare_client_kwargs(self, llm_config: LLMConfig) -> dict:
    api_key, _, _ = self.get_byok_overrides(llm_config)

    # é»˜è®¤ä½¿ç”¨å…¨å±€ OpenAI key
    if not api_key:
        api_key = model_settings.openai_api_key or os.environ.get("OPENAI_API_KEY")

    kwargs = {"api_key": api_key, "base_url": llm_config.model_endpoint}

    # âš ï¸ åªæœ‰ OpenRouter æ‰æ”¯æŒè‡ªå®šä¹‰ headers
    is_openrouter = (llm_config.model_endpoint and "openrouter.ai" in llm_config.model_endpoint) or (
        llm_config.provider_name == "openrouter"
    )
    if is_openrouter:
        or_key = model_settings.openrouter_api_key or os.environ.get("OPENROUTER_API_KEY")
        if or_key:
            kwargs["api_key"] = or_key
        # æ·»åŠ  OpenRouter ç‰¹å®š headers
        headers = {}
        if model_settings.openrouter_referer:
            headers["HTTP-Referer"] = model_settings.openrouter_referer
        if model_settings.openrouter_title:
            headers["X-Title"] = model_settings.openrouter_title
        if headers:
            kwargs["default_headers"] = headers

    # OpenAI client å¿…é¡»æœ‰ä¸€ä¸ª API key
    kwargs["api_key"] = kwargs.get("api_key") or "DUMMY_API_KEY"

    return kwargs
```

**å…³é”®å‘ç°**ï¼š

1. âœ… OpenAI Client æ”¯æŒé€šè¿‡ `default_headers` å‚æ•°æ·»åŠ è‡ªå®šä¹‰ headers
2. âŒ **ä½†åªå¯¹ OpenRouter å¯ç”¨**ï¼Œä¸å¯¹å…¶ä»– provider å¯ç”¨
3. âŒ **æ²¡æœ‰**ä»ç¯å¢ƒå˜é‡è¯»å– `OPENAI_API_HEADERS` çš„é€»è¾‘

#### 16.2.3 OpenAI Python SDK çš„è®¤è¯æœºåˆ¶

**OpenAI SDK æ ‡å‡†**ï¼š

```python
from openai import OpenAI

# æ ‡å‡†è®¤è¯æ–¹å¼
client = OpenAI(
    api_key="sk-...",  # æˆ–ä» OPENAI_API_KEY ç¯å¢ƒå˜é‡è¯»å–
    base_url="https://api.example.com/v1"
)

# SDK è‡ªåŠ¨åœ¨æ‰€æœ‰è¯·æ±‚ä¸­æ·»åŠ ï¼š
# Authorization: Bearer sk-...
```

**å…³é”®ç‰¹æ€§**ï¼š

1. OpenAI SDK **è‡ªåŠ¨**ä» `OPENAI_API_KEY` ç¯å¢ƒå˜é‡è¯»å– API key
2. SDK **è‡ªåŠ¨**åœ¨æ‰€æœ‰ HTTP è¯·æ±‚ä¸­æ·»åŠ  `Authorization: Bearer <key>` header
3. **ä¸éœ€è¦**æ‰‹åŠ¨è®¾ç½® `Authorization` header

#### 16.2.4 lingyunapi.com çš„è®¤è¯æœºåˆ¶

**lingyunapi.com** æ˜¯ä¸€ä¸ª OpenAI-compatible API ä»£ç†æœåŠ¡ã€‚

**æ ‡å‡†é…ç½®**ï¼š

```bash
OPENAI_API_KEY=sk-your-key
OPENAI_API_BASE=https://lingyunapi.com/v1
```

**å·¥ä½œåŸç†**ï¼š

1. Letta ä½¿ç”¨ OpenAI Python SDK
2. SDK ä» `OPENAI_API_KEY` è¯»å– API key
3. SDK è‡ªåŠ¨æ·»åŠ  `Authorization: Bearer sk-your-key` header
4. lingyunapi.com æ¥å—æ ‡å‡†çš„ OpenAI è®¤è¯æ–¹å¼

### 16.3 å®éªŒéªŒè¯

#### 16.3.1 æµ‹è¯•ç¯å¢ƒé…ç½®

**ç§»é™¤ OPENAI_API_HEADERS å‰çš„é…ç½®**ï¼š

```yaml
# docker-compose.yml (æ—§)
environment:
  - OPENAI_API_KEY=${OPENAI_API_KEY}
  - OPENAI_API_BASE=${OPENAI_API_BASE}
  - OPENAI_API_HEADERS=${OPENAI_API_HEADERS}  # âŒ è¿™ä¸ªæ²¡ç”¨
  - LETTA_SERVER_PASSWORD=${LETTA_SERVER_PASSWORD}
```

**ç§»é™¤ OPENAI_API_HEADERS åçš„é…ç½®**ï¼š

```yaml
# docker-compose.yml (æ–°)
environment:
  - OPENAI_API_KEY=${OPENAI_API_KEY}
  - OPENAI_API_BASE=${OPENAI_API_BASE}
  # âŒ ç§»é™¤äº†è¿™ä¸€è¡Œ - OPENAI_API_HEADERS=${OPENAI_API_HEADERS}
  - LETTA_SERVER_PASSWORD=${LETTA_SERVER_PASSWORD}
```

**.env æ–‡ä»¶**ï¼š

```bash
OPENAI_API_KEY=sk-tlegmZDKQBW5rce5sGaMdQeprOvDZgaRhr37KMhkieoiRIvh
OPENAI_API_BASE=https://lingyunapi.com/v1
LETTA_SERVER_PASSWORD=LiXinYing0115@@
# OPENAI_API_HEADERS ç§»é™¤ - ä¸éœ€è¦
```

#### 16.3.2 åˆ›å»ºæµ‹è¯• Agent

**Agent é…ç½®**ï¼š

```json
{
  "name": "test-lingyun",
  "llm_config": {
    "model": "claude-haiku-4-5-20251001",
    "model_endpoint_type": "openai",
    "model_endpoint": "https://lingyunapi.com/v1",
    "provider_name": "openai",
    "provider_category": "base",
    "context_window": 30000
  },
  "embedding_config": {
    "embedding_model": "letta",
    "embedding_endpoint_type": "hugging-face",
    "embedding_endpoint": "https://embeddings.letta.com"
  }
}
```

**åˆ›å»ºç»“æœ**ï¼šâœ… æˆåŠŸ

#### 16.3.3 æµ‹è¯•æ¶ˆæ¯å‘é€

**æµ‹è¯• 1**ï¼š

```bash
# ç”¨æˆ·æ¶ˆæ¯
"Hello! Can you hear me? Please respond with just: Yes, I can hear you!"

# AI å“åº”
"Yes, I can hear you!"
```

**æµ‹è¯• 2**ï¼š

```bash
# ç”¨æˆ·æ¶ˆæ¯
"What is 2+2? Answer with just the number."

# AI å“åº”
"4"
```

**ç»“è®º**ï¼šâœ… **è®¤è¯å®Œå…¨æ­£å¸¸ï¼Œä¸éœ€è¦ OPENAI_API_HEADERS**

### 16.4 ä¸ºä»€ä¹ˆä¹‹å‰"æ·»åŠ  headers è§£å†³äº†é—®é¢˜"ï¼Ÿ

å¯èƒ½çš„åŸå› ï¼š

#### åŸå›  1ï¼šå·§åˆï¼ˆæœ€å¯èƒ½ï¼‰

çœŸæ­£è§£å†³é—®é¢˜çš„å¯èƒ½æ˜¯å…¶ä»–é…ç½®å˜æ›´ï¼š

1. **æ­£ç¡®è®¾ç½®äº† OPENAI_API_BASE**
   ```bash
   # ä¹‹å‰å¯èƒ½æ²¡æœ‰è®¾ç½®æˆ–è®¾ç½®é”™è¯¯
   OPENAI_API_BASE=https://lingyunapi.com/v1
   ```

2. **æ­£ç¡®è®¾ç½®äº† OPENAI_API_KEY**
   ```bash
   # ä¹‹å‰å¯èƒ½ key é”™è¯¯æˆ–è¿‡æœŸ
   OPENAI_API_KEY=sk-...
   ```

3. **Agent é…ç½®ä¿®æ”¹**
   - å¯èƒ½åŒæ—¶ä¿®æ”¹äº† agent çš„ `model_endpoint`
   - å¯èƒ½æ”¹ç”¨äº†æ­£ç¡®çš„ `model_endpoint_type`

#### åŸå›  2ï¼šè¯¯è§£

- é—®é¢˜æœ¬æ¥å°±æ²¡è§£å†³ï¼Œåªæ˜¯çœ‹èµ·æ¥è§£å†³äº†
- æˆ–è€…é—®é¢˜æ˜¯é—´æ­‡æ€§çš„ï¼Œæ°å¥½é‚£ä¸ªæ—¶å€™æ¢å¤äº†

#### åŸå›  3ï¼šå…¶ä»–ä¿®æ”¹

- å¯èƒ½åŒæ—¶ä¿®æ”¹äº† docker-compose.yml çš„å…¶ä»–é…ç½®
- å¯èƒ½é‡å¯äº†å®¹å™¨è§£å†³äº†ä¸´æ—¶é—®é¢˜

### 16.5 OPENAI_API_HEADERS çš„å®é™…ä½œç”¨

#### Docker ç¯å¢ƒå˜é‡å±•å¼€

```bash
# .env æ–‡ä»¶
OPENAI_API_KEY=sk-...
OPENAI_API_HEADERS={"Authorization":"Bearer ${OPENAI_API_KEY}",...}
```

**Docker Compose å¤„ç†**ï¼š

```yaml
environment:
  - OPENAI_API_HEADERS=${OPENAI_API_HEADERS}
```

**å®¹å™¨å†…å®é™…æ¥æ”¶**ï¼š

```bash
OPENAI_API_HEADERS={"Authorization":"Bearer sk-...","Content-Type":"application/json",...}
```

- âœ… Docker ä¼šå±•å¼€ `${OPENAI_API_KEY}` å˜é‡
- âŒ ä½† Letta ä»£ç ä¸ä¼šè¯»å–è¿™ä¸ªç¯å¢ƒå˜é‡
- âŒ è¿™ä¸ªç¯å¢ƒå˜é‡è¢«å®Œå…¨å¿½ç•¥

### 16.6 ä½•æ—¶éœ€è¦è‡ªå®šä¹‰ Headersï¼Ÿ

#### éœ€è¦ default_headers çš„åœºæ™¯

**OpenRouter ç¤ºä¾‹**ï¼ˆLetta å·²æ”¯æŒï¼‰ï¼š

```python
# OpenRouter éœ€è¦ç‰¹å®š headers
kwargs["default_headers"] = {
    "HTTP-Referer": "https://your-site.com",
    "X-Title": "Your App Name"
}
```

**å…¶ä»–å¯èƒ½éœ€è¦è‡ªå®šä¹‰ headers çš„åœºæ™¯**ï¼š

1. **API ä»£ç†è¦æ±‚ç‰¹å®š headers**
   ```python
   kwargs["default_headers"] = {
       "X-API-Key": "custom-key",
       "X-Custom-Header": "value"
   }
   ```

2. **äº‘æœåŠ¡å•†ç‰¹æ®Šè¦æ±‚**
   - æŸäº›äº‘æœåŠ¡å•†å¯èƒ½éœ€è¦ç‰¹å®šçš„ headers
   - é€šå¸¸åœ¨æœåŠ¡å•†æ–‡æ¡£ä¸­è¯´æ˜

#### lingyunapi.com ä¸éœ€è¦è‡ªå®šä¹‰ headers

**åŸå› **ï¼š

1. âœ… å®ƒæ˜¯ OpenAI-compatible API
2. âœ… æ¥å—æ ‡å‡†çš„ `Authorization: Bearer <key>` è®¤è¯
3. âœ… OpenAI SDK è‡ªåŠ¨å¤„ç†è®¤è¯
4. âŒ ä¸éœ€è¦é¢å¤–çš„ headers

### 16.7 æ­£ç¡®çš„é…ç½®æ–¹å¼

#### âœ… æ¨èé…ç½®

**.env æ–‡ä»¶**ï¼š

```bash
# åªéœ€è¦è¿™ä¸¤ä¸ªæ ¸å¿ƒé…ç½®
OPENAI_API_KEY=sk-your-api-key
OPENAI_API_BASE=https://lingyunapi.com/v1

# å…¶ä»–é…ç½®
LETTA_SERVER_PASSWORD=your-password
EXA_API_KEY=your-exa-key  # å¦‚æœä½¿ç”¨ Exa æœç´¢
```

**docker-compose.yml**ï¼š

```yaml
services:
  letta-server:
    image: letta/letta:latest
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_API_BASE=${OPENAI_API_BASE}
      - LETTA_SERVER_PASSWORD=${LETTA_SERVER_PASSWORD}
      # âŒ ä¸è¦æ·»åŠ è¿™ä¸€è¡Œ
      # - OPENAI_API_HEADERS=${OPENAI_API_HEADERS}
```

#### âŒ ä¸æ¨èçš„é…ç½®

```bash
# .env æ–‡ä»¶
OPENAI_API_KEY=sk-...
OPENAI_API_BASE=https://lingyunapi.com/v1
OPENAI_API_HEADERS={"Authorization":"Bearer ${OPENAI_API_KEY}",...}  # âŒ æ²¡ç”¨
```

### 16.8 ä»£ç ä¿®æ”¹å»ºè®®

å¦‚æœç¡®å®éœ€è¦æ”¯æŒè‡ªå®šä¹‰ headersï¼ˆæŸäº›ç‰¹æ®Šåœºæ™¯ï¼‰ï¼Œå¯ä»¥ä¿®æ”¹ Letta ä»£ç ï¼š

#### æ–¹æ¡ˆ 1ï¼šæ·»åŠ  OPENAI_API_HEADERS æ”¯æŒ

**ä¿®æ”¹ä½ç½®**ï¼š`letta/llm_api/openai_client.py`

```python
def _prepare_client_kwargs(self, llm_config: LLMConfig) -> dict:
    api_key, _, _ = self.get_byok_overrides(llm_config)

    if not api_key:
        api_key = model_settings.openai_api_key or os.environ.get("OPENAI_API_KEY")

    kwargs = {"api_key": api_key, "base_url": llm_config.model_endpoint}

    # OpenRouter headers
    is_openrouter = ...
    if is_openrouter:
        # ... ç°æœ‰ä»£ç  ...

    # âœ… æ–°å¢ï¼šæ”¯æŒè‡ªå®šä¹‰ headers
    custom_headers = os.environ.get("OPENAI_API_HEADERS")
    if custom_headers:
        try:
            import json
            headers_dict = json.loads(custom_headers)
            if "default_headers" in kwargs:
                kwargs["default_headers"].update(headers_dict)
            else:
                kwargs["default_headers"] = headers_dict
        except json.JSONDecodeError:
            logger.warning(f"Invalid OPENAI_API_HEADERS format: {custom_headers}")

    kwargs["api_key"] = kwargs.get("api_key") or "DUMMY_API_KEY"
    return kwargs
```

**ä½†è¿™ç§ä¿®æ”¹é€šå¸¸æ˜¯ä¸å¿…è¦çš„**ï¼Œå› ä¸ºï¼š
- OpenAI SDK å·²ç»è‡ªåŠ¨å¤„ç† `Authorization` header
- å¤§å¤šæ•° OpenAI-compatible API éƒ½æ¥å—æ ‡å‡†è®¤è¯
- è‡ªå®šä¹‰ headers åªåœ¨æå°‘æ•°ç‰¹æ®Šåœºæ™¯ä¸‹éœ€è¦

### 16.9 æ€»ç»“

#### æ ¸å¿ƒå‘ç°

1. **Letta ä¸æ”¯æŒ OPENAI_API_HEADERS**
   - ä»£ç ä¸­å®Œå…¨æ²¡æœ‰è¯»å–è¿™ä¸ªç¯å¢ƒå˜é‡çš„é€»è¾‘
   - å³ä½¿è®¾ç½®äº†ï¼Œä¹Ÿä¼šè¢«å®Œå…¨å¿½ç•¥

2. **OpenAI SDK è‡ªåŠ¨å¤„ç†è®¤è¯**
   - SDK è‡ªåŠ¨ä» `OPENAI_API_KEY` è¯»å– API key
   - SDK è‡ªåŠ¨æ·»åŠ  `Authorization: Bearer <key>` header
   - ä¸éœ€è¦æ‰‹åŠ¨è®¾ç½®è®¤è¯ headers

3. **lingyunapi.com ä¸éœ€è¦è‡ªå®šä¹‰ headers**
   - å®ƒæ˜¯æ ‡å‡†çš„ OpenAI-compatible API
   - æ¥å— OpenAI çš„æ ‡å‡†è®¤è¯æ–¹å¼
   - åªéœ€è¦ `OPENAI_API_KEY` å’Œ `OPENAI_API_BASE`

4. **ä¹‹å‰"è§£å†³é—®é¢˜"çš„åŸå› **
   - å¾ˆå¯èƒ½æ˜¯å·§åˆï¼ˆå…¶ä»–é…ç½®å˜æ›´ï¼‰
   - æˆ–è€…è¯¯è§£ï¼ˆé—®é¢˜æœ¬æ¥å°±æ²¡è§£å†³ï¼‰
   - `OPENAI_API_HEADERS` æœ¬èº«æ²¡æœ‰èµ·ä½œç”¨

#### æ¨èé…ç½®

**ç”Ÿäº§ç¯å¢ƒé…ç½®**ï¼š

```bash
# .env
OPENAI_API_KEY=sk-your-key
OPENAI_API_BASE=https://lingyunapi.com/v1
LETTA_SERVER_PASSWORD=your-password
# âŒ ä¸éœ€è¦ OPENAI_API_HEADERS
```

```yaml
# docker-compose.yml
environment:
  - OPENAI_API_KEY=${OPENAI_API_KEY}
  - OPENAI_API_BASE=${OPENAI_API_BASE}
  - LETTA_SERVER_PASSWORD=${LETTA_SERVER_PASSWORD}
  # âŒ ç§»é™¤è¿™ä¸€è¡Œ - OPENAI_API_HEADERS=${OPENAI_API_HEADERS}
```

#### æœ€ä½³å®è·µ

1. **ä¿¡ä»» OpenAI SDK**
   - SDK ä¼šè‡ªåŠ¨å¤„ç†è®¤è¯
   - ä¸éœ€è¦æ‰‹åŠ¨æ·»åŠ  `Authorization` header

2. **ä¿æŒé…ç½®ç®€æ´**
   - åªé…ç½®å¿…éœ€çš„ç¯å¢ƒå˜é‡
   - ä¸è¦æ·»åŠ æ— ç”¨çš„é…ç½®é¡¹

3. **éªŒè¯é…ç½®**
   - åˆ›å»ºæµ‹è¯• agent
   - å‘é€æµ‹è¯•æ¶ˆæ¯
   - ç¡®è®¤è®¤è¯æ­£å¸¸

4. **ç†è§£ç¯å¢ƒå˜é‡çš„ä½œç”¨**
   - ä¸æ˜¯æ‰€æœ‰ç¯å¢ƒå˜é‡éƒ½è¢« Letta è¯»å–
   - æŸ¥çœ‹ä»£ç ç¡®è®¤å“ªäº›ç¯å¢ƒå˜é‡æœ‰æ•ˆ
   - ä¸è¦ç›²ç›®æ·»åŠ é…ç½®

#### å…³é”®è¦ç‚¹

> âš ï¸ **é‡è¦**ï¼š`OPENAI_API_HEADERS` ç¯å¢ƒå˜é‡å¯¹ Letta **æ²¡æœ‰ä»»ä½•ä½œç”¨**ã€‚å¦‚æœä½ ä¹‹å‰æ·»åŠ å®ƒæ¥"è§£å†³è®¤è¯é—®é¢˜"ï¼Œé‚£ä¹ˆçœŸæ­£è§£å†³é—®é¢˜çš„å¯èƒ½æ˜¯å…¶ä»–é…ç½®å˜æ›´ï¼Œè€Œä¸æ˜¯è¿™ä¸ªç¯å¢ƒå˜é‡ã€‚

---

## 17. Letta é¡¹ç›®æ ¸å¿ƒåŸç†æ·±åº¦åˆ†æï¼ˆ2026-01-07ï¼‰

### 17.1 é¡¹ç›®å®šä½

Letta æ˜¯ä¸€ä¸ª**æœ‰çŠ¶æ€ AI Agent å¹³å°**ï¼Œæ ¸å¿ƒç‰¹æ€§æ˜¯è®© AI Agent æ‹¥æœ‰**æŒä¹…åŒ–è®°å¿†**ï¼Œèƒ½å¤Ÿå­¦ä¹ å’Œè‡ªæˆ‘æ”¹è¿›ã€‚

**å‰èº«ä¸º MemGPT**ï¼ˆMemory GPTï¼‰ï¼Œå¼ºè°ƒå°†å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä¸æŒä¹…åŒ–è®°å¿†ç³»ç»Ÿç»“åˆã€‚

### 17.2 æ ¸å¿ƒæ¶æ„ç»„ä»¶

#### 17.2.1 ä¸‰å±‚æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API å±‚ (REST/WebSocket)               â”‚
â”‚       letta/server/rest_api/, letta/server/ws_api/       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Agent æ‰§è¡Œå±‚                           â”‚
â”‚  letta/agents/ (LettaAgentV3, BaseAgentV2, agent_loop)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  æœåŠ¡å’Œæ•°æ®å±‚                            â”‚
â”‚  letta/services/ (managers, tool_executor, summarizer)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 17.2.2 æ ¸å¿ƒç»„ä»¶å…³ç³»

**AgentState** (`letta/schemas/agent.py:61-150`)
- Agent çš„å®Œæ•´çŠ¶æ€è¡¨ç¤º
- åŒ…å«ï¼šé…ç½®ã€è®°å¿†å—ã€å·¥å…·ã€æ¶ˆæ¯ ID åˆ—è¡¨
- æŒä¹…åŒ–åˆ°æ•°æ®åº“

**LettaAgentV3** (`letta/agents/letta_agent_v3.py:61-72`)
- å½“å‰æœ€æ–°çš„ Agent å®ç°
- ç®€åŒ–äº† V2 çš„å¤æ‚æ€§
- æ ¸å¿ƒæ–¹æ³•ï¼š`step()`, `stream()`, `_step()`

**Agent Loop** (`letta/agents/agent_loop.py:15-64`)
- Agent å·¥å‚ç±»
- æ ¹æ® `agent_type` é€‰æ‹©å¯¹åº”çš„ Agent å®ç°
- æ”¯æŒï¼šLettaV3, LettaV2, Sleeptime, Voice ç­‰

### 17.3 Agent æ‰§è¡Œæµç¨‹

#### 17.3.1 æ ¸å¿ƒæ‰§è¡Œå¾ªç¯

```python
async def step(input_messages, max_steps=DEFAULT_MAX_STEPS):
    """
    å®Œæ•´çš„ Agent æ‰§è¡Œæµç¨‹
    """
    # 1. åˆå§‹åŒ–çŠ¶æ€
    self._initialize_state()

    # 2. å‡†å¤‡ in-context æ¶ˆæ¯
    in_context_messages, input_messages_to_persist = \
        await _prepare_in_context_messages_no_persist_async(...)

    # 3. æ‰§è¡Œå¤šæ­¥å¾ªç¯
    for i in range(max_steps):
        # 3.1 å•æ­¥æ‰§è¡Œ
        async for chunk in self._step(
            messages=in_context_messages + input_messages,
            llm_adapter=llm_adapter,
        ):
            yield chunk

        # 3.2 æ£€æŸ¥æ˜¯å¦ç»§ç»­
        if not self.should_continue:
            break

    # 4. è¿”å›ç»“æœ
    return LettaResponse(messages, stop_reason, usage)
```

#### 17.3.2 å•æ­¥æ‰§è¡Œï¼ˆ`_step`ï¼‰

è¿™æ˜¯ Agent çš„**æ ¸å¿ƒæ‰§è¡Œå•å…ƒ**ï¼š

```python
async def _step(messages, llm_adapter, ...):
    """
    å•æ­¥æ‰§è¡Œï¼šä¸€æ¬¡ LLM è°ƒç”¨ + å·¥å…·æ‰§è¡Œ
    """
    # 1. æ„å»ºç³»ç»Ÿæç¤ºï¼ˆåŒ…å«è®°å¿†å—ï¼‰
    system_message = self._build_system_message(...)

    # 2. è°ƒç”¨ LLM
    llm_response = await llm_adapter.send_messages(
        messages=[system_message] + messages,
        tools=self.agent_state.tools,
    )

    # 3. å¤„ç†å“åº”
    if llm_response.tool_calls:
        # 3a. æ‰§è¡Œå·¥å…·
        for tool_call in llm_response.tool_calls:
            result = await self.execute_tool(tool_call)
            tool_returns.append(result)

        # 3b. æŒä¹…åŒ–æ¶ˆæ¯
        await self._persist_messages(...)

        # 3c. è¿”å›å·¥å…·è°ƒç”¨ç»“æœ
        yield ToolCallMessage(...)
        yield ToolReturnMessage(...)

    else:
        # 4a. çº¯æ–‡æœ¬å“åº”
        yield AssistantMessage(...)

        # 4b. ç»“æŸå¾ªç¯
        self.should_continue = False
```

**ä»£ç ä½ç½®**ï¼š`letta/agents/letta_agent_v3.py:460-650`

### 17.4 è®°å¿†ç³»ç»Ÿæ¶æ„

#### 17.4.1 ä¸‰çº§è®°å¿†ç»“æ„

Letta ä½¿ç”¨**ä¸‰çº§è®°å¿†æ¶æ„**æ¥çªç ´ LLM çš„ä¸Šä¸‹æ–‡çª—å£é™åˆ¶ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Core Memory (æ ¸å¿ƒè®°å¿† - In-Context)              â”‚
â”‚     - Block ç»“æ„ï¼ˆhuman, persona, summaryï¼‰           â”‚
â”‚     - ç›´æ¥åœ¨ LLM ä¸Šä¸‹æ–‡ä¸­                             â”‚
â”‚     - æœ‰é™å¤§å°ï¼ˆé€šå¸¸ 2000-4000 å­—ç¬¦ï¼‰                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Recall Memory (å¯¹è¯å†å² - In-Context)            â”‚
â”‚     - æœ€è¿‘çš„æ¶ˆæ¯åˆ—è¡¨                                  â”‚
â”‚     - åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œä¼šè¢«æ€»ç»“                            â”‚
â”‚     - åŠ¨æ€ç®¡ç†ï¼ˆæ¶ˆæ¯ç¼“å†²åŒºï¼‰                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Archival Memory (æ¡£æ¡ˆè®°å¿† - Out-of-Context)      â”‚
â”‚     - Passage + Archive                               â”‚
â”‚     - ä½¿ç”¨å‘é‡æœç´¢ï¼ˆembeddingï¼‰                      â”‚
â”‚     - æ— é™å¤§å°ï¼ŒæŒ‰éœ€æ£€ç´¢                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 17.4.2 Blockï¼ˆæ ¸å¿ƒè®°å¿†å—ï¼‰

**æ•°æ®ç»“æ„** (`letta/schemas/block.py:13-100`)ï¼š

```python
class Block(BaseBlock):
    id: str                          # å”¯ä¸€æ ‡è¯†
    label: str                       # æ ‡ç­¾ï¼ˆå¦‚ "human", "persona"ï¼‰
    value: str                       # å®é™…å†…å®¹
    limit: int = 2000                # å­—ç¬¦é™åˆ¶
    description: str                 # æè¿°
    read_only: bool = False          # æ˜¯å¦åªè¯»
```

**å†…å­˜æ¸²æŸ“** (`letta/schemas/memory.py:110-140`)ï¼š

```python
def _render_memory_blocks_standard(self, s: StringIO):
    s.write("<memory_blocks>\n")
    s.write("The following memory blocks are in your core memory:\n\n")

    for block in self.blocks:
        s.write(f"<{block.label}>\n")
        s.write(f"<description>{block.description}</description>\n")
        s.write(f"<value>{block.value}</value>\n")
        s.write(f"</{block.label}>\n")

    s.write("</memory_blocks>")
```

**LLM çœ‹åˆ°çš„æ ¼å¼**ï¼š

```xml
<memory_blocks>
The following memory blocks are in your core memory unit:

<human>
<description>
Details about the human user
</description>
<metadata>
- chars_current=150
- chars_limit=2000
</metadata>
<value>
Name: Timber. Status: dog. Occupation: building Letta...
</value>
</human>

<persona>
<description>
Agent's persona
</description>
<value>
I am a self-improving superintelligence...
</value>
</persona>

</memory_blocks>
```

#### 17.4.3 Passageï¼ˆæ¡£æ¡ˆè®°å¿†ï¼‰

**æ•°æ®ç»“æ„** (`letta/schemas/passage.py:35-45`)ï¼š

```python
class Passage(PassageBase):
    id: str
    text: str                       # æ–‡æœ¬å†…å®¹
    embedding: List[float]          # å‘é‡åµŒå…¥ï¼ˆ1536 ç»´ï¼‰
    embedding_config: EmbeddingConfig
    archive_id: str                 # æ‰€å±æ¡£æ¡ˆ
    tags: List[str]                 # æ ‡ç­¾
    metadata: Dict                  # å…ƒæ•°æ®
```

**å‘é‡æœç´¢** (`letta/services/passage_manager.py:51-120`)ï¼š

```python
class PassageManager:
    async def search_passages(
        self,
        query: str,
        archive_id: str,
        limit: int = 10,
    ) -> List[Passage]:
        # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = await get_openai_embedding_async(
            text=query,
            model=self.embedding_config.model,
            endpoint=self.embedding_config.embedding_endpoint,
        )

        # 2. å‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        results = await self.query_passages_by_embedding(
            query_embedding=query_embedding,
            archive_id=archive_id,
            limit=limit,
        )

        return results
```

### 17.5 å·¥å…·ç³»ç»Ÿ

#### 17.5.1 å·¥å…·ç±»å‹åˆ†ç±»

**ä»£ç ä½ç½®**ï¼š`letta/schemas/enums.py` - `ToolType`

```python
class ToolType(str, Enum):
    LETTA_CORE = "letta_core"              # æ ¸å¿ƒè®°å¿†å·¥å…·
    LETTA_MEMORY_CORE = "letta_memory_core" # è®°å¿†ç®¡ç†
    LETTA_MULTI_AGENT_CORE = "letta_multi_agent_core"  # å¤š Agent
    LETTA_BUILTIN = "letta_builtin"        # å†…ç½®å·¥å…·
    LETTA_FILES_CORE = "letta_files_core"  # æ–‡ä»¶æ“ä½œ
    EXTERNAL_MCP = "external_mcp"          # MCP å¤–éƒ¨å·¥å…·
    CUSTOM = "custom"                      # ç”¨æˆ·è‡ªå®šä¹‰
```

#### 17.5.2 å·¥å…·æ‰§è¡Œæµç¨‹

**å·¥å‚æ¨¡å¼** (`letta/services/tool_executor/tool_execution_manager.py:33-66`)ï¼š

```python
class ToolExecutorFactory:
    _executor_map = {
        ToolType.LETTA_CORE: LettaCoreToolExecutor,
        ToolType.LETTA_BUILTIN: LettaBuiltinToolExecutor,
        ToolType.EXTERNAL_MCP: ExternalMCPToolExecutor,
        # ...
    }

    @classmethod
    def get_executor(cls, tool_type, ...) -> ToolExecutor:
        executor_class = cls._executor_map.get(
            tool_type,
            SandboxToolExecutor  # é»˜è®¤ä½¿ç”¨æ²™ç®±æ‰§è¡Œ
        )
        return executor_class(...)
```

**æ‰§è¡Œç®¡ç†å™¨** (`letta/services/tool_executor/tool_execution_manager.py:69-150`)ï¼š

```python
class ToolExecutionManager:
    async def execute_tool_async(
        self,
        function_name: str,
        function_args: dict,
        tool: Tool,
        step_id: str | None = None,
    ) -> ToolExecutionResult:
        # 1. è·å–å¯¹åº”çš„æ‰§è¡Œå™¨
        executor = ToolExecutorFactory.get_executor(tool.tool_type, ...)

        # 2. æ‰§è¡Œå·¥å…·
        async with AsyncTimer(callback_func=_metrics_callback):
            result = await executor.execute(
                function_name,
                function_args,
                tool,
                self.actor,
                self.agent_state,
            )

        # 3. æˆªæ–­è¿‡é•¿çš„è¿”å›å€¼
        return_str = json.dumps(result.func_return)
        if len(return_str) > tool.return_char_limit:
            result.func_return = FUNCTION_RETURN_VALUE_TRUNCATED(...)

        return result
```

#### 17.5.3 æ ¸å¿ƒå·¥å…·ç¤ºä¾‹

**è®°å¿†å·¥å…·** (`letta/functions/core/letta_core.py`)ï¼š

```python
def core_memory_append(
    block_label: str,
    content: str,
    agent_state: AgentState,
    block_manager: BlockManager,
) -> str:
    """å‘æ ¸å¿ƒè®°å¿†å—è¿½åŠ å†…å®¹"""
    block = get_block_by_label(agent_state, block_label)

    # æ£€æŸ¥é™åˆ¶
    if len(block.value) + len(content) > block.limit:
        raise ValueError(f"Exceeds {block.limit} character limit")

    # æ›´æ–°å—
    updated_block = BlockUpdate(value=block.value + "\n" + content)
    block_manager.update_block_by_id(block.id, updated_block, actor)

    return f"Added to {block_label}: {content}"
```

**å¯¹è¯æœç´¢å·¥å…·**ï¼š

```python
def conversation_search(
    query: str,
    agent_state: AgentState,
    message_manager: MessageManager,
) -> str:
    """åœ¨å¯¹è¯å†å²ä¸­æœç´¢ç›¸å…³æ¶ˆæ¯"""
    # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
    query_embedding = await get_embedding(query)

    # 2. å‘é‡æœç´¢
    messages = await message_manager.search_messages(
        agent_id=agent_state.id,
        query_embedding=query_embedding,
        limit=5,
    )

    # 3. æ ¼å¼åŒ–ç»“æœ
    return format_search_results(messages)
```

**æ¡£æ¡ˆæœç´¢å·¥å…·**ï¼š

```python
def archival_memory_search(
    query: str,
    agent_state: AgentState,
    passage_manager: PassageManager,
) -> str:
    """åœ¨æ¡£æ¡ˆè®°å¿†ä¸­æœç´¢ç›¸å…³å†…å®¹"""
    # 1. å‘é‡æœç´¢
    passages = await passage_manager.search_passages(
        query=query,
        archive_id=agent_state.archive_id,
        limit=5,
    )

    # 2. æ ¼å¼åŒ–ç»“æœ
    return format_passages(passages)
```

### 17.6 æ¶ˆæ¯æµä¸å“åº”æ ¼å¼

#### 17.6.1 Letta æ¶ˆæ¯ç±»å‹

**ä»£ç ä½ç½®**ï¼š`letta/schemas/letta_message.py`

```python
class LettaMessage(BaseModel):
    message_type: MessageType

    # æ¶ˆæ¯ç±»å‹åŒ…æ‹¬ï¼š
    - "system_message"         # ç³»ç»Ÿæ¶ˆæ¯
    - "user_message"           # ç”¨æˆ·æ¶ˆæ¯
    - "assistant_message"      # åŠ©æ‰‹å“åº”
    - "tool_call_message"      # å·¥å…·è°ƒç”¨
    - "tool_return_message"    # å·¥å…·è¿”å›
    - "reasoning_message"      # æ¨ç†è¿‡ç¨‹
    - "approval_request_message"  # å®¡æ‰¹è¯·æ±‚
```

#### 17.6.2 å“åº”ç»“æ„

**LettaResponse** (`letta/schemas/letta_response.py:33-60`)ï¼š

```python
class LettaResponse(BaseModel):
    messages: List[LettaMessageUnion]  # æ¶ˆæ¯åˆ—è¡¨
    stop_reason: LettaStopReason       # åœæ­¢åŸå› 
    usage: LettaUsageStatistics        # ä½¿ç”¨ç»Ÿè®¡

class LettaStopReason:
    stop_reason: StopReasonType
    # ç±»å‹ï¼š
    - "end_turn"              # æ­£å¸¸ç»“æŸ
    - "max_steps"             # è¾¾åˆ°æœ€å¤§æ­¥æ•°
    - "cancelled"             # ç”¨æˆ·å–æ¶ˆ
    - "llm_api_error"         # LLM API é”™è¯¯
    - "error"                 # å…¶ä»–é”™è¯¯
```

#### 17.6.3 æµå¼å“åº”ï¼ˆSSEï¼‰

**æµå¼é€‚é…å™¨** (`letta/agents/letta_agent_v3.py:227-399`)ï¼š

```python
async def stream(
    self,
    input_messages: list[MessageCreate],
    stream_tokens: bool = False,
) -> AsyncGenerator[str, None]:
    """
    æµå¼å“åº”ï¼Œä½¿ç”¨ Server-Sent Events (SSE)
    """
    # 1. é€‰æ‹©é€‚é…å™¨
    if stream_tokens:
        llm_adapter = SimpleLLMStreamAdapter(...)  # Token çº§åˆ«
    else:
        llm_adapter = SimpleLLMRequestAdapter(...)  # æ¶ˆæ¯çº§åˆ«

    # 2. æ‰§è¡Œå¾ªç¯
    for i in range(max_steps):
        async for chunk in self._step(...):
            # 3. å‘é€ SSE äº‹ä»¶
            yield f"data: {chunk.model_dump_json()}\n\n"

    # 4. å‘é€ç»“æŸäº‹ä»¶
    for finish_chunk in self.get_finish_chunks_for_stream(...):
        yield f"data: {finish_chunk}\n\n"
```

**SSE äº‹ä»¶æ ¼å¼**ï¼š

```
data: {"message_type":"tool_call_message","tool_call":{...}}

data: {"message_type":"tool_return_message","tool_return":{...}}

data: {"message_type":"assistant_message","message":"Hello!"}

data: {"stop_reason":"end_turn"}

data: {"usage":{"total_tokens":1000,...}}
```

### 17.7 è®°å¿†ç®¡ç†æœºåˆ¶

#### 17.7.1 æ€»ç»“å™¨ï¼ˆSummarizerï¼‰

**ä»£ç ä½ç½®**ï¼š`letta/services/summarizer/summarizer.py:34-100`

```python
class Summarizer:
    """
    ç®¡ç†å¯¹è¯å†å²çš„æ€»ç»“å’Œå‹ç¼©
    """

    def __init__(
        self,
        mode: SummarizationMode,
        message_buffer_limit: int = 10,  # æ¶ˆæ¯ç¼“å†²åŒºå¤§å°
    ):
        self.mode = mode
        self.message_buffer_limit = message_buffer_limit

    async def summarize(
        self,
        in_context_messages: List[Message],
        new_letta_messages: List[Message],
        force: bool = False,
    ) -> Tuple[List[Message], bool]:
        """
        æ ¹æ®æ¨¡å¼æ€»ç»“æˆ–è£å‰ªæ¶ˆæ¯
        """
        if self.mode == SummarizationMode.STATIC_MESSAGE_BUFFER:
            # ä¿æŒå›ºå®šæ•°é‡çš„æ¶ˆæ¯
            return self._static_buffer_summarization(...)

        elif self.mode == SummarizationMode.PARTIAL_EVICT_MESSAGE_BUFFER:
            # éƒ¨åˆ† eviction
            return await self._partial_evict_buffer_summarization(...)
```

#### 17.7.2 ä¸Šä¸‹æ–‡çª—å£ç®¡ç†

**åŠ¨æ€æ€»ç»“è§¦å‘**ï¼ˆåœ¨ `letta/agents/letta_agent_v3.py:160-180` ä¸­å·²æ³¨é‡Šï¼‰ï¼š

```python
# å½“æ¥è¿‘ä¸Šä¸‹æ–‡é™åˆ¶æ—¶è§¦å‘æ€»ç»“
if (
    self.context_token_estimate is not None
    and self.context_token_estimate > context_window * 0.8  # 80% é˜ˆå€¼
):
    # è§¦å‘æ€»ç»“
    await self.summarize_conversation_history(
        in_context_messages=in_context_messages,
        new_letta_messages=self.response_messages,
        force=True,
    )
```

### 17.8 Provider ä¸ LLM é›†æˆ

#### 17.8.1 LLM å®¢æˆ·ç«¯æ¶æ„

**ä»£ç ä½ç½®**ï¼š`letta/llm_api/llm_client.py`

```python
class LLMClient:
    """
    ç»Ÿä¸€çš„ LLM å®¢æˆ·ç«¯æ¥å£
    æ”¯æŒå¤šä¸ª Providerï¼šOpenAI, Anthropic, Google, ç­‰
    """

    @staticmethod
    def create(provider_type: ProviderType, ...) -> LLMClientBase:
        match provider_type:
            case ProviderType.openai:
                return OpenAIClient(...)
            case ProviderType.anthropic:
                return AnthropicClient(...)
            case ProviderType.google:
                return GoogleClient(...)
            case _:
                return OpenAIClient(...)  # é»˜è®¤
```

#### 17.8.2 è¯·æ±‚é€‚é…å™¨

**é€‚é…å™¨æ¨¡å¼** (`letta/adapters/`)ï¼š

```python
class LettaLLMAdapter(ABC):
    """
    å°† Letta æ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸º LLM Provider æ ¼å¼
    """

    @abstractmethod
    async def send_messages(
        self,
        messages: List[Message],
        tools: List[Tool],
    ) -> ChatCompletionResponse:
        pass

class SimpleLLMRequestAdapter(LettaLLMAdapter):
    """éæµå¼è¯·æ±‚"""

class SimpleLLMStreamAdapter(LettaLLMAdapter):
    """æµå¼è¯·æ±‚ï¼ˆToken çº§åˆ«ï¼‰"""
```

### 17.9 æ•°æ®åº“ä¸æŒä¹…åŒ–

#### 17.9.1 Manager å±‚

**æ ¸å¿ƒ Managers**ï¼š

```python
# Agent ç®¡ç†
class AgentManager:
    - create_agent()
    - get_agent()
    - update_agent()
    - delete_agent()
    - update_message_ids_async()

# æ¶ˆæ¯ç®¡ç†
class MessageManager:
    - create_message()
    - get_messages()
    - search_messages()  # å‘é‡æœç´¢

# è®°å¿†å—ç®¡ç†
class BlockManager:
    - create_block()
    - update_block()
    - get_block()

# æ¡£æ¡ˆç®¡ç†
class PassageManager:
    - create_passage()
    - search_passages()  # å‘é‡æœç´¢

# å·¥å…·ç®¡ç†
class ToolManager:
    - create_tool()
    - get_tool()
    - execute_tool()
```

#### 17.9.2 ORM æ¨¡å‹

**ä»£ç ä½ç½®**ï¼š`letta/orm/agent.py`, `letta/orm/message.py`, ç­‰

```python
class Agent(SqlalchemyBase):
    __tablename__ = "agents"

    id: str
    name: str
    system: str
    agent_type: str
    llm_config: dict
    embedding_config: dict
    # ...

    # å…³ç³»
    blocks: List[Block]
    tools: List[Tool]
    messages: List[Message]
```

### 17.10 å¤š Agent åä½œ

#### 17.10.1 Agent ç»„ï¼ˆGroupï¼‰

**ä»£ç ä½ç½®**ï¼š`letta/groups/`

```python
class SleeptimeMultiAgentV4(BaseAgentV2):
    """
    å¤š Agent ç³»ç»Ÿï¼š
    - ä¸» Agentï¼šå¤„ç†ç”¨æˆ·äº¤äº’
    - Sleeptime Agentï¼šåå°å¤„ç†è®°å¿†ç®¡ç†
    """

    def __init__(
        self,
        agent_state: AgentState,
        group: Group,  # Agent ç»„
        actor: User,
    ):
        self.primary_agent = LettaAgentV3(agent_state, actor)
        self.sleeptime_agent = ...  # åå° Agent
```

#### 17.10.2 å·¥å…·ä¼ é€’

```python
@tool
def send_message_to_agent(
    agent_id: str,
    message: str,
    multi_agent_tool_executor: MultiAgentToolExecutor,
) -> str:
    """å‘å¦ä¸€ä¸ª Agent å‘é€æ¶ˆæ¯"""
    return await multi_agent_tool_executor.send_message(
        agent_id=agent_id,
        content=message,
    )
```

### 17.11 å‰ç«¯æ¶æ„è®¾è®¡è¦ç‚¹

åŸºäºä»¥ä¸Šåˆ†æï¼ŒFlutter å‰ç«¯éœ€è¦å…³æ³¨ï¼š

#### 17.11.1 æ ¸å¿ƒ API é›†æˆ

```dart
// 1. Agent ç®¡ç†
GET    /v1/agents/
POST   /v1/agents/
GET    /v1/agents/{id}
PUT    /v1/agents/{id}
DELETE /v1/agents/{id}

// 2. æ¶ˆæ¯å‘é€ï¼ˆSSE æµå¼å“åº”ï¼‰
POST   /v1/agents/{id}/messages

// 3. å·¥å…·ç®¡ç†
GET    /v1/tools/
POST   /v1/tools/

// 4. è®°å¿†ç®¡ç†
GET    /v1/blocks/
PUT    /v1/blocks/{id}

// 5. æ¡£æ¡ˆæœç´¢
POST   /v1/passages/search
```

#### 17.11.2 SSE æµå¼å“åº”å¤„ç†

```dart
class AgentChatService {
  Stream<Message> sendMessageStream({
    required String agentId,
    required String content,
  }) async* {
    final client = SSEClient.connect(
      url: '/v1/agents/$agentId/messages',
      method: 'POST',
      headers: {'Authorization': 'Bearer $token'},
      body: jsonEncode({'messages': [{'role': 'user', 'content': content}]}),
    );

    await for (final event in client.events) {
      if (event.type == 'message') {
        final data = jsonDecode(event.data);
        yield Message.fromJson(data);
      }
    }
  }
}
```

#### 17.11.3 çŠ¶æ€ç®¡ç†ç­–ç•¥

```dart
// Riverpod Provider
@riverpod
class AgentState extends _$AgentState {
  @override
  Future<List<Agent>> build() async {
    final response = await client.get('/v1/agents/');
    return [ ... ];
  }

  Future<void> createAgent(CreateAgentRequest request) async {
    state = const AsyncValue.loading();
    state = await AsyncValue.guard(() async {
      await client.post('/v1/agents/', request);
      return ref.refresh(self.future);
    });
  }
}

@riverpod
class ChatMessages extends _$ChatMessages {
  @override
  List<Message> build() => [];

  void addMessage(Message message) {
    state = [...state, message];
  }
}
```

### 17.12 æ€»ç»“ï¼šLetta çš„æ ¸å¿ƒåˆ›æ–°

1. **æŒä¹…åŒ–è®°å¿†**ï¼šçªç ´ LLM æ— çŠ¶æ€é™åˆ¶
2. **ä¸‰çº§è®°å¿†æ¶æ„**ï¼šå¹³è¡¡é€Ÿåº¦ã€å®¹é‡ã€æˆæœ¬
3. **å·¥å…·ä¼˜å…ˆ**ï¼šé€šè¿‡ Function Calling æ‰©å±•èƒ½åŠ›
4. **æµå¼å“åº”**ï¼šå®æ—¶ç”¨æˆ·ä½“éªŒ
5. **å¤š Provider æ”¯æŒ**ï¼šçµæ´»çš„ LLM é€‰æ‹©
6. **å‘é‡æ£€ç´¢**ï¼šæ™ºèƒ½è®°å¿†æœç´¢
7. **å¤š Agent åä½œ**ï¼šå¤æ‚ä»»åŠ¡åˆ†è§£

### 17.13 å…³é”®ä»£ç æ–‡ä»¶æ¸…å•

**æ ¸å¿ƒæ‰§è¡Œ**ï¼š
- `letta/agents/letta_agent_v3.py` - Agent ä¸»å®ç°
- `letta/agents/base_agent_v2.py` - æŠ½è±¡åŸºç±»
- `letta/agents/agent_loop.py` - Agent å·¥å‚

**æ•°æ®æ¨¡å‹**ï¼š
- `letta/schemas/agent.py` - Agent çŠ¶æ€
- `letta/schemas/memory.py` - è®°å¿†ç³»ç»Ÿ
- `letta/schemas/block.py` - è®°å¿†å—
- `letta/schemas/message.py` - æ¶ˆæ¯
- `letta/schemas/tool.py` - å·¥å…·
- `letta/schemas/passage.py` - æ¡£æ¡ˆ

**æœåŠ¡å±‚**ï¼š
- `letta/services/agent_manager.py` - Agent ç®¡ç†
- `letta/services/message_manager.py` - æ¶ˆæ¯ç®¡ç†
- `letta/services/block_manager.py` - è®°å¿†å—ç®¡ç†
- `letta/services/passage_manager.py` - æ¡£æ¡ˆç®¡ç†
- `letta/services/tool_executor/tool_execution_manager.py` - å·¥å…·æ‰§è¡Œ
- `letta/services/summarizer/summarizer.py` - æ€»ç»“å™¨

**LLM é›†æˆ**ï¼š
- `letta/llm_api/llm_client.py` - LLM å®¢æˆ·ç«¯
- `letta/llm_api/openai_client.py` - OpenAI å®ç°
- `letta/llm_api/anthropic_client.py` - Anthropic å®ç°
- `letta/adapters/` - è¯·æ±‚é€‚é…å™¨

**API å±‚**ï¼š
- `letta/server/rest_api/routers/v1/agents.py` - Agents API
- `letta/server/rest_api/routers/v1/messages.py` - Messages API
- `letta/server/ws_api/interface.py` - WebSocket API

---

## 18. å‰ç«¯åˆ›å»º Agent çš„ BYOK æ¨¡å¼å®ç°ï¼ˆ2026-01-09ï¼‰â­ æ–°å¢

### 18.1 èƒŒæ™¯ï¼šBYOK vs é BYOK æ¨¡å¼

Letta æ”¯æŒä¸¤ç§æ¨¡å¼æ¥é…ç½® LLM å’Œ Embedding æ¨¡å‹ï¼š

| ç‰¹æ€§ | BYOK æ¨¡å¼ | é BYOK æ¨¡å¼ |
|------|-----------|---------------|
| **Provider æ¥æº** | æ•°æ®åº“ï¼ˆç”¨æˆ·åˆ›å»ºï¼‰ | ç¯å¢ƒå˜é‡ï¼ˆå†…å­˜ï¼‰ |
| **Provider Category** | `byok` | `base` |
| **API Keys** | å­˜å‚¨åœ¨æ•°æ®åº“ Provider ä¸­ | ä»ç¯å¢ƒå˜é‡è¯»å– |
| **ä½¿ç”¨åœºæ™¯** | éœ€è¦è‡ªå®šä¹‰ API keysã€å¤šç”¨æˆ·éš”ç¦» | å•ç”¨æˆ·ã€å¼€å‘æµ‹è¯• |
| **é…ç½®å¤æ‚åº¦** | éœ€è¦å®Œæ•´é…ç½® | ç®€åŒ–é…ç½® |

### 18.2 é BYOK æ¨¡å¼åˆ›å»º Agent

#### 18.2.1 API è¯·æ±‚æ ¼å¼

é BYOK æ¨¡å¼ä½¿ç”¨ç®€åŒ–çš„ JSON æ ¼å¼ï¼š

```json
{
  "name": "my-agent",
  "model": "openai-proxy/claude-sonnet-4-5-20250929",
  "embedding": "openai-proxy/text-embedding-3-small",
  "system": "You are a helpful assistant.",
  "description": "Optional description"
}
```

**å…³é”®ç‚¹**ï¼š
- `model` å’Œ `embedding` å­—æ®µä½¿ç”¨**æ¨¡å‹çš„ handle**ï¼Œæ ¼å¼ä¸º `provider_name/model_name`
- Letta æ ¹æ® handle çš„å‰ç¼€ï¼ˆ`openai-proxy`ï¼‰è‡ªåŠ¨æŸ¥æ‰¾å¯¹åº”çš„ base provider
- Base provider çš„é…ç½®ï¼ˆAPI keysã€endpointsï¼‰æ¥è‡ªç¯å¢ƒå˜é‡

#### 18.2.2 å‰ç«¯å®ç°

**Dart æ¨¡å‹** (`lib/core/models/create_agent_request.dart`):

```dart
/// Convert to simple format (non-BYOK mode)
Map<String, dynamic> toSimpleJson() {
  final json = <String, dynamic>{
    'name': name,
    'model': llmModel.handle,  // e.g., "openai-proxy/claude-sonnet-4-5-20250929"
    'embedding': embeddingModel.handle,  // e.g., "openai-proxy/text-embedding-3-small"
  };

  if (description != null) {
    json['description'] = description;
  }
  if (systemPrompt != null) {
    json['system'] = systemPrompt;
  }

  return json;
}
```

**æ¨¡å‹åŠ è½½é€»è¾‘** (`lib/features/agents/screens/agent_create_screen.dart`):

```dart
if (!_byokMode) {
  // Non-BYOK mode: load base models directly
  final allModels = await ref.read(baseLLMModelListProvider.future);

  // Load LLM models from /v1/models/?provider_category=base
  final llmModels = allModels.where((m) => m.modelType == 'llm').toList();

  // Load embedding models from /v1/models/embedding
  final embeddingResponse = await client.get('/models/embedding');
  final embeddingModels = // ... parse response

  setState(() {
    _availableLLMModels = llmModels;
    _availableEmbeddingModels = embeddingModels;
  });
}
```

**å…³é”®ç‚¹**ï¼š
- LLM æ¨¡å‹ä» `/v1/models/?provider_category=base` è·å–
- Embedding æ¨¡å‹ä» `/v1/models/embedding` è·å–ï¼ˆå› ä¸º base æ¨¡å‹åˆ—è¡¨ä¸åŒ…å« embeddingï¼‰
- æ‰€æœ‰æ¨¡å‹çš„ `provider_category` é»˜è®¤ä¸º `"base"`

### 18.3 BYOK æ¨¡å¼åˆ›å»º Agent

#### 18.3.1 API è¯·æ±‚æ ¼å¼

BYOK æ¨¡å¼ä½¿ç”¨å®Œæ•´çš„é…ç½®æ ¼å¼ï¼š

```json
{
  "name": "my-agent",
  "llm_config": {
    "model": "claude-haiku-4-5-20251001-thinking",
    "provider_name": "openai-proxy",
    "provider_category": "byok",
    "model_endpoint_type": "openai",
    "context_window": 30000
  },
  "embedding_config": {
    "provider_name": "openai-embedding",
    "provider_category": "byok",
    "embedding_endpoint_type": "openai",
    "embedding_model": "text-embedding-3-small",
    "embedding_dim": 1536
  },
  "system": "You are a helpful assistant."
}
```

**å…³é”®ç‚¹**ï¼š
- å¿…é¡»æä¾› `provider_name` å’Œ `provider_category`ï¼ŒLetta ç”¨è¿™ä¸¤ä¸ªå­—æ®µåœ¨æ•°æ®åº“ä¸­æŸ¥æ‰¾å¯¹åº”çš„ Provider
- æ‰¾åˆ° Provider åï¼Œä½¿ç”¨å…¶é…ç½®ï¼ˆAPI keysã€base URLs ç­‰ï¼‰æ¥è°ƒç”¨ LLM API

#### 18.3.2 Letta åç«¯çš„ Provider æŸ¥æ‰¾æµç¨‹

1. **æ¥æ”¶è¯·æ±‚**ï¼š
   ```json
   {
     "llm_config": {
       "provider_name": "openai-proxy",
       "provider_category": "byok"
     }
   }
   ```

2. **æ•°æ®åº“æŸ¥è¯¢**ï¼š
   ```python
   # ä¼ªä»£ç 
   provider = db.query(Provider).filter_by(
       name="openai-proxy",
       category="byok"
   ).first()
   ```

3. **è·å–é…ç½®**ï¼š
   ```python
   api_key = provider.api_key  # ä»æ•°æ®åº“è¯»å–
   base_url = provider.base_url
   ```

4. **è°ƒç”¨ LLM API**ï¼š
   ```python
   response = openai_client.chat(
       api_key=api_key,
       base_url=base_url,
       model="claude-haiku-4-5-20251001-thinking"
   )
   ```

#### 18.3.3 å‰ç«¯å®ç°

**Dart æ¨¡å‹** (`lib/core/models/create_agent_request.dart`):

```dart
/// Convert to full config format (BYOK mode)
Map<String, dynamic> toBYOKJson() {
  final json = <String, dynamic>{
    'name': name,
    'llm_config': {
      'model': llmModel.model,
      'provider_name': llmModel.providerName,  // å…³é”®å­—æ®µ
      'provider_category': llmModel.providerCategory,  // å…³é”®å­—æ®µ
      'model_endpoint_type': llmModel.modelEndpointType,
      'context_window': llmModel.contextWindow,
    },
    'embedding_config': {
      'provider_name': embeddingModel.providerName,  // å…³é”®å­—æ®µ
      'provider_category': embeddingModel.providerCategory,  // å…³é”®å­—æ®µ
      'embedding_endpoint_type': embeddingModel.modelEndpointType,
      'embedding_model': embeddingModel.model,
      'embedding_dim': 1536,
    },
  };

  if (description != null) {
    json['description'] = description;
  }
  if (systemPrompt != null) {
    json['system'] = systemPrompt;
  }

  return json;
}
```

**æ¨¡å¼æ£€æµ‹**ï¼š

```dart
/// Check if this is a BYOK mode request
bool get isBYOK => llmModel.providerCategory == 'byok';

/// Convert to JSON based on mode
Map<String, dynamic> toJson() {
  return isBYOK ? toBYOKJson() : toSimpleJson();
}
```

**æ¨¡å‹åŠ è½½é€»è¾‘**ï¼š

```dart
if (_byokMode) {
  // BYOK mode: load providers from database
  final providers = await ref.read(providerListProvider.future);

  setState(() {
    _availableProviders = providers;
    // User selects a provider first, then load models for that provider
  });
}
```

### 18.4 å®Œæ•´åˆ›å»ºæµç¨‹å¯¹æ¯”

#### 18.4.1 é BYOK æ¨¡å¼æµç¨‹

```
ç”¨æˆ·é€‰æ‹©æ¨¡å‹
    â†“
ä½¿ç”¨ handle (openai-proxy/claude-sonnet-4-5-20250929)
    â†“
å‘é€ç®€åŒ– JSON: {"model": "openai-proxy/...", "embedding": "openai-proxy/..."}
    â†“
Letta è§£æ handleï¼Œæå– provider_name = "openai-proxy"
    â†“
Letta ä»ç¯å¢ƒå˜é‡åŠ è½½ openai-proxy çš„é…ç½®
    â†“
åˆ›å»º Agent æˆåŠŸ
```

#### 18.4.2 BYOK æ¨¡å¼æµç¨‹

```
ç”¨æˆ·é€‰æ‹© Provider (openai-proxy, openai-embedding)
    â†“
åŠ è½½è¯¥ Provider ä¸‹çš„æ¨¡å‹åˆ—è¡¨
    â†“
ç”¨æˆ·é€‰æ‹©å…·ä½“æ¨¡å‹
    â†“
å‘é€å®Œæ•´ JSON: {
  "llm_config": {"provider_name": "openai-proxy", "provider_category": "byok", ...},
  "embedding_config": {"provider_name": "openai-embedding", "provider_category": "byok", ...}
}
    â†“
Letta æ ¹æ® provider_name + provider_category åœ¨æ•°æ®åº“ä¸­æŸ¥æ‰¾ Provider
    â†“
Letta è¯»å– Provider çš„ API keys å’Œé…ç½®
    â†“
åˆ›å»º Agent æˆåŠŸ
```

### 18.5 å…³é”®å­—æ®µè¯´æ˜

#### 18.5.1 æ¨¡å‹ (LLMModel) å­—æ®µ

| å­—æ®µ | ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|------|
| `handle` | String | å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œæ ¼å¼ `provider_name/model` | `openai-proxy/claude-sonnet-4-5-20250929` |
| `model` | String | æ¨¡å‹åç§° | `claude-sonnet-4-5-20250929` |
| `provider_name` | String | Provider åç§° | `openai-proxy` |
| `provider_category` | String | Provider ç±»åˆ«ï¼š`base` æˆ– `byok` | `base`, `byok` |
| `model_endpoint_type` | String | ç«¯ç‚¹ç±»å‹ | `openai`, `anthropic` |
| `context_window` | int | ä¸Šä¸‹æ–‡çª—å£å¤§å° | `30000` |

#### 18.5.2 Embedding æ¨¡å‹å­—æ®µ

| å­—æ®µ | ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|------|
| `handle` | String | å”¯ä¸€æ ‡è¯†ç¬¦ | `openai-proxy/text-embedding-3-small` |
| `model` / `embedding_model` | String | æ¨¡å‹åç§° | `text-embedding-3-small` |
| `provider_name` | String | Provider åç§° | `openai-proxy` |
| `provider_category` | String | Provider ç±»åˆ« | `base`, `byok` |
| `model_endpoint_type` / `embedding_endpoint_type` | String | ç«¯ç‚¹ç±»å‹ | `openai` |
| `embedding_dim` | int | Embedding ç»´åº¦ | `1536` |

**æ³¨æ„**ï¼šEmbedding æ¨¡å‹å¯èƒ½ä½¿ç”¨ `embedding_model` å’Œ `embedding_endpoint_type` å­—æ®µåï¼Œè€Œä¸æ˜¯ `model` å’Œ `model_endpoint_type`ã€‚

### 18.6 å¸¸è§é—®é¢˜

#### Q1: ä¸ºä»€ä¹ˆ BYOK æ¨¡å¼éœ€è¦ `provider_name` å’Œ `provider_category`ï¼Ÿ

**A**: Letta éœ€è¦é€šè¿‡è¿™ä¸¤ä¸ªå­—æ®µåœ¨æ•°æ®åº“ä¸­ç²¾ç¡®æŸ¥æ‰¾å¯¹åº”çš„ Provider é…ç½®ã€‚åªæœ‰æ‰¾åˆ°æ­£ç¡®çš„ Providerï¼ŒLetta æ‰èƒ½è·å– API keysã€base URLs ç­‰é…ç½®æ¥è°ƒç”¨ LLM APIã€‚

#### Q2: é BYOK æ¨¡å¼ä¸ºä»€ä¹ˆä¸æä¾› `provider_name`ï¼Ÿ

**A**: é BYOK æ¨¡å¼ä½¿ç”¨ handle æ ¼å¼ï¼ˆ`provider_name/model`ï¼‰ï¼ŒLetta ä¼šä» handle ä¸­æå– `provider_name`ï¼Œç„¶åä»**ç¯å¢ƒå˜é‡**åŠ è½½å¯¹åº”çš„é…ç½®ï¼Œä¸éœ€è¦æŸ¥è¯¢æ•°æ®åº“ã€‚

#### Q3: å¦‚ä½•åˆ¤æ–­ä½¿ç”¨å“ªç§æ¨¡å¼ï¼Ÿ

**A**:
- **å‰ç«¯**: æ£€æŸ¥ `llmModel.providerCategory == 'byok'`
- **åç«¯**: Letta æ£€æŸ¥ `llm_config.provider_category` æˆ– `embedding_config.provider_category`

#### Q4: ä¸¤ç§æ¨¡å¼å¯ä»¥æ··ç”¨å—ï¼Ÿ

**A**: ä¸å¯ä»¥ã€‚åˆ›å»º Agent æ—¶ï¼ŒLLM å’Œ Embedding å¿…é¡»ä½¿ç”¨ç›¸åŒçš„æ¨¡å¼ï¼š
- è¦ä¹ˆéƒ½æ˜¯ BYOKï¼ˆ`llm_config` + `embedding_config`ï¼‰
- è¦ä¹ˆéƒ½ä¸æ˜¯ BYOKï¼ˆ`model` + `embedding`ï¼‰

### 18.7 å‰ç«¯ä»£ç å®ç°è¦ç‚¹

#### 18.7.1 LLMModel å…¼å®¹æ€§å¤„ç†

ç”±äº `/v1/models/embedding` è¿”å›çš„æ¨¡å‹æ ¼å¼ä¸ LLM æ¨¡å‹ä¸åŒï¼Œéœ€è¦å…¼å®¹å¤„ç†ï¼š

```dart
factory LLMModel.fromJson(Map<String, dynamic> json) {
  // Embedding æ¨¡å‹æ²¡æœ‰ provider_category å­—æ®µï¼Œé»˜è®¤ä¸º 'base'
  final providerCategory = json['provider_category'] as String? ?? 'base';

  return LLMModel(
    handle: json['handle'] as String,
    name: json['name'] as String,
    displayName: json['display_name'] as String,
    providerType: json['provider_type'] as String?
                ?? json['embedding_endpoint_type'] as String?
                ?? 'unknown',
    providerName: json['provider_name'] as String,
    // Embedding æ¨¡å‹ä½¿ç”¨ embedding_model å­—æ®µ
    model: json['model'] as String?
           ?? json['embedding_model'] as String?
           ?? '',
    // Embedding æ¨¡å‹ä½¿ç”¨ embedding_endpoint_type å­—æ®µ
    modelEndpointType: json['model_endpoint_type'] as String?
                      ?? json['embedding_endpoint_type'] as String?
                      ?? 'unknown',
    modelEndpoint: json['model_endpoint'] as String?
                  ?? json['embedding_endpoint'] as String?
                  ?? '',
    providerCategory: providerCategory,
    modelType: json['model_type'] as String?
               ?? (json.containsKey('embedding_model') ? 'embedding' : 'llm'),
    contextWindow: json['context_window'] as int? ?? 30000,
    // ... å…¶ä»–å­—æ®µ
  );
}
```

#### 18.7.2 æ¨¡å¼è‡ªåŠ¨åˆ‡æ¢

```dart
class CreateAgentRequest {
  final LLMModel llmModel;
  final LLMModel embeddingModel;

  // è‡ªåŠ¨æ£€æµ‹æ¨¡å¼
  bool get isBYOK => llmModel.providerCategory == 'byok';

  // æ ¹æ®æ¨¡å¼ç”Ÿæˆæ­£ç¡®çš„ JSON
  Map<String, dynamic> toJson() {
    return isBYOK ? toBYOKJson() : toSimpleJson();
  }
}
```

### 18.8 æ€»ç»“

| ç‰¹æ€§ | é BYOK æ¨¡å¼ | BYOK æ¨¡å¼ |
|------|-------------|-----------|
| **é…ç½®æ¥æº** | ç¯å¢ƒå˜é‡ | æ•°æ®åº“ |
| **API æ ¼å¼** | ç®€åŒ– (`model` + `embedding`) | å®Œæ•´ (`llm_config` + `embedding_config`) |
| **æ¨¡å‹æ ‡è¯†** | handle (`provider/model`) | å®Œæ•´é…ç½® |
| **Provider æŸ¥æ‰¾** | ä» handle æå– provider_name | ä½¿ç”¨ `provider_name` + `provider_category` |
| **API Keys** | ç¯å¢ƒå˜é‡ | Provider é…ç½® |
| **é€‚ç”¨åœºæ™¯** | å¼€å‘ã€æµ‹è¯• | ç”Ÿäº§ã€å¤šç”¨æˆ· |

**æ ¸å¿ƒè®¾è®¡ç†å¿µ**ï¼š
- é BYOK æ¨¡å¼ï¼šç®€å•ä¼˜å…ˆï¼Œé€‚åˆå¿«é€Ÿå¼€å‘å’Œæµ‹è¯•
- BYOK æ¨¡å¼ï¼šçµæ´»ä¼˜å…ˆï¼Œæ”¯æŒå¤šç”¨æˆ·ã€å¤š Providerã€è‡ªå®šä¹‰é…ç½®

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv2.6
**æœ€åæ›´æ–°**ï¼š2026-01-07
**è°ƒæŸ¥è€…**ï¼šClaude Code (Sonnet 4.5)

## 19. Base æ¨¡å‹å’Œ Embedding æ¨¡å‹çš„è·å–æµç¨‹ï¼ˆ2026-01-09ï¼‰

### 19.1 æ¦‚è¿°

Letta åç«¯æä¾›äº†ä¸¤ä¸ªä¸»è¦çš„ API ç«¯ç‚¹æ¥è·å–æ¨¡å‹åˆ—è¡¨ï¼š

1. **`GET /v1/models/?provider_category=base`** - è·å– Base LLM æ¨¡å‹åˆ—è¡¨
2. **`GET /v1/models/embedding`** - è·å– Base Embedding æ¨¡å‹åˆ—è¡¨

è¿™ä¸¤ä¸ªç«¯ç‚¹çš„å®ç°å®Œå…¨ä¸åŒï¼Œæœ¬æ–‡æ¡£è¯¦ç»†åˆ†æå®ƒä»¬çš„è·å–æµç¨‹ã€‚

---

### 19.2 Base LLM æ¨¡å‹è·å–æµç¨‹

#### 19.2.1 API ç«¯ç‚¹

```python
# letta/server/rest_api/routers/v1/llms.py:15
@router.get("/", response_model=List[Model], operation_id="list_models")
async def list_llm_models(
    provider_category: Optional[List[ProviderCategory]] = Query(None),
    provider_name: Optional[str] = Query(None),
    provider_type: Optional[ProviderType] = Query(None),
    server: "SyncServer" = Depends(get_letta_server),
    headers: HeaderParams = Depends(get_headers),
):
    actor = await server.user_manager.get_actor_or_default_async(actor_id=headers.actor_id)
    
    models = await server.list_llm_models_async(
        provider_category=provider_category,
        provider_name=provider_name,
        provider_type=provider_type,
        actor=actor,
    )
    
    return [Model.from_llm_config(model) for model in models]
```

**è¯·æ±‚ç¤ºä¾‹**ï¼š
```bash
GET /v1/models/?provider_category=base
```

**å“åº”ç¤ºä¾‹**ï¼š
```json
[
  {
    "handle": "openai-proxy/claude-sonnet-4-5-20250929",
    "name": "claude-sonnet-4-5-20250929",
    "display_name": "claude-sonnet-4-5-20250929",
    "provider_type": "openai",
    "provider_name": "openai-proxy",
    "model_type": "llm",
    "model": "claude-sonnet-4-5-20250929",
    "model_endpoint_type": "openai",
    "model_endpoint": "https://lingyunapi.com/v1",
    "provider_category": "base",
    "context_window": 30000,
    ...
  }
]
```

#### 19.2.2 æ ¸å¿ƒå®ç°æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. REST API æ¥æ”¶è¯·æ±‚                                         â”‚
â”‚     list_llm_models(provider_category=[ProviderCategory.base])â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Server å±‚å¤„ç†                                            â”‚
â”‚     server.list_llm_models_async(...)                       â”‚
â”‚     letta/server/server.py:1047                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. è·å–å¯ç”¨çš„ Provider åˆ—è¡¨                                 â”‚
â”‚     providers = await self.get_enabled_providers_async(     â”‚
â”‚         provider_category=[ProviderCategory.base],          â”‚
â”‚         actor=actor                                         â”‚
â”‚     )                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. get_enabled_providers_async é€»è¾‘                        â”‚
â”‚     letta/server/server.py:1124                             â”‚
â”‚                                                              â”‚
â”‚     providers = []                                          â”‚
â”‚     if not provider_category or ProviderCategory.base in    â”‚
â”‚        provider_category:                                   â”‚
â”‚         # ä»ç¯å¢ƒå˜é‡åŠ è½½çš„ Provider                          â”‚
â”‚         providers_from_env = [p for p in                    â”‚
â”‚             self._enabled_providers]                        â”‚
â”‚         providers.extend(providers_from_env)                â”‚
â”‚                                                              â”‚
â”‚     if not provider_category or ProviderCategory.byok in    â”‚
â”‚        provider_category:                                   â”‚
â”‚         # ä»æ•°æ®åº“åŠ è½½çš„ Provider                            â”‚
â”‚         providers_from_db = await self                      â”‚
â”‚             .provider_manager.list_providers_async(...)     â”‚
â”‚         providers.extend(providers_from_db)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. å¹¶å‘è·å–æ‰€æœ‰ Provider çš„æ¨¡å‹åˆ—è¡¨                         â”‚
â”‚     import asyncio                                          â”‚
â”‚     async def get_provider_models(provider):                â”‚
â”‚         return await provider.list_llm_models_async()       â”‚
â”‚                                                              â”‚
â”‚     provider_results = await asyncio.gather(*[              â”‚
â”‚         get_provider_models(p) for p in providers           â”‚
â”‚     ])                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. Provider.list_llm_models_async() å®ç°                   â”‚
â”‚     æ¯ä¸ª Provider å­ç±»å®ç°è‡ªå·±çš„æ¨¡å‹åˆ—è¡¨                     â”‚
â”‚                                                              â”‚
â”‚     ä¾‹å¦‚ï¼šOpenAIProvider                                    â”‚
â”‚     - è°ƒç”¨ OpenAI API çš„ GET /v1/models ç«¯ç‚¹                 â”‚
â”‚     - è§£æè¿”å›çš„æ¨¡å‹åˆ—è¡¨                                     â”‚
â”‚     - è¿‡æ»¤å‡ºä¸æ”¯æŒçš„æ¨¡å‹                                     â”‚
â”‚     - æ„é€  LLMConfig å¯¹è±¡                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. è¿”å›åˆå¹¶çš„æ¨¡å‹åˆ—è¡¨                                       â”‚
â”‚     all_models = []                                         â”‚
â”‚     for models in provider_results:                         â”‚
â”‚         all_models.extend(models)                           â”‚
â”‚     return all_models                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 19.2.3 _enabled_providers åˆå§‹åŒ–

`_enabled_providers` æ˜¯åœ¨ Server å¯åŠ¨æ—¶ä»**ç¯å¢ƒå˜é‡**åˆå§‹åŒ–çš„ï¼š

```python
# letta/server/server.py:211
self._enabled_providers: List[Provider] = [LettaProvider(name="letta")]

# OpenAI Provider
if model_settings.openai_api_key:
    self._enabled_providers.append(
        OpenAIProvider(
            name="openai",
            api_key_enc=Secret.from_plaintext(model_settings.openai_api_key),
            base_url=model_settings.openai_api_base,
        )
    )

# Anthropic Provider
if model_settings.anthropic_api_key:
    self._enabled_providers.append(
        AnthropicProvider(
            name="anthropic",
            api_key_enc=Secret.from_plaintext(model_settings.anthropic_api_key),
        )
    )

# ... å…¶ä»– Providerï¼ˆOllama, Google, Azure, Groq, etc.ï¼‰
```

**å…³é”®ç‚¹**ï¼š
- `_enabled_providers` æ˜¯**å†…å­˜ Provider**ï¼ˆProviderCategory.baseï¼‰
- è¿™äº› Provider çš„é…ç½®æ¥è‡ª**ç¯å¢ƒå˜é‡**
- Server å¯åŠ¨æ—¶ä¸€æ¬¡æ€§åŠ è½½ï¼Œè¿è¡ŒæœŸé—´ä¸å˜

#### 19.2.4 Provider çš„ list_llm_models_async() å®ç°

ä¸åŒçš„ Provider æœ‰ä¸åŒçš„å®ç°ï¼š

**LettaProvider**ï¼ˆç¡¬ç¼–ç ï¼‰ï¼š
```python
# letta/schemas/providers.py:162
async def list_llm_models_async(self) -> List[LLMConfig]:
    return [
        LLMConfig(
            model="letta-free",
            model_endpoint_type="openai",
            model_endpoint=LETTA_MODEL_ENDPOINT,
            context_window=30000,
            handle=self.get_handle("letta-free"),
            provider_name=self.name,
            provider_category=self.provider_category,
        )
    ]
```

**OpenAIProvider**ï¼ˆè°ƒç”¨ APIï¼‰ï¼š
```python
# letta/schemas/providers.py:254
async def list_llm_models_async(self) -> List[LLMConfig]:
    data = await self._get_models_async()
    return self._list_llm_models(data)

async def _get_models_async(self) -> List[dict]:
    from letta.llm_api.openai import openai_get_model_list_async
    
    response = await openai_get_model_list_async(
        self.base_url,
        api_key=self.api_key,
        extra_params=extra_params,
    )
    
    if "data" in response:
        return response["data"]
    else:
        return response

def _list_llm_models(self, data) -> List[LLMConfig]:
    configs = []
    for model in data:
        model_name = model["id"]
        
        # è¿‡æ»¤ä¸æ”¯æŒçš„æ¨¡å‹
        if self.base_url == "https://api.openai.com/v1":
            # è·³è¿‡ä¸æ”¯æŒ tool calling çš„æ¨¡å‹
            disallowed_types = ["transcribe", "search", "realtime", 
                                "tts", "audio", "o1-mini", "o1-preview"]
            if any(keyword in model_name for keyword in disallowed_types):
                continue
        
        # æ„é€  LLMConfig
        llm_config = LLMConfig(
            model=model_name,
            model_endpoint_type="openai",
            model_endpoint=self.base_url,
            context_window=self.get_model_context_window_size(model_name),
            handle=self.get_handle(model_name),
            provider_name=self.name,
            provider_category=self.provider_category,
        )
        configs.append(llm_config)
    
    return configs
```

**AnthropicProvider**ï¼ˆè°ƒç”¨ APIï¼‰ï¼š
```python
# letta/schemas/providers.py:769
async def list_llm_models_async(self) -> List[LLMConfig]:
    from letta.llm_api.anthropic import anthropic_get_model_list_async
    
    models = await anthropic_get_model_list_async(api_key=self.api_key)
    return self._list_llm_models(models)
```

---

### 19.3 Base Embedding æ¨¡å‹è·å–æµç¨‹

#### 19.3.1 API ç«¯ç‚¹

```python
# letta/server/rest_api/routers/v1/llms.py:42
@router.get("/embedding", response_model=List[EmbeddingModel], 
           operation_id="list_embedding_models")
async def list_embedding_models(
    server: "SyncServer" = Depends(get_letta_server),
    headers: HeaderParams = Depends(get_headers),
):
    actor = await server.user_manager.get_actor_or_default_async(
        actor_id=headers.actor_id
    )
    models = await server.list_embedding_models_async(actor=actor)
    
    return [EmbeddingModel.from_embedding_config(model) 
            for model in models]
```

**è¯·æ±‚ç¤ºä¾‹**ï¼š
```bash
GET /v1/models/embedding
```

**å“åº”ç¤ºä¾‹**ï¼š
```json
[
  {
    "handle": "openai/text-embedding-3-small",
    "name": "text-embedding-3-small",
    "display_name": "text-embedding-3-small",
    "provider_type": "openai",
    "provider_name": "openai",
    "model_type": "embedding",
    "embedding_endpoint_type": "openai",
    "embedding_endpoint": "https://lingyunapi.com/v1",
    "embedding_model": "text-embedding-3-small",
    "embedding_dim": 1536,
    "embedding_chunk_size": 300,
    "batch_size": 1024
  }
]
```

**æ³¨æ„**ï¼šEmbedding æ¨¡å‹çš„å“åº”**æ²¡æœ‰** `provider_category` å­—æ®µï¼

#### 19.3.2 æ ¸å¿ƒå®ç°æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. REST API æ¥æ”¶è¯·æ±‚                                         â”‚
â”‚     list_embedding_models()                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Server å±‚å¤„ç†                                            â”‚
â”‚     server.list_embedding_models_async(actor)               â”‚
â”‚     letta/server/server.py:1098                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. è·å–æ‰€æœ‰å¯ç”¨çš„ Providerï¼ˆåŒ…æ‹¬ base å’Œ byokï¼‰            â”‚
â”‚     providers = await self.get_enabled_providers_async(     â”‚
â”‚         actor=actor                                         â”‚
â”‚     )                                                       â”‚
â”‚                                                              â”‚
â”‚     æ³¨æ„ï¼šè¿™é‡Œä¸ä¼  provider_category å‚æ•°                   â”‚
â”‚     æ‰€ä»¥ä¼šåŒæ—¶è¿”å› base å’Œ byok çš„ Provider                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. å¹¶å‘è·å–æ‰€æœ‰ Provider çš„ Embedding æ¨¡å‹åˆ—è¡¨             â”‚
â”‚     async def get_provider_embedding_models(provider):      â”‚
â”‚         try:                                                â”‚
â”‚             return await provider.list_embedding_models_async()â”‚
â”‚         except Exception as e:                              â”‚
â”‚             logger.exception(...)                           â”‚
â”‚             return []                                        â”‚
â”‚                                                              â”‚
â”‚     provider_results = await asyncio.gather(*[              â”‚
â”‚         get_provider_embedding_models(p)                    â”‚
â”‚         for p in providers                                  â”‚
â”‚     ])                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Provider.list_embedding_models_async() å®ç°            â”‚
â”‚     æ¯ä¸ª Provider å­ç±»å®ç°è‡ªå·±çš„ Embedding åˆ—è¡¨              â”‚
â”‚                                                              â”‚
â”‚     ä¾‹å¦‚ï¼šOpenAIProvider                                    â”‚
â”‚     - ç¡¬ç¼–ç æ”¯æŒçš„ embedding æ¨¡å‹åˆ—è¡¨                        â”‚
â”‚     - æ„é€  EmbeddingConfig å¯¹è±¡                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. è¿”å›åˆå¹¶çš„ Embedding æ¨¡å‹åˆ—è¡¨                           â”‚
â”‚     embedding_models = []                                  â”‚
â”‚     for models in provider_results:                         â”‚
â”‚         embedding_models.extend(models)                     â”‚
â”‚     return embedding_models                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 19.3.3 Provider çš„ list_embedding_models_async() å®ç°

**LettaProvider**ï¼ˆç¡¬ç¼–ç ï¼‰ï¼š
```python
# letta/schemas/providers.py:175
def list_embedding_models(self):
    return [
        EmbeddingConfig(
            embedding_model="letta-free",
            embedding_endpoint_type="hugging-face",
            embedding_endpoint="https://embeddings.memgpt.ai",
            embedding_dim=1024,
            embedding_chunk_size=300,
            handle=self.get_handle("letta-free", is_embedding=True),
            batch_size=32,
        )
    ]
```

**OpenAIProvider**ï¼ˆç¡¬ç¼–ç ï¼‰ï¼š
```python
# letta/schemas/providers.py:391
async def list_embedding_models_async(self) -> List[EmbeddingConfig]:
    if self.base_url == "https://api.openai.com/v1":
        # TODO: å®é™…ä¸Šåº”è¯¥è‡ªåŠ¨åˆ—å‡º OpenAI çš„æ¨¡å‹
        return [
            EmbeddingConfig(
                embedding_model="text-embedding-ada-002",
                embedding_endpoint_type="openai",
                embedding_endpoint=self.base_url,
                embedding_dim=1536,
                embedding_chunk_size=300,
                handle=self.get_handle("text-embedding-ada-002", 
                                     is_embedding=True),
                batch_size=1024,
            ),
            EmbeddingConfig(
                embedding_model="text-embedding-3-small",
                embedding_endpoint_type="openai",
                embedding_endpoint=self.base_url,
                embedding_dim=2000,
                embedding_chunk_size=300,
                handle=self.get_handle("text-embedding-3-small", 
                                     is_embedding=True),
                batch_size=1024,
            ),
            EmbeddingConfig(
                embedding_model="text-embedding-3-large",
                embedding_endpoint_type="openai",
                embedding_endpoint=self.base_url,
                embedding_dim=2000,
                embedding_chunk_size=300,
                handle=self.get_handle("text-embedding-3-large", 
                                     is_embedding=True),
                batch_size=1024,
            ),
        ]
    else:
        # é OpenAI å®˜æ–¹ç«¯ç‚¹ï¼Œä» API åŠ¨æ€è·å–
        return self.list_embedding_models()
```

**AnthropicProvider**ï¼ˆä¸æ”¯æŒï¼‰ï¼š
```python
# letta/schemas/providers.py:867
def list_embedding_models(self) -> List[EmbeddingConfig]:
    # Anthropic ä¸æ”¯æŒ embedding
    return []
```

**GoogleAIProvider**ï¼ˆè°ƒç”¨ APIï¼‰ï¼š
```python
# letta/schemas/providers.py:1295
async def list_embedding_models_async(self):
    from letta.llm_api.google_ai_client import google_ai_get_model_list_async
    
    model_options = await google_ai_get_model_list_async(
        base_url=self.base_url, 
        api_key=self.api_key
    )
    return self._list_embedding_models(model_options)

def _list_embedding_models(self, model_options):
    # è¿‡æ»¤å‡ºæ”¯æŒ 'embedContent' çš„æ¨¡å‹
    model_options = [mo for mo in model_options 
                     if "embedContent" in mo["supportedGenerationMethods"]]
    model_options = [str(m["name"]) for m in model_options]
    model_options = [mo[len("models/"):] if mo.startswith("models/") 
                     else mo for mo in model_options]
    
    configs = []
    for model in model_options:
        configs.append(
            EmbeddingConfig(
                embedding_model=model,
                embedding_endpoint_type="google_ai",
                embedding_endpoint=self.base_url,
                embedding_dim=768,
                embedding_chunk_size=300,
                handle=self.get_handle(model, is_embedding=True),
                batch_size=1024,
            )
        )
    return configs
```

---

### 19.4 å…³é”®åŒºåˆ«æ€»ç»“

#### 19.4.1 API ç«¯ç‚¹

| ç‰¹æ€§ | LLM æ¨¡å‹ | Embedding æ¨¡å‹ |
|------|---------|---------------|
| **ç«¯ç‚¹** | `GET /v1/models/?provider_category=base` | `GET /v1/models/embedding` |
| **å‚æ•°** | æ”¯æŒ `provider_category`, `provider_name`, `provider_type` | æ— å‚æ•° |
| **è¿”å›æ ¼å¼** | `List[Model]` (extends LLMConfig) | `List[EmbeddingModel]` (extends EmbeddingConfig) |
| **å­—æ®µå·®å¼‚** | æœ‰ `provider_category` å­—æ®µ | **æ— ** `provider_category` å­—æ®µ |

#### 19.4.2 å®ç°å·®å¼‚

| ç‰¹æ€§ | LLM æ¨¡å‹ | Embedding æ¨¡å‹ |
|------|---------|---------------|
| **Provider ç­›é€‰** | æ ¹æ® `provider_category` å‚æ•°ç­›é€‰ | è·å–æ‰€æœ‰ Provider çš„ Embedding æ¨¡å‹ |
| **æ¨¡å‹æ¥æº** | å¤§éƒ¨åˆ† Provider ä» API åŠ¨æ€è·å– | å¤§éƒ¨åˆ† Provider ç¡¬ç¼–ç æ¨¡å‹åˆ—è¡¨ |
| **è¿‡æ»¤é€»è¾‘** | å¤æ‚ï¼ˆè¿‡æ»¤ä¸æ”¯æŒ tool calling çš„æ¨¡å‹ï¼‰ | ç®€å•ï¼ˆé€šå¸¸ç›´æ¥è¿”å›å›ºå®šåˆ—è¡¨ï¼‰ |

#### 19.4.3 ä¸ºä»€ä¹ˆ `/v1/models/?provider_category=base` ä¸è¿”å› Embedding æ¨¡å‹ï¼Ÿ

**åŸå› **ï¼š

1. **æ¨¡å‹ç±»å‹è¿‡æ»¤**ï¼š
   - `Provider.list_llm_models_async()` åªè¿”å› `LLMConfig` å¯¹è±¡
   - æŸäº› Providerï¼ˆå¦‚ TogetherAIï¼‰ä¼šæ ¹æ® API è¿”å›çš„ `type` å­—æ®µè¿‡æ»¤ï¼š
     ```python
     if "type" in model and model["type"] not in ["chat", "language"]:
         continue  # è·³è¿‡ embedding æ¨¡å‹
     ```

2. **API ç«¯ç‚¹è®¾è®¡**ï¼š
   - `/v1/models/` è®¾è®¡ç”¨äºåˆ—å‡º **LLM æ¨¡å‹**
   - `/v1/models/embedding` ä¸“é—¨ç”¨äºåˆ—å‡º **Embedding æ¨¡å‹**
   - ä¸¤è€…åˆ†ç¦»ï¼Œé¿å…æ··æ·†

3. **å“åº”æ ¼å¼å·®å¼‚**ï¼š
   - LLM æ¨¡å‹ï¼š`model`, `model_endpoint_type`, `model_endpoint`
   - Embedding æ¨¡å‹ï¼š`embedding_model`, `embedding_endpoint_type`, `embedding_endpoint`

---

### 19.5 å‰ç«¯è°ƒç”¨å»ºè®®

#### 19.5.1 é BYOK æ¨¡å¼

```dart
// 1. åŠ è½½ LLM æ¨¡å‹
final allModels = await ref.read(baseLLMModelListProvider.future);
final llmModels = allModels.where((m) => m.modelType == 'llm').toList();

// 2. åŠ è½½ Embedding æ¨¡å‹ï¼ˆéœ€è¦å•ç‹¬è°ƒç”¨ï¼‰
final embeddingResponse = await ref.read(apiClientProvider).get('/models/embedding');
final List<dynamic> embeddingData = jsonDecode(embeddingResponse.body);
final embeddingModels = embeddingData
    .map((json) => LLMModel.fromJson(json as Map<String, dynamic>))
    .toList();
```

#### 19.5.2 BYOK æ¨¡å¼

```dart
// 1. å…ˆè·å– Provider åˆ—è¡¨
final providers = await ref.read(providerListProvider.future);

// 2. æ ¹æ®é€‰æ‹©çš„ Provider åŠ¨æ€åŠ è½½æ¨¡å‹
final provider = providers.firstWhere((p) => p.name == selectedProviderName);

// 3. åŠ è½½ LLM æ¨¡å‹
final llmModels = await ref.read(llmModelListByProviderProvider(provider.name).future);

// 4. åŠ è½½ Embedding æ¨¡å‹ï¼ˆä» Provider çš„ embedding_models å­—æ®µï¼‰
// æˆ–è€…è°ƒç”¨ /v1/models/embedding å¹¶æŒ‰ provider_name è¿‡æ»¤
```

---

### 19.6 å¸¸è§é—®é¢˜

#### Q1: ä¸ºä»€ä¹ˆ `/v1/models/?provider_category=base` ä¸è¿”å› Embedding æ¨¡å‹ï¼Ÿ

**A**: 
- `/v1/models/` ç«¯ç‚¹è®¾è®¡ç”¨äºåˆ—å‡º **LLM æ¨¡å‹**ï¼ˆ`model_type="llm"`ï¼‰
- Embedding æ¨¡å‹éœ€è¦å•ç‹¬è°ƒç”¨ `/v1/models/embedding` ç«¯ç‚¹
- ä¸¤è€…è¿”å›çš„æ•°æ®ç»“æ„ä¸åŒï¼ˆå­—æ®µåä¸åŒï¼‰

#### Q2: Embedding æ¨¡å‹ä¸ºä»€ä¹ˆæ²¡æœ‰ `provider_category` å­—æ®µï¼Ÿ

**A**:
- `/v1/models/embedding` ç«¯ç‚¹ä¼šè¿”å›**æ‰€æœ‰** Provider çš„ Embedding æ¨¡å‹ï¼ˆåŒ…æ‹¬ base å’Œ byokï¼‰
- å› æ­¤ä¸åŒºåˆ† `provider_category`
- å‰ç«¯é»˜è®¤å°† Embedding æ¨¡å‹è§†ä¸º `provider_category="base"`

#### Q3: å¦‚ä½•åˆ¤æ–­ä¸€ä¸ªæ¨¡å‹æ˜¯ LLM è¿˜æ˜¯ Embeddingï¼Ÿ

**A**:
- æ£€æŸ¥ `model_type` å­—æ®µï¼š
  - `"llm"` â†’ LLM æ¨¡å‹
  - `"embedding"` â†’ Embedding æ¨¡å‹
- æˆ–è€…æ£€æŸ¥å­—æ®µåï¼š
  - æœ‰ `model` å­—æ®µ â†’ LLM æ¨¡å‹
  - æœ‰ `embedding_model` å­—æ®µ â†’ Embedding æ¨¡å‹

#### Q4: BYOK æ¨¡å¼ä¸‹å¦‚ä½•è·å– Embedding æ¨¡å‹ï¼Ÿ

**A**:
- æ–¹æ³• 1ï¼šè°ƒç”¨ `/v1/models/embedding`ï¼Œç„¶åæŒ‰ `provider_name` è¿‡æ»¤
- æ–¹æ³• 2ï¼šä»æ•°æ®åº“ Provider é…ç½®ä¸­è¯»å– `embedding_models` å­—æ®µï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰

---

### 19.7 æ€»ç»“

#### 19.7.1 æµç¨‹å¯¹æ¯”å›¾

```
LLM æ¨¡å‹è·å–æµç¨‹:
GET /v1/models/?provider_category=base
  â†“
server.list_llm_models_async(provider_category=[base])
  â†“
get_enabled_providers_async(provider_category=[base])
  â†“ (åªè¿”å› _enabled_providers)
è¿”å› base Providers (ä»ç¯å¢ƒå˜é‡)
  â†“
provider.list_llm_models_async()
  â†“ (åŠ¨æ€è°ƒç”¨ API æˆ–ç¡¬ç¼–ç )
è¿”å› LLM æ¨¡å‹åˆ—è¡¨

Embedding æ¨¡å‹è·å–æµç¨‹:
GET /v1/models/embedding
  â†“
server.list_embedding_models_async()
  â†“
get_enabled_providers_async() (æ— å‚æ•°)
  â†“ (è¿”å›æ‰€æœ‰ Providers)
è¿”å› base + byok Providers
  â†“
provider.list_embedding_models_async()
  â†“ (é€šå¸¸ç¡¬ç¼–ç )
è¿”å› Embedding æ¨¡å‹åˆ—è¡¨
```

#### 19.7.2 å…³é”®è¦ç‚¹

1. **ä¸¤ä¸ªç‹¬ç«‹çš„ç«¯ç‚¹**ï¼š
   - `/v1/models/` â†’ LLM æ¨¡å‹
   - `/v1/models/embedding` â†’ Embedding æ¨¡å‹

2. **Provider ç­›é€‰é€»è¾‘ä¸åŒ**ï¼š
   - LLMï¼šæ ¹æ® `provider_category` å‚æ•°ç­›é€‰
   - Embeddingï¼šè·å–æ‰€æœ‰ Provider çš„ Embedding æ¨¡å‹

3. **æ¨¡å‹æ¥æºä¸åŒ**ï¼š
   - LLMï¼šå¤§éƒ¨åˆ†ä» API åŠ¨æ€è·å–
   - Embeddingï¼šå¤§éƒ¨åˆ†ç¡¬ç¼–ç åˆ—è¡¨

4. **å‰ç«¯éœ€è¦åˆ†åˆ«è°ƒç”¨**ï¼š
   - é BYOK æ¨¡å¼ï¼šå…ˆè°ƒç”¨ `/v1/models/?provider_category=base`ï¼Œå†è°ƒç”¨ `/v1/models/embedding`
   - BYOK æ¨¡å¼ï¼šå…ˆé€‰æ‹© Providerï¼Œå†åŠ¨æ€åŠ è½½å¯¹åº”çš„æ¨¡å‹

---

## 20. Agent æ˜¾ç¤ºå’Œæ¨¡å¼åˆ¤æ–­çš„å…³é”®å‘ç°ï¼ˆ2026-01-09ï¼‰

### 20.1 é—®é¢˜èƒŒæ™¯

åœ¨å®ç°å‰ç«¯ Agent åˆ—è¡¨å’Œè¯¦æƒ…é¡µé¢æ—¶ï¼Œå‘ç°äº†ä¸‰ä¸ªå…³é”®é—®é¢˜ï¼š

1. **Agent åˆ—è¡¨æ˜¾ç¤ºé—®é¢˜**ï¼šæœ‰äº› Agent æ˜¾ç¤ºæ¨¡å‹ handleï¼ˆå¦‚ `openai-proxy/claude-opus-4-1`ï¼‰ï¼Œæœ‰äº›åªæ˜¾ç¤ºæ¨¡å‹åï¼ˆå¦‚ `claude-opus-4-1`ï¼‰
2. **è¯¦æƒ…é¡µé¢ä¿¡æ¯æ˜¾ç¤ºä¸æ¸…æ™°**ï¼šé”®å€¼å¯¹çš„å­—ä½“å¤§å°å’Œé—´è·è®©äººéš¾ä»¥åŒºåˆ†
3. **æ‰€æœ‰ Agent çš„ Category éƒ½æ˜¾ç¤º BYOK**ï¼šå³ä½¿ Base æ¨¡å¼åˆ›å»ºçš„ Agent ä¹Ÿæ˜¾ç¤ºä¸º BYOK

### 20.2 æ ¸å¿ƒå‘ç°

#### 20.2.1 Agent API è¿”å›æ ¼å¼çš„çœŸç›¸

é€šè¿‡æµ‹è¯• `/v1/agents/` APIï¼Œæˆ‘ä»¬å‘ç°äº†é‡è¦çš„äº‹å®ï¼š

**æµ‹è¯•ç»“æœ**ï¼š
```bash
curl -H "Authorization: Bearer $TOKEN" http://38.175.200.93:8283/v1/agents/ | python3 -c "
import sys, json
agents = json.load(sys.stdin)
base_agents = [a for a in agents if a.get('model') is not None]
byok_agents = [a for a in agents if a.get('model') is None]
print(f'Total agents: {len(agents)}')
print(f'Base mode agents: {len(base_agents)}')
print(f'BYOK mode agents: {len(byok_agents)}')
"
```

**è¾“å‡º**ï¼š
```
Total agents: 13
Base mode agents: 4
BYOK mode agents: 13
```

ç­‰ç­‰ï¼13 ä¸ª BYOK + 4 ä¸ª Base = 17ï¼Œä½†æ€»å…±åªæœ‰ 13 ä¸ª Agentï¼Ÿ

**å…³é”®å‘ç°**ï¼š
- **æ‰€æœ‰ Agent éƒ½æœ‰ `llm_config` å­—æ®µ**ï¼ˆåŒ…æ‹¬ Base æ¨¡å¼çš„ï¼‰
- Base æ¨¡å¼ Agentï¼šæœ‰ `model` å­—æ®µï¼ˆhandle æ ¼å¼ï¼‰ï¼Œä¹Ÿæœ‰ `llm_config`
- BYOK æ¨¡å¼ Agentï¼š`model` å­—æ®µä¸º `null`ï¼Œåªæœ‰ `llm_config`

#### 20.2.2 Base æ¨¡å¼ Agent çš„å®Œæ•´ç»“æ„ç¤ºä¾‹

```json
{
  "id": "agent-2ebdb596-ce9e-4598-b673-c47d4e11e00b",
  "name": "123",
  "model": "openai-proxy/claude-opus-4-1-20250805-thinking",  // â† Base æ¨¡å¼æ ‡è®°
  "embedding": "openai/text-embedding-3-small",               // â† Base æ¨¡å¼æ ‡è®°
  "llm_config": {
    "model": "claude-opus-4-1-20250805-thinking",
    "provider_name": "openai-proxy",
    "provider_category": "byok",  // â† æ³¨æ„ï¼šè¿™é‡Œæ˜¯ "byok"ï¼Œä½†å®é™…æ˜¯ Base æ¨¡å¼ï¼
    "handle": "openai-proxy/claude-opus-4-1-20250805-thinking"  // â† Base æ¨¡å¼æœ‰ handle
  },
  "embedding_config": {
    "embedding_model": "text-embedding-3-small",
    "provider_name": "openai",
    "handle": "openai/text-embedding-3-small"  // â† Base æ¨¡å¼æœ‰ handle
  }
}
```

#### 20.2.3 BYOK æ¨¡å¼ Agent çš„å®Œæ•´ç»“æ„ç¤ºä¾‹

```json
{
  "id": "agent-81bbe323-aa3c-48c5-8a03-3f8c66350adf",
  "name": "ggvvygv ygv",
  "model": null,           // â† BYOK æ¨¡å¼æ ‡è®°
  "embedding": null,       // â† BYOK æ¨¡å¼æ ‡è®°
  "llm_config": {
    "model": "claude-opus-4-1-20250805-thinking",
    "provider_name": "openai-proxy",
    "provider_category": "byok",
    "handle": null  // â† BYOK æ¨¡å¼ handle ä¸º null
  },
  "embedding_config": {
    "embedding_model": "text-embedding-3-large",
    "provider_name": "openai-proxy",
    "handle": null  // â† BYOK æ¨¡å¼ handle ä¸º null
  }
}
```

### 20.3 å…³é”®ç»“è®º

#### 20.3.1 å¦‚ä½•åˆ¤æ–­ Agent æ˜¯ Base è¿˜æ˜¯ BYOK æ¨¡å¼ï¼Ÿ

**âŒ é”™è¯¯æ–¹æ³•**ï¼š
```dart
// ä¸è¦ç”¨ provider_category å­—æ®µåˆ¤æ–­ï¼
if (agent.llmConfig['provider_category'] == 'base') {
  // è¿™ä¸ªåˆ¤æ–­ä¸å¯é ï¼Œå› ä¸º Base æ¨¡å¼çš„ agent ä¹Ÿå¯èƒ½æ˜¾ç¤º "byok"
}
```

**âœ… æ­£ç¡®æ–¹æ³•**ï¼š
```dart
// æ–¹æ³• 1ï¼šæ£€æŸ¥ model å­—æ®µï¼ˆæ¨èï¼‰
if (agent.model != null) {
  // Base æ¨¡å¼
} else {
  // BYOK æ¨¡å¼
}

// æ–¹æ³• 2ï¼šæ£€æŸ¥ llm_config.handle å­—æ®µ
if (agent.llmConfig?['handle'] != null) {
  // Base æ¨¡å¼
} else {
  // BYOK æ¨¡å¼
}
```

#### 20.3.2 Agent åˆ—è¡¨é¡µé¢æ˜¾ç¤ºæ¨¡å‹çš„æ­£ç¡®æ–¹å¼

**é—®é¢˜**ï¼š
- Base æ¨¡å¼ï¼š`agent.model` = `"openai-proxy/claude-opus-4-1"`ï¼ˆå®Œæ•´ handleï¼‰
- BYOK æ¨¡å¼ï¼š`agent.model` = `null`ï¼Œåªæœ‰ `agent.llmConfig['model']` = `"claude-opus-4-1"`ï¼ˆåªæœ‰æ¨¡å‹åï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š
```dart
String _getModelLabel(Agent agent) {
  // Base æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨ agent.modelï¼ˆå·²ç»æ˜¯ handle æ ¼å¼ï¼‰
  if (agent.model != null) {
    return agent.model!;
  }

  // BYOK æ¨¡å¼ï¼šç»„åˆ provider_name + model
  if (agent.llmConfig != null) {
    final provider = agent.llmConfig!['provider_name']?.toString() ?? 'unknown';
    final model = agent.llmConfig!['model']?.toString() ?? 'unknown';
    return '$provider/$model';  // æ„é€  handle æ ¼å¼
  }

  return 'Unknown';
}
```

#### 20.3.3 è¯¦æƒ…é¡µé¢çš„æ¸…æ™°æ˜¾ç¤ºè®¾è®¡

**é—®é¢˜**ï¼šåŸæ¥çš„ `_InfoRow` ç»„ä»¶ä½¿ç”¨å‚ç›´å¸ƒå±€ï¼Œlabel å’Œ value ä¸Šä¸‹æ’åˆ—ï¼Œéš¾ä»¥åŒºåˆ†ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šæ”¹ç”¨æ°´å¹³å¸ƒå±€ï¼Œlabel å›ºå®šå®½åº¦ï¼Œvalue è‡ªé€‚åº”ã€‚

```dart
class _InfoRow extends StatelessWidget {
  @override
  Widget build(BuildContext context) {
    return Row(
      crossAxisAlignment: CrossAxisAlignment.start,
      children: [
        // Label (å·¦å¯¹é½ï¼Œå›ºå®šå®½åº¦ 120px)
        SizedBox(
          width: 120,
          child: Text(
            label,
            style: AppTheme.labelMedium.copyWith(
              color: AppTheme.textSecondaryColor,
              fontWeight: FontWeight.w600,  // æ›´é†’ç›®
            ),
          ),
        ),
        // Value (å³å¯¹é½ï¼Œè‡ªé€‚åº”å®½åº¦)
        Expanded(
          child: Text(
            value,
            style: (valueStyle ?? AppTheme.bodyMedium).copyWith(
              fontWeight: FontWeight.w500,  // æ›´æ¸…æ™°
            ),
            maxLines: isMultiline ? null : 3,
            overflow: isMultiline ? null : TextOverflow.ellipsis,
          ),
        ),
      ],
    );
  }
}
```

**æ•ˆæœ**ï¼š
```
Agent ID          agent-2ebdb596-ce9e-4598...
Name              123
Description       This is a test agent
```

### 20.4 API è¿”å›æ ¼å¼æ€»ç»“è¡¨

| å­—æ®µ | Base æ¨¡å¼ | BYOK æ¨¡å¼ | è¯´æ˜ |
|------|-----------|-----------|------|
| `model` | `"openai-proxy/claude-opus-4-1"` | `null` | Base æ¨¡å¼çš„å”¯ä¸€æ ‡è¯† |
| `embedding` | `"openai/text-embedding-3-small"` | `null` | Base æ¨¡å¼çš„ embedding æ ‡è¯† |
| `llm_config.model` | `"claude-opus-4-1"` | `"claude-opus-4-1"` | æ¨¡å‹åï¼ˆä¸å« providerï¼‰ |
| `llm_config.provider_name` | `"openai-proxy"` | `"openai-proxy"` | Provider åç§° |
| `llm_config.provider_category` | å¯èƒ½æ˜¯ `"byok"` âŒ | `"byok"` âœ… | **ä¸å¯é ï¼ä¸è¦ç”¨** |
| `llm_config.handle` | `"openai-proxy/claude-opus-4-1"` | `null` | Base æ¨¡å¼æœ‰å€¼ |
| `embedding_config.embedding_model` | `"text-embedding-3-small"` | `"text-embedding-3-large"` | Embedding æ¨¡å‹å |
| `embedding_config.provider_name` | `"openai"` | `"openai-proxy"` | Provider åç§° |
| `embedding_config.handle` | `"openai/text-embedding-3-small"` | `null` | Base æ¨¡å¼æœ‰å€¼ |

### 20.5 å‰ç«¯å®ç°å»ºè®®

#### 20.5.1 Agent æ¨¡å‹å®šä¹‰

```dart
class Agent {
  final String id;
  final String name;
  final String? description;
  final String? model;  // â† Base æ¨¡å¼æ ‡è®°
  final Map<String, dynamic>? llmConfig;  // â† æ‰€æœ‰ Agent éƒ½æœ‰
  final Map<String, dynamic>? embeddingConfig;  // â† æ‰€æœ‰ Agent éƒ½æœ‰

  // åˆ¤æ–­æ¨¡å¼çš„æ–¹æ³•
  bool get isBaseMode => model != null;
  bool get isBYOKMode => model == null;
}
```

#### 20.5.2 Agent åˆ—è¡¨å¡ç‰‡æ˜¾ç¤º

```dart
Widget build(BuildContext context) {
  return AgentCard(
    agent: agent,
    // æ˜¾ç¤ºæ¨¡å‹ï¼ˆç»Ÿä¸€ä½¿ç”¨ handle æ ¼å¼ï¼‰
    modelLabel: _getModelLabel(agent),
  );
}

String _getModelLabel(Agent agent) {
  if (agent.isBaseMode) {
    return agent.model!;
  } else {
    final provider = agent.llmConfig!['provider_name'];
    final model = agent.llmConfig!['model'];
    return '$provider/$model';
  }
}
```

#### 20.5.3 Agent è¯¦æƒ…é¡µé¢æ˜¾ç¤º

```dart
Widget build(BuildContext context) {
  return Column(
    children: [
      // Base æ¨¡å¼é…ç½®
      if (agent.isBaseMode)
        _SectionCard(
          title: 'Model Configuration (Base Mode)',
          child: _InfoRow(
            label: 'Model Handle',
            value: agent.model!,
          ),
        ),

      // BYOK æ¨¡å¼é…ç½®
      if (agent.isBYOKMode)
        _SectionCard(
          title: 'LLM Configuration (BYOK Mode)',
          child: Column(
            children: [
              _InfoRow(label: 'Model', value: agent.llmConfig!['model']),
              _InfoRow(label: 'Provider', value: agent.llmConfig!['provider_name']),
              _InfoRow(label: 'Context Window', value: '${agent.llmConfig!['context_window']} tokens'),
            ],
          ),
        ),

      // Embedding é…ç½®ï¼ˆæ‰€æœ‰æ¨¡å¼ï¼‰
      _SectionCard(
        title: 'Embedding Configuration',
        child: Column(
          children: [
            _InfoRow(label: 'Model', value: agent.embeddingConfig!['embedding_model']),
            _InfoRow(label: 'Provider', value: agent.embeddingConfig!['provider_name']),
            _InfoRow(label: 'Dimension', value: agent.embeddingConfig!['embedding_dim'].toString()),
          ],
        ),
      ),
    ],
  );
}
```

### 20.6 å¸¸è§é—®é¢˜

#### Q1: ä¸ºä»€ä¹ˆ Base æ¨¡å¼çš„ Agent ä¹Ÿæœ‰ `llm_config`ï¼Ÿ

**A**: Letta åç«¯è®¾è®¡å†³å®šã€‚å³ä½¿æ˜¯ Base æ¨¡å¼ï¼Œåç«¯ä¹Ÿä¼šå¡«å…… `llm_config` å’Œ `embedding_config` å­—æ®µï¼Œä½†æ˜¯é¢å¤–æä¾› `model` å’Œ `embedding` å­—æ®µä½œä¸ºç®€åŒ–çš„ handle æ ¼å¼ã€‚

#### Q2: ä¸ºä»€ä¹ˆ `provider_category` å­—æ®µä¸å¯é ï¼Ÿ

**A**: å®é™…æµ‹è¯•å‘ç°ï¼ŒBase æ¨¡å¼çš„ Agent çš„ `llm_config.provider_category` ä¹Ÿå¯èƒ½æ˜¯ `"byok"`ã€‚åç«¯çš„è¿™ä¸ªå­—æ®µä¼¼ä¹ä¸æ˜¯ç”¨æ¥åŒºåˆ† Base/BYOK æ¨¡å¼çš„ï¼Œè€Œæ˜¯æœ‰å…¶ä»–ç”¨é€”ã€‚

#### Q3: å¦‚ä½•åœ¨åˆ›å»º Agent æ—¶æŒ‡å®šæ¨¡å¼ï¼Ÿ

**A**:
- **Base æ¨¡å¼**ï¼šå‘é€ `{"name": "...", "model": "openai-proxy/claude-opus-4", "embedding": "openai/text-embedding-3-small"}`
- **BYOK æ¨¡å¼**ï¼šå‘é€ `{"name": "...", "llm_config": {...}, "embedding_config": {...}}`

å‰ç«¯éœ€è¦æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„æ¨¡å¼ï¼Œä½¿ç”¨ä¸åŒçš„ JSON æ ¼å¼ã€‚

#### Q4: ä¸ºä»€ä¹ˆæœ‰äº› Agent åˆ—è¡¨é¡¹æ˜¾ç¤ºå®Œæ•´çš„ handleï¼Œæœ‰äº›ä¸æ˜¾ç¤ºï¼Ÿ

**A**: å› ä¸ºå‰ç«¯ä»£ç ä¹‹å‰åªæ£€æŸ¥ `agent.model != null`ï¼Œå¯¼è‡´ BYOK æ¨¡å¼çš„ Agent ä¸æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯ã€‚ä¿®å¤åï¼ŒBYOK æ¨¡å¼ä¹Ÿä¼šæ˜¾ç¤ºç»„åˆåçš„ handleï¼ˆ`provider/model`ï¼‰ã€‚

### 20.7 æ€»ç»“

1. **åˆ¤æ–­ Agent æ¨¡å¼çš„å”¯ä¸€å¯é æ–¹æ³•**ï¼šæ£€æŸ¥ `agent.model` å­—æ®µæ˜¯å¦ä¸º `null`
2. **ä¸è¦ä¿¡ä»» `provider_category` å­—æ®µ**ï¼šè¿™ä¸ªå­—æ®µåœ¨ Base å’Œ BYOK æ¨¡å¼ä¸‹éƒ½å¯èƒ½æ˜¾ç¤º `"byok"`
3. **æ‰€æœ‰ Agent éƒ½æœ‰ `llm_config` å’Œ `embedding_config`**ï¼šè¿™ä¸æ˜¯ BYOK æ¨¡å¼çš„ä¸“å±
4. **å‰ç«¯éœ€è¦ç»Ÿä¸€æ˜¾ç¤ºæ ¼å¼**ï¼šæ— è®ºå“ªç§æ¨¡å¼ï¼Œéƒ½åº”è¯¥æ˜¾ç¤ºå®Œæ•´çš„ handle æ ¼å¼ï¼ˆ`provider/model`ï¼‰
5. **è¯¦æƒ…é¡µé¢ä½¿ç”¨æ°´å¹³å¸ƒå±€**ï¼šlabel å›ºå®šå®½åº¦ï¼Œvalue è‡ªé€‚åº”ï¼Œæ›´æ¸…æ™°æ˜“è¯»

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼šv2.8
**æœ€åæ›´æ–°**ï¼š2026-01-09
**æœ¬ç« ä½œè€…**ï¼šKosmo + Claude Sonnet 4.5
